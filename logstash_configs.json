[
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-alter/master/lib/logstash/filters/alter.rb",
    "name": "alter",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Sets the value of field_name to the first nonnull expression among its arguments.\n\nExample:\n[source,ruby]\n    filter {\n      alter {\n        coalesce => [\n             \"field_name\", \"value1\", \"value2\", \"value3\", ...\n        ]\n      }\n    }",
        "base": false,
        "name": "coalesce",
        "validate": "array"
      },
      {
        "comments": "Change the content of the field to the specified value\nif the actual content is equal to the expected one.\n\nExample:\n[source,ruby]\n    filter {\n      alter {\n        condrewrite => [\n             \"field_name\", \"expected_value\", \"new_value\",\n             \"field_name2\", \"expected_value2, \"new_value2\",\n             ....\n           ]\n      }\n    }",
        "base": false,
        "name": "condrewrite",
        "validate": "array"
      },
      {
        "comments": "Change the content of the field to the specified value\nif the content of another field is equal to the expected one.\n\nExample:\n[source,ruby]\n    filter {\n      alter {\n        condrewriteother => [\n             \"field_name\", \"expected_value\", \"field_name_to_change\", \"value\",\n             \"field_name2\", \"expected_value2, \"field_name_to_change2\", \"value2\",\n             ....\n        ]\n      }\n    }",
        "base": false,
        "name": "condrewriteother",
        "validate": "array"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-dns/master/lib/logstash/filters/dns.rb",
    "name": "dns",
    "type": "filter",
    "params": [
      {
        "comments": "Determine what action to do: append or replace the values in the fields\nspecified under `reverse` and `resolve`.",
        "base": false,
        "name": "action",
        "validate": [
          "append",
          "replace"
        ],
        "default": "append"
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Use custom nameserver(s). For example: `[\"8.8.8.8\", \"8.8.4.4\"]`",
        "base": false,
        "name": "nameserver",
        "validate": "array"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Forward resolve one or more fields.",
        "base": false,
        "name": "resolve",
        "validate": "array"
      },
      {
        "comments": "Reverse resolve one or more fields.",
        "base": false,
        "name": "reverse",
        "validate": "array"
      },
      {
        "comments": "`resolv` calls will be wrapped in a timeout instance",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "default": "2"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-clone/master/lib/logstash/filters/clone.rb",
    "name": "clone",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "A new clone will be created with the given type for each type in this list.",
        "base": false,
        "name": "clones",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-csv/master/lib/logstash/filters/csv.rb",
    "name": "csv",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Define whether column names should autogenerated or not.\nDefaults to true. If set to false, columns not having a header specified will not be parsed.",
        "base": false,
        "name": "autogenerate_column_names",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Define a list of column names (in the order they appear in the CSV,\nas if it were a header line). If `columns` is not configured, or there\nare not enough columns specified, the default column names are\n\"column1\", \"column2\", etc. In the case that there are more columns\nin the data than specified in this column list, extra columns will be auto-numbered:\n(e.g. \"user_defined_1\", \"user_defined_2\", \"column3\", \"column4\", etc.)",
        "base": false,
        "name": "columns",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Define a set of datatype conversions to be applied to columns.\nPossible conversions are integer, float, date, date_time, boolean\n\n# Example:\n[source,ruby]\n    filter {\n      csv {\n        convert => { \"column1\" => \"integer\", \"column2\" => \"boolean\" }\n      }\n    }",
        "base": false,
        "name": "convert",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Define the character used to quote CSV fields. If this is not specified\nthe default is a double quote `\"`.\nOptional.",
        "base": false,
        "name": "quote_char",
        "validate": "string",
        "default": "\"\"\""
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Define the column separator value. If this is not specified, the default\nis a comma `,`.\nOptional.",
        "base": false,
        "name": "separator",
        "validate": "string",
        "default": ","
      },
      {
        "comments": "Define whether empty columns should be skipped.\nDefaults to false. If set to true, columns containing no value will not get set.",
        "base": false,
        "name": "skip_empty_columns",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The CSV data in the value of the `source` field will be expanded into a\ndata structure.",
        "base": false,
        "name": "source",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "Define target field for placing the data.\nDefaults to writing to the root of the event.",
        "base": false,
        "name": "target",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-date/master/lib/logstash/filters/date.rb",
    "name": "date",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Specify a locale to be used for date parsing using either IETF-BCP47 or POSIX language tag.\nSimple examples are `en`,`en-US` for BCP47 or `en_US` for POSIX.\n\nThe locale is mostly necessary to be set for parsing month names (pattern with `MMM`) and\nweekday names (pattern with `EEE`).\n\nIf not specified, the platform default will be used but for non-english platform default\nan english parser will also be used as a fallback mechanism.",
        "base": false,
        "name": "locale",
        "validate": "string"
      },
      {
        "comments": "The date formats allowed are anything allowed by Joda-Time (java time\nlibrary). You can see the docs for this format here:\n\nhttp://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html[joda.time.format.DateTimeFormat]\n\nAn array with field name first, and format patterns following, `[ field,\nformats... ]`\n\nIf your time field has multiple possible formats, you can do this:\n[source,ruby]\n    match => [ \"logdate\", \"MMM dd YYY HH:mm:ss\",\n              \"MMM  d YYY HH:mm:ss\", \"ISO8601\" ]\n\nThe above will match a syslog (rfc3164) or `iso8601` timestamp.\n\nThere are a few special exceptions. The following format literals exist\nto help you save time and ensure correctness of date parsing.\n\n* `ISO8601` - should parse any valid ISO8601 timestamp, such as\n  `2011-04-19T03:44:01.103Z`\n* `UNIX` - will parse *float or int* value expressing unix time in seconds since epoch like 1326149001.132 as well as 1326149001\n* `UNIX_MS` - will parse **int** value expressing unix time in milliseconds since epoch like 1366125117000\n* `TAI64N` - will parse tai64n time values\n\nFor example, if you have a field `logdate`, with a value that looks like\n`Aug 13 2010 00:03:44`, you would use this configuration:\n[source,ruby]\n    filter {\n      date {\n        match => [ \"logdate\", \"MMM dd YYYY HH:mm:ss\" ]\n      }\n    }\n\nIf your field is nested in your structure, you can use the nested\nsyntax `[foo][bar]` to match its value. For more information, please refer to\n<<logstash-config-field-references>>",
        "base": false,
        "name": "match",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Append values to the `tags` field when there has been no\nsuccessful match",
        "base": false,
        "name": "tag_on_failure",
        "validate": "array",
        "default": "[\"_dateparsefailure\"]"
      },
      {
        "comments": "Store the matching timestamp into the given target field.  If not provided,\ndefault to updating the `@timestamp` field of the event.",
        "base": false,
        "name": "target",
        "validate": "string",
        "default": "LogStashEventTIMESTAMP"
      },
      {
        "comments": "Specify a time zone canonical ID to be used for date parsing.\nThe valid IDs are listed on the http://joda-time.sourceforge.net/timezones.html[Joda.org available time zones page].\nThis is useful in case the time zone cannot be extracted from the value,\nand is not the platform default.\nIf this is not specified the platform default will be used.\nCanonical ID is good as it takes care of daylight saving time for you\nFor example, `America/Los_Angeles` or `Europe/Paris` are valid IDs.\nThis field can be dynamic and include parts of the event using the `%{field}` syntax",
        "base": false,
        "name": "timezone",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-collate/master/lib/logstash/filters/collate.rb",
    "name": "collate",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "How many logs should be collated.",
        "base": false,
        "name": "count",
        "validate": "number",
        "default": "1000"
      },
      {
        "comments": "The `interval` is the time window which how long the logs should be collated. (default `1m`)",
        "base": false,
        "name": "interval",
        "validate": "string",
        "default": "1m"
      },
      {
        "comments": "The `order` collated events should appear in.",
        "base": false,
        "name": "order",
        "validate": [
          "ascending",
          "descending"
        ],
        "default": "ascending"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-elasticsearch/master/lib/logstash/filters/elasticsearch.rb",
    "name": "elasticsearch",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "SSL Certificate Authority file",
        "base": false,
        "name": "ca_file",
        "validate": "path"
      },
      {
        "comments": "Hash of fields to copy from old event (found via elasticsearch) into new event",
        "base": false,
        "name": "fields",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "List of elasticsearch hosts to use for querying.",
        "base": false,
        "name": "hosts",
        "validate": "array"
      },
      {
        "comments": "Basic Auth - password",
        "base": false,
        "name": "password",
        "validate": "password"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Elasticsearch query string",
        "base": false,
        "name": "query",
        "validate": "string"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Comma-delimited list of `<field>:<direction>` pairs that define the sort order",
        "base": false,
        "name": "sort",
        "validate": "string",
        "default": "@timestampdesc"
      },
      {
        "comments": "SSL",
        "base": false,
        "name": "ssl",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Basic Auth - username",
        "base": false,
        "name": "user",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-kafka/master/lib/logstash/inputs/kafka.rb",
    "name": "kafka",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "`smallest` or `largest` - (optional, default `largest`) If the consumer does not already\nhave an established offset or offset is invalid, start with the earliest message present in the\nlog (`smallest`) or after the last message in the log (`largest`).",
        "base": false,
        "name": "auto_offset_reset",
        "validate": "%w( largest smallest )",
        "default": "largest"
      },
      {
        "comments": "Blacklist of topics to exclude from consumption.",
        "base": false,
        "name": "black_list",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "A unique id for the consumer; generated automatically if not set.",
        "base": false,
        "name": "consumer_id",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Option to restart the consumer loop on error",
        "base": false,
        "name": "consumer_restart_on_error",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Time in millis to wait for consumer to restart after an error",
        "base": false,
        "name": "consumer_restart_sleep_ms",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "Number of threads to read from the partitions. Ideally you should have as many threads as the\nnumber of partitions for a perfect balance. More threads than partitions means that some\nthreads will be idle. Less threads means a single thread could be consuming from more than\none partition",
        "base": false,
        "name": "consumer_threads",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "Throw a timeout exception to the consumer if no message is available for consumption after\nthe specified interval",
        "base": false,
        "name": "consumer_timeout_ms",
        "validate": "number",
        "default": "-1"
      },
      {
        "comments": "The serializer class for messages. The default decoder takes a byte[] and returns the same byte[]",
        "base": false,
        "name": "decoder_class",
        "validate": "string",
        "default": "kafka.serializer.DefaultDecoder"
      },
      {
        "comments": "Option to add Kafka metadata like topic, message size to the event.\nThis will add a field named `kafka` to the logstash event containing the following attributes:\n  `msg_size`: The complete serialized size of this message in bytes (including crc, header attributes, etc)\n  `topic`: The topic this message is associated with\n  `consumer_group`: The consumer group used to read in this event\n  `partition`: The partition this message is associated with\n  `key`: A ByteBuffer containing the message key",
        "base": false,
        "name": "decorate_events",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The number of byes of messages to attempt to fetch for each topic-partition in each fetch\nrequest. These bytes will be read into memory for each partition, so this helps control\nthe memory used by the consumer. The fetch request size must be at least as large as the\nmaximum message size the server allows or else it is possible for the producer to send\nmessages larger than the consumer can fetch.",
        "base": false,
        "name": "fetch_message_max_bytes",
        "validate": "number",
        "default": "1048576"
      },
      {
        "comments": "A string that uniquely identifies the group of consumer processes to which this consumer\nbelongs. By setting the same group id multiple processes indicate that they are all part of\nthe same consumer group.",
        "base": false,
        "name": "group_id",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "The serializer class for keys (defaults to the same default as for messages)",
        "base": false,
        "name": "key_decoder_class",
        "validate": "string",
        "default": "kafka.serializer.DefaultDecoder"
      },
      {
        "comments": "Internal Logstash queue size used to hold events in memory after it has been read from Kafka",
        "base": false,
        "name": "queue_size",
        "validate": "number",
        "default": "20"
      },
      {
        "comments": "Backoff time between retries during rebalance.",
        "base": false,
        "name": "rebalance_backoff_ms",
        "validate": "number",
        "default": "2000"
      },
      {
        "comments": "When a new consumer joins a consumer group the set of consumers attempt to \"rebalance\" the\nload to assign partitions to each consumer. If the set of consumers changes while this\nassignment is taking place the rebalance will fail and retry. This setting controls the\nmaximum number of attempts before giving up.",
        "base": false,
        "name": "rebalance_max_retries",
        "validate": "number",
        "default": "4"
      },
      {
        "comments": "Reset the consumer group to start at the earliest message present in the log by clearing any\noffsets for the group stored in Zookeeper. This is destructive! Must be used in conjunction\nwith auto_offset_reset => 'smallest'",
        "base": false,
        "name": "reset_beginning",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "The topic to consume messages from",
        "base": false,
        "name": "topic_id",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Whitelist of topics to include for consumption.",
        "base": false,
        "name": "white_list",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Specifies the ZooKeeper connection string in the form hostname:port where host and port are\nthe host and port of a ZooKeeper server. You can also specify multiple hosts in the form\n`hostname1:port1,hostname2:port2,hostname3:port3`.\n\nThe server may also have a ZooKeeper chroot path as part of it's ZooKeeper connection string\nwhich puts its data under some path in the global ZooKeeper namespace. If so the consumer\nshould use the same chroot path in its connection string. For example to give a chroot path of\n`/chroot/path` you would give the connection string as\n`hostname1:port1,hostname2:port2,hostname3:port3/chroot/path`.",
        "base": false,
        "name": "zk_connect",
        "validate": "string",
        "default": "localhost2181"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-grok/master/lib/logstash/filters/grok.rb",
    "name": "grok",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Break on first match. The first successful match by grok will result in the\nfilter being finished. If you want grok to try all patterns (maybe you are\nparsing different things), then set this to false.",
        "base": false,
        "name": "break_on_match",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "If `true`, keep empty captures as event fields.",
        "base": false,
        "name": "keep_empty_captures",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "",
        "base": false,
        "name": "match",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If `true`, only store named captures from grok.",
        "base": false,
        "name": "named_captures_only",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "The fields to overwrite.\n\nThis allows you to overwrite a value in a field that already exists.\n\nFor example, if you have a syslog line in the `message` field, you can\noverwrite the `message` field with part of the match like so:\n[source,ruby]\n    filter {\n      grok {\n        match => { \"message\" => \"%{SYSLOGBASE} %{DATA:message}\" }\n        overwrite => [ \"message\" ]\n      }\n    }\n\nIn this case, a line like `May 29 16:37:11 sadness logger: hello world`\nwill be parsed and `hello world` will overwrite the original message.",
        "base": false,
        "name": "overwrite",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Specify a pattern to parse with. This will match the `message` field.\n\nIf you want to match other fields than message, use the `match` setting.\nMultiple patterns is fine.",
        "base": false,
        "name": "pattern",
        "validate": "array",
        "deprecated": "You should use this instead match => { \"message\" => \"your pattern here\" }"
      },
      {
        "comments": "\nLogstash ships by default with a bunch of patterns, so you don't\nnecessarily need to define this yourself unless you are adding additional\npatterns. You can point to multiple pattern directories using this setting\nNote that Grok will read all files in the directory matching the patterns_files_glob\nand assume its a pattern file (including any tilde backup files)\n[source,ruby]\n    patterns_dir => [\"/opt/logstash/patterns\", \"/opt/logstash/extra_patterns\"]\n\nPattern files are plain text with format:\n[source,ruby]\n    NAME PATTERN\n\nFor example:\n[source,ruby]\n    NUMBER \\d+",
        "base": false,
        "name": "patterns_dir",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Glob pattern, used to select the pattern files in the directories\nspecified by patterns_dir",
        "base": false,
        "name": "patterns_files_glob",
        "validate": "string",
        "default": "*"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If `true`, make single-value fields simply that value, not an array\ncontaining that one value.",
        "base": false,
        "name": "singles",
        "validate": "boolean",
        "default": true,
        "deprecated": "\"This behavior is the default now, you don\"t need to set it.\""
      },
      {
        "comments": "Append values to the `tags` field when there has been no\nsuccessful match",
        "base": false,
        "name": "tag_on_failure",
        "validate": "array",
        "default": "[\"_grokparsefailure\"]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-prune/master/lib/logstash/filters/prune.rb",
    "name": "prune",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Exclude fields which names match specified regexps, by default exclude unresolved `%{field}` strings.\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        tags            => [ \"apache-accesslog\" ]\n        blacklist_names => [ \"method\", \"(referrer|status)\", \"${some}_field\" ]\n      }\n    }",
        "base": false,
        "name": "blacklist_names",
        "validate": "array",
        "default": "[ \"%\\{[^}]+\\}\" ]"
      },
      {
        "comments": "Exclude specified fields if their values match regexps.\nIn case field values are arrays, the fields are pruned on per array item\nin case all array items are matched whole field will be deleted.\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        tags             => [ \"apache-accesslog\" ]\n        blacklist_values => [ \"uripath\", \"/index.php\",\n                              \"method\", \"(HEAD|OPTIONS)\",\n                              \"status\", \"^[^2]\" ]\n      }\n    }",
        "base": false,
        "name": "blacklist_values",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Trigger whether configation fields and values should be interpolated for\ndynamic values.\nProbably adds some performance overhead. Defaults to false.",
        "base": false,
        "name": "interpolate",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Include only fields only if their names match specified regexps, default to empty list which means include everything.\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        tags            => [ \"apache-accesslog\" ]\n        whitelist_names => [ \"method\", \"(referrer|status)\", \"${some}_field\" ]\n      }\n    }",
        "base": false,
        "name": "whitelist_names",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Include specified fields only if their values match regexps.\nIn case field values are arrays, the fields are pruned on per array item\nthus only matching array items will be included.\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        tags             => [ \"apache-accesslog\" ]\n        whitelist_values => [ \"uripath\", \"/index.php\",\n                              \"method\", \"(GET|POST)\",\n                              \"status\", \"^[^2]\" ]\n      }\n    }",
        "base": false,
        "name": "whitelist_values",
        "validate": "hash",
        "default": "{}"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-extractnumbers/master/lib/logstash/filters/extractnumbers.rb",
    "name": "extractnumbers",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The source field for the data. By default is message.",
        "base": false,
        "name": "source",
        "validate": "string",
        "default": "message"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-throttle/master/lib/logstash/filters/throttle.rb",
    "name": "throttle",
    "type": "filter",
    "params": [
      {
        "comments": "The key used to identify events. Events with the same key will be throttled\nas a group.  Field substitutions are allowed, so you can combine multiple\nfields.",
        "base": false,
        "name": "key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Events greater than this count will be throttled. Setting this value to -1, the\ndefault, will cause no messages to be throttled based on the upper bound.",
        "base": false,
        "name": "after_count",
        "validate": "number",
        "default": "-1",
        "required": false
      },
      {
        "comments": "Events less than this count will be throttled. Setting this value to -1, the\ndefault, will cause no messages to be throttled based on the lower bound.",
        "base": false,
        "name": "before_count",
        "validate": "number",
        "default": "-1",
        "required": false
      },
      {
        "comments": "The maximum number of counters to store before the oldest counter is purged. Setting\nthis value to -1 will prevent an upper bound no constraint on the number of counters\nand they will only be purged after expiration. This configuration value should only\nbe used as a memory control mechanism and can cause early counter expiration if the\nvalue is reached. It is recommended to leave the default value and ensure that your\nkey is selected such that it limits the number of counters required (i.e. don't\nuse UUID as the key!)",
        "base": false,
        "name": "max_counters",
        "validate": "number",
        "default": "100000",
        "required": false
      },
      {
        "comments": "The period in seconds after the first occurrence of an event until the count is\nreset for the event. This period is tracked per unique key value.  Field\nsubstitutions are allowed in this value.  They will be evaluated when the _first_\nevent for a given key is seen.  This allows you to specify that certain kinds\nof events throttle for a specific period.",
        "base": false,
        "name": "period",
        "validate": "string",
        "default": "3600",
        "required": false
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-syslog_pri/master/lib/logstash/filters/syslog_pri.rb",
    "name": "syslog_pri",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Labels for facility levels. This comes from RFC3164.",
        "base": false,
        "name": "facility_labels",
        "validate": "array",
        "default": "["
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Labels for severity levels. This comes from RFC3164.",
        "base": false,
        "name": "severity_labels",
        "validate": "array",
        "default": "["
      },
      {
        "comments": "Name of field which passes in the extracted PRI part of the syslog message",
        "base": false,
        "name": "syslog_pri_field_name",
        "validate": "string",
        "default": "syslog_pri"
      },
      {
        "comments": "Add human-readable names after parsing severity and facility from PRI",
        "base": false,
        "name": "use_labels",
        "validate": "boolean",
        "default": "true"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-split/master/lib/logstash/filters/split.rb",
    "name": "split",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The field which value is split by the terminator",
        "base": false,
        "name": "field",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The field within the new event which the value is split into.\nIf not set, target field defaults to split field name.",
        "base": false,
        "name": "target",
        "validate": "string"
      },
      {
        "comments": "The string to split on. This is usually a line terminator, but can be any\nstring.",
        "base": false,
        "name": "terminator",
        "validate": "string",
        "default": "\\n"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-urldecode/master/lib/logstash/filters/urldecode.rb",
    "name": "urldecode",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Urldecode all fields",
        "base": false,
        "name": "all_fields",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Thel character encoding used in this filter. Examples include `UTF-8`\nand `cp1252`\n\nThis setting is useful if your url decoded string are in `Latin-1` (aka `cp1252`)\nor in another character set other than `UTF-8`.",
        "base": false,
        "name": "charset",
        "validate": "Encoding.name_list",
        "default": "UTF-8"
      },
      {
        "comments": "The field which value is urldecoded",
        "base": false,
        "name": "field",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-useragent/master/lib/logstash/filters/useragent.rb",
    "name": "useragent",
    "type": "filter",
    "params": [
      {
        "comments": "The field containing the user agent string. If this field is an\narray, only the first value will be used.",
        "base": false,
        "name": "source",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "UA parsing is surprisingly expensive. This filter uses an LRU cache to take advantage of the fact that\nuser agents are often found adjacent to one another in log files and rarely have a random distribution.\nThe higher you set this the more likely an item is to be in the cache and the faster this filter will run.\nHowever, if you set this too high you can use more memory than desired.\n\nExperiment with different values for this option to find the best performance for your dataset.\n\nThis MUST be set to a value > 0. There is really no reason to not want this behavior, the overhead is minimal\nand the speed gains are large.\n\nIt is important to note that this config value is global. That is to say all instances of the user agent filter\nshare the same cache. The last declared cache size will 'win'. The reason for this is that there would be no benefit\nto having multiple caches for different instances at different points in the pipeline, that would just increase the\nnumber of cache misses and waste memory.",
        "base": false,
        "name": "lru_cache_size",
        "validate": "number",
        "default": "1000"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "A string to prepend to all of the extracted keys",
        "base": false,
        "name": "prefix",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "`regexes.yaml` file to use\n\nIf not specified, this will default to the `regexes.yaml` that ships\nwith logstash.\n\nYou can find the latest version of this here:\n<https://github.com/tobie/ua-parser/blob/master/regexes.yaml>",
        "base": false,
        "name": "regexes",
        "validate": "string"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The name of the field to assign user agent data into.\n\nIf not specified user agent data will be stored in the root of the event.",
        "base": false,
        "name": "target",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-fingerprint/master/lib/logstash/filters/fingerprint.rb",
    "name": "fingerprint",
    "type": "filter",
    "params": [
      {
        "comments": "Fingerprint method",
        "base": false,
        "name": "method",
        "validate": [
          "SHA1",
          "SHA256",
          "SHA384",
          "SHA512",
          "MD5",
          "MURMUR3",
          "IPV4_NETWORK",
          "UUID",
          "PUNCTUATION"
        ],
        "required": true,
        "default": "SHA1"
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "When set to 'true', SHA1', 'SHA256', 'SHA384', 'SHA512' and 'MD5' fingerprint methods will be returned\nbase64 encoded rather than hex encoded.",
        "base": false,
        "name": "base64encode",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "When set to `true`, we concatenate the values of all fields into 1 string like the old checksum filter.",
        "base": false,
        "name": "concatenate_sources",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "When used with `IPV4_NETWORK` method fill in the subnet prefix length\nNot required for `MURMUR3` or `UUID` methods\nWith other methods fill in the `HMAC` key",
        "base": false,
        "name": "key",
        "validate": "string"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Source field(s)",
        "base": false,
        "name": "source",
        "validate": "array",
        "default": "message"
      },
      {
        "comments": "Target field.\nwill overwrite current value of a field if it exists.",
        "base": false,
        "name": "target",
        "validate": "string",
        "default": "fingerprint"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-kv/master/lib/logstash/filters/kv.rb",
    "name": "kv",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "A bool option for removing duplicate key/value pairs. When set to false, only\none unique key/value pair will be preserved.\n\nFor example, consider a source like `from=me from=me`. `[from]` will map to\nan Array with two elements: `[\"me\", \"me\"]`. to only keep unique key/value pairs,\nyou could use this configuration:\n[source,ruby]\n    filter {\n      kv {\n        allow_duplicate_values => false\n      }\n    }",
        "base": false,
        "name": "allow_duplicate_values",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "A hash specifying the default keys and their values which should be added to the event\nin case these keys do not exist in the source field being parsed.\n[source,ruby]\n    filter {\n      kv {\n        default_keys => [ \"from\", \"logstash@example.com\",\n                         \"to\", \"default@dev.null\" ]\n      }\n    }",
        "base": false,
        "name": "default_keys",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "An array specifying the parsed keys which should not be added to the event.\nBy default no keys will be excluded.\n\nFor example, consider a source like `Hey, from=<abc>, to=def foo=bar`.\nTo exclude `from` and `to`, but retain the `foo` key, you could use this configuration:\n[source,ruby]\n    filter {\n      kv {\n        exclude_keys => [ \"from\", \"to\" ]\n      }\n    }",
        "base": false,
        "name": "exclude_keys",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "A string of characters to use as delimiters for parsing out key-value pairs.\n\nThese characters form a regex character class and thus you must escape special regex\ncharacters like `[` or `]` using `\\`.\n\n#### Example with URL Query Strings\n\nFor example, to split out the args from a url query string such as\n`?pin=12345~0&d=123&e=foo@bar.com&oq=bobo&ss=12345`:\n[source,ruby]\n    filter {\n      kv {\n        field_split => \"&?\"\n      }\n    }\n\nThe above splits on both `&` and `?` characters, giving you the following\nfields:\n\n* `pin: 12345~0`\n* `d: 123`\n* `e: foo@bar.com`\n* `oq: bobo`\n* `ss: 12345`",
        "base": false,
        "name": "field_split",
        "validate": "string",
        "default": " "
      },
      {
        "comments": "A boolean specifying whether to include brackets as value `wrappers`\n(the default is true)\n[source,ruby]\n    filter {\n      kv {\n        include_brackets => true\n      }\n    }\n\nFor example, the result of this line:\n`bracketsone=(hello world) bracketstwo=[hello world]`\n\nwill be:\n\n* bracketsone: hello world\n* bracketstwo: hello world\n\ninstead of:\n\n* bracketsone: (hello\n* bracketstwo: [hello\n",
        "base": false,
        "name": "include_brackets",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "An array specifying the parsed keys which should be added to the event.\nBy default all keys will be added.\n\nFor example, consider a source like `Hey, from=<abc>, to=def foo=bar`.\nTo include `from` and `to`, but exclude the `foo` key, you could use this configuration:\n[source,ruby]\n    filter {\n      kv {\n        include_keys => [ \"from\", \"to\" ]\n      }\n    }",
        "base": false,
        "name": "include_keys",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "A string to prepend to all of the extracted keys.\n\nFor example, to prepend arg_ to all keys:\n[source,ruby]\n    filter { kv { prefix => \"arg_\" } }",
        "base": false,
        "name": "prefix",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "A boolean specifying whether to drill down into values\nand recursively get more key-value pairs from it.\nThe extra key-value pairs will be stored as subkeys of the root key.\n\nDefault is not to recursive values.\n[source,ruby]\n    filter {\n      kv {\n        recursive => \"true\"\n      }\n    }\n",
        "base": false,
        "name": "recursive",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The field to perform `key=value` searching on\n\nFor example, to process the `not_the_message` field:\n[source,ruby]\n    filter { kv { source => \"not_the_message\" } }",
        "base": false,
        "name": "source",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "The name of the container to put all of the key-value pairs into.\n\nIf this setting is omitted, fields will be written to the root of the\nevent, as individual fields.\n\nFor example, to place all keys into the event field kv:\n[source,ruby]\n    filter { kv { target => \"kv\" } }",
        "base": false,
        "name": "target",
        "validate": "string"
      },
      {
        "comments": "A string of characters to trim from the value. This is useful if your\nvalues are wrapped in brackets or are terminated with commas (like postfix\nlogs).\n\nThese characters form a regex character class and thus you must escape special regex\ncharacters like `[` or `]` using `\\`.\n\nFor example, to strip `<`, `>`, `[`, `]` and `,` characters from values:\n[source,ruby]\n    filter {\n      kv {\n        trim => \"<>\\[\\],\"\n      }\n    }",
        "base": false,
        "name": "trim",
        "validate": "string"
      },
      {
        "comments": "A string of characters to trim from the key. This is useful if your\nkeys are wrapped in brackets or start with space.\n\nThese characters form a regex character class and thus you must escape special regex\ncharacters like `[` or `]` using `\\`.\n\nFor example, to strip `<` `>` `[` `]` and `,` characters from keys:\n[source,ruby]\n    filter {\n      kv {\n        trimkey => \"<>\\[\\],\"\n      }\n    }",
        "base": false,
        "name": "trimkey",
        "validate": "string"
      },
      {
        "comments": "A string of characters to use as delimiters for identifying key-value relations.\n\nThese characters form a regex character class and thus you must escape special regex\ncharacters like `[` or `]` using `\\`.\n\nFor example, to identify key-values such as\n`key1:value1 key2:value2`:\n[source,ruby]\n    filter { kv { value_split => \":\" } }",
        "base": false,
        "name": "value_split",
        "validate": "string",
        "default": "="
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-sleep/master/lib/logstash/filters/sleep.rb",
    "name": "sleep",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Sleep on every N'th. This option is ignored in replay mode.\n\nExample:\n[source,ruby]\n    filter {\n      sleep {\n        time => \"1\"   # Sleep 1 second\n        every => 10   # on every 10th event\n      }\n    }",
        "base": false,
        "name": "every",
        "validate": "string",
        "default": "1"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Enable replay mode.\n\nReplay mode tries to sleep based on timestamps in each event.\n\nThe amount of time to sleep is computed by subtracting the\nprevious event's timestamp from the current event's timestamp.\nThis helps you replay events in the same timeline as original.\n\nIf you specify a `time` setting as well, this filter will\nuse the `time` value as a speed modifier. For example,\na `time` value of 2 will replay at double speed, while a\nvalue of 0.25 will replay at 1/4th speed.\n\nFor example:\n[source,ruby]\n    filter {\n      sleep {\n        time => 2\n        replay => true\n      }\n    }\n\nThe above will sleep in such a way that it will perform\nreplay 2-times faster than the original time speed.",
        "base": false,
        "name": "replay",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The length of time to sleep, in seconds, for every event.\n\nThis can be a number (eg, 0.5), or a string (eg, `%{foo}`)\nThe second form (string with a field value) is useful if\nyou have an attribute of your event that you want to use\nto indicate the amount of time to sleep.\n\nExample:\n[source,ruby]\n    filter {\n      sleep {\n        # Sleep 1 second for every event.\n        time => \"1\"\n      }\n    }",
        "base": false,
        "name": "time",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-tld/master/lib/logstash/filters/tld.rb",
    "name": "tld",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The source field to parse",
        "base": false,
        "name": "source",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "The target field to place all the data",
        "base": false,
        "name": "target",
        "validate": "string",
        "default": "tld"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-example/master/lib/logstash/filters/example.rb",
    "name": "example",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Replace the message with this value.",
        "base": false,
        "name": "message",
        "validate": "string",
        "default": "Hello World!"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-rss/master/lib/logstash/inputs/rss.rb",
    "name": "rss",
    "type": "input",
    "params": [
      {
        "comments": "Interval to run the command. Value is in seconds.",
        "base": false,
        "name": "interval",
        "validate": "number",
        "required": true
      },
      {
        "comments": "RSS/Atom feed URL",
        "base": false,
        "name": "url",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-drupal_dblog/master/lib/logstash/inputs/drupal_dblog.rb",
    "name": "drupal_dblog",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "By default, the event only contains the current user id as a field.\nIf you whish to add the username as an additional field, set this to true.",
        "base": false,
        "name": "add_usernames",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The amount of log messages that should be fetched with each query.\nBulk fetching is done to prevent querying huge data sets when lots of\nmessages are in the database.",
        "base": false,
        "name": "bulksize",
        "validate": "number",
        "default": "5000"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Specify all drupal databases that you whish to import from.\nThis can be as many as you whish.\nThe format is a hash, with a unique site name as the key, and a databse\nurl as the value.\n\nExample:\n[\n  \"site1\", \"mysql://user1:password@host1.com/databasename\",\n  \"other_site\", \"mysql://user2:password@otherhost.com/databasename\",\n  ...\n]",
        "base": false,
        "name": "databases",
        "validate": "hash"
      },
      {
        "comments": "Time between checks in minutes.",
        "base": false,
        "name": "interval",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Label this input with a type.\nTypes are used mainly for filter activation.\n\n\nIf you create an input with type \"foobar\", then only filters\nwhich also have type \"foobar\" will act on them.\n\nThe type is also stored as part of the event itself, so you\ncan also use the type to search for in the web interface.",
        "base": false,
        "name": "type",
        "validate": "string",
        "default": "watchdog"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-jms/master/lib/logstash/inputs/jms.rb",
    "name": "jms",
    "type": "input",
    "params": [
      {
        "comments": "Name of the destination queue or topic to use.",
        "base": false,
        "name": "destination",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Url to use when connecting to the JMS provider",
        "base": false,
        "name": "broker_url",
        "validate": "string"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Name of JMS Provider Factory class",
        "base": false,
        "name": "factory",
        "validate": "string"
      },
      {
        "comments": "Include JMS Message Body in the event\nSupports TextMessage, MapMessage and ByteMessage\nIf the JMS Message is a TextMessage or ByteMessage, then the value will be in the \"message\" field of the event\nIf the JMS Message is a MapMessage, then all the key/value pairs will be added in the Hashmap of the event\nStreamMessage and ObjectMessage are not supported",
        "base": false,
        "name": "include_body",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "A JMS message has three parts :\n Message Headers (required)\n Message Properties (optional)\n Message Bodies (optional)\nYou can tell the input plugin which parts should be included in the event produced by Logstash\n\nInclude JMS Message Header Field values in the event",
        "base": false,
        "name": "include_header",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Include JMS Message Properties Field values in the event",
        "base": false,
        "name": "include_properties",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Polling interval in seconds.\nThis is the time sleeping between asks to a consumed Queue.\nThis parameter has non influence in the case of a subcribed Topic.",
        "base": false,
        "name": "interval",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "Mandatory if jndi lookup is being used,\ncontains details on how to connect to JNDI server",
        "base": false,
        "name": "jndi_context",
        "validate": "hash"
      },
      {
        "comments": "Name of JNDI entry at which the Factory can be found",
        "base": false,
        "name": "jndi_name",
        "validate": "string"
      },
      {
        "comments": "Password to use when connecting to the JMS provider",
        "base": false,
        "name": "password",
        "validate": "string"
      },
      {
        "comments": "If pub-sub (topic) style should be used or not.",
        "base": false,
        "name": "pub_sub",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "An optional array of Jar file names to load for the specified\nJMS provider. By using this option it is not necessary\nto put all the JMS Provider specific jar files into the\njava CLASSPATH prior to starting Logstash.",
        "base": false,
        "name": "require_jars",
        "validate": "array"
      },
      {
        "comments": "Choose an implementation of the run block. Value can be either consumer, async or thread",
        "base": false,
        "name": "runner",
        "validate": [
          "consumer",
          "async",
          "thread"
        ],
        "default": "consumer"
      },
      {
        "comments": "Set the selector to use to get messages off the queue or topic",
        "base": false,
        "name": "selector",
        "validate": "string"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Initial connection timeout in seconds.",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Convert the JMSTimestamp header field to the @timestamp value of the event\nDon't use it for now, it is buggy",
        "base": false,
        "name": "use_jms_timestamp",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Username to connect to JMS provider with",
        "base": false,
        "name": "username",
        "validate": "string"
      },
      {
        "comments": "Yaml config file",
        "base": false,
        "name": "yaml_file",
        "validate": "string"
      },
      {
        "comments": "Yaml config file section name\nFor some known examples, see: [Example jms.yml](https://github.com/reidmorrison/jruby-jms/blob/master/examples/jms.yml)",
        "base": false,
        "name": "yaml_section",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-couchdb_changes/master/lib/logstash/inputs/couchdb_changes.rb",
    "name": "couchdb_changes",
    "type": "input",
    "params": [
      {
        "comments": "The CouchDB db to connect to.\nRequired parameter.",
        "base": false,
        "name": "db",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Reconnect flag.  When true, always try to reconnect after a failure",
        "base": false,
        "name": "always_reconnect",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Path to a CA certificate file, used to validate certificates",
        "base": false,
        "name": "ca_file",
        "validate": "path"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Logstash connects to CouchDB's _changes with feed=continuous\nThe heartbeat is how often (in milliseconds) Logstash will ping\nCouchDB to ensure the connection is maintained.  Changing this\nsetting is not recommended unless you know what you are doing.",
        "base": false,
        "name": "heartbeat",
        "validate": "number",
        "default": "1000"
      },
      {
        "comments": "IP or hostname of your CouchDB instance",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "Future feature! Until implemented, changing this from the default\nwill not do anything.\n\nIgnore attachments associated with CouchDB documents.",
        "base": false,
        "name": "ignore_attachments",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "If unspecified, Logstash will attempt to read the last sequence number\nfrom the `sequence_path` file.  If that is empty or non-existent, it will\nbegin with 0 (the beginning).\n\nIf you specify this value, it is anticipated that you will\nonly be doing so for an initial read under special circumstances\nand that you will unset this value afterwards.",
        "base": false,
        "name": "initial_sequence",
        "validate": "number"
      },
      {
        "comments": "Preserve the CouchDB document revision \"_rev\" value in the\noutput.",
        "base": false,
        "name": "keep_revision",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Password, if authentication is needed to connect to\nCouchDB",
        "base": false,
        "name": "password",
        "validate": "password",
        "default": "nil"
      },
      {
        "comments": "Port of your CouchDB instance.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "5984"
      },
      {
        "comments": "Reconnect delay: time between reconnect attempts, in seconds.",
        "base": false,
        "name": "reconnect_delay",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "Connect to CouchDB's _changes feed securely (via https)\nDefault: false (via http)",
        "base": false,
        "name": "secure",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "File path where the last sequence number in the _changes\nstream is stored. If unset it will write to `$HOME/.couchdb_seq`",
        "base": false,
        "name": "sequence_path",
        "validate": "string"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Timeout: Number of milliseconds to wait for new data before\nterminating the connection.  If a timeout is set it will disable\nthe heartbeat configuration option.",
        "base": false,
        "name": "timeout",
        "validate": "number"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Username, if authentication is needed to connect to\nCouchDB",
        "base": false,
        "name": "username",
        "validate": "string",
        "default": "nil"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-meetup/master/lib/logstash/inputs/meetup.rb",
    "name": "meetup",
    "type": "input",
    "params": [
      {
        "comments": "Interval to run the command. Value is in seconds.",
        "base": false,
        "name": "interval",
        "validate": "number",
        "required": true
      },
      {
        "comments": "Meetup Key",
        "base": false,
        "name": "meetupkey",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Event Status'",
        "base": false,
        "name": "eventstatus",
        "validate": "string",
        "default": "upcoming,past"
      },
      {
        "comments": "The Group ID, multiple may be specified seperated by commas\nMust have one of `urlname`, `venueid`, `groupid`",
        "base": false,
        "name": "groupid",
        "validate": "string"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "URLName - the URL name ie `ElasticSearch-Oklahoma-City`\nMust have one of urlname, venue_id, group_id",
        "base": false,
        "name": "urlname",
        "validate": "string"
      },
      {
        "comments": "The venue ID\nMust have one of `urlname`, `venue_id`, `group_id`",
        "base": false,
        "name": "venueid",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-zeromq/master/lib/logstash/filters/zeromq.rb",
    "name": "zeromq",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "tag to add if zeromq timeout expires before getting back an answer.\nIf set to \"\" then no tag will be added.",
        "base": false,
        "name": "add_tag_on_timeout",
        "validate": "string",
        "default": "zeromqtimeout"
      },
      {
        "comments": "0mq socket address to connect or bind\nPlease note that inproc:// will not work with logstash\nas we use a context per thread\nBy default, filters connect",
        "base": false,
        "name": "address",
        "validate": "string",
        "default": "tcp//127.0.0.12121"
      },
      {
        "comments": "The field to send off-site for processing\nIf this is unset, the whole event will be sent",
        "base": false,
        "name": "field",
        "validate": "string"
      },
      {
        "comments": "0mq mode\nserver mode binds/listens\nclient mode connects",
        "base": false,
        "name": "mode",
        "validate": [
          "server",
          "client"
        ],
        "default": "client"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "number of retries, used for both sending and receiving messages.\nfor sending, retries should return instantly.\nfor receiving, the total blocking time is up to retries X timeout,\nwhich by default is 3 X 500 = 1500ms",
        "base": false,
        "name": "retries",
        "validate": "number",
        "default": "3"
      },
      {
        "comments": "0mq socket options\nThis exposes zmq_setsockopt\nfor advanced tuning\nsee http://api.zeromq.org/2-1:zmq-setsockopt for details\n\nThis is where you would set values like:\nZMQ::HWM - high water mark\nZMQ::IDENTITY - named queues\nZMQ::SWAP_SIZE - space for disk overflow\nZMQ::SUBSCRIBE - topic filters for pubsub\n\nexample: sockopt => [\"ZMQ::HWM\", 50, \"ZMQ::IDENTITY\", \"my_named_queue\"]",
        "base": false,
        "name": "sockopt",
        "validate": "hash"
      },
      {
        "comments": "timeout in milliseconds on which to wait for a reply.",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "default": "500"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-zabbix/master/lib/logstash/outputs/zabbix.rb",
    "name": "zabbix",
    "type": "output",
    "params": [
      {
        "comments": "The field name which holds the Zabbix host name. This can be a sub-field of\nthe @metadata field.",
        "base": false,
        "name": "zabbix_host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Use the `multi_value` directive to send multiple key/value pairs.\nThis can be thought of as an array, like:\n\n`[ zabbix_key1, zabbix_value1, zabbix_key2, zabbix_value2, ... zabbix_keyN, zabbix_valueN ]`\n\n...where `zabbix_key1` is an instance of `zabbix_key`, and `zabbix_value1`\nis an instance of `zabbix_value`.  If the field referenced by any\n`zabbix_key` or `zabbix_value` does not exist, that entry will be ignored.\n\nThis directive cannot be used in conjunction with the single-value directives\n`zabbix_key` and `zabbix_value`.",
        "base": false,
        "name": "multi_value",
        "validate": "array"
      },
      {
        "comments": "The number of seconds to wait before giving up on a connection to the Zabbix\nserver. This number should be very small, otherwise delays in delivery of\nother outputs could result.",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "A single field name which holds the value you intend to use as the Zabbix\nitem key. This can be a sub-field of the @metadata field.\nThis directive will be ignored if using `multi_value`\n\nIMPORTANT: `zabbix_key` is required if not using `multi_value`.\n",
        "base": false,
        "name": "zabbix_key",
        "validate": "string"
      },
      {
        "comments": "The IP or resolvable hostname where the Zabbix server is running",
        "base": false,
        "name": "zabbix_server_host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "The port on which the Zabbix server is running",
        "base": false,
        "name": "zabbix_server_port",
        "validate": "number",
        "default": "10051"
      },
      {
        "comments": "The field name which holds the value you want to send.\nThis directive will be ignored if using `multi_value`",
        "base": false,
        "name": "zabbix_value",
        "validate": "string",
        "default": "message"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-metaevent/master/lib/logstash/filters/metaevent.rb",
    "name": "metaevent",
    "type": "filter",
    "params": [
      {
        "comments": "syntax: `followed_by_tags => [ \"tag\", \"tag\" ]`",
        "base": false,
        "name": "followed_by_tags",
        "validate": "array",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "syntax: `period => 60`",
        "base": false,
        "name": "period",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-gelf/master/lib/logstash/inputs/gelf.rb",
    "name": "gelf",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The IP address or hostname to listen on.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "The port to listen on. Remember that ports less than 1024 (privileged\nports) may require root to use.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "12201"
      },
      {
        "comments": "Whether or not to remap the GELF message fields to Logstash event fields or\nleave them intact.\n\nRemapping converts the following GELF fields to Logstash equivalents:\n\n* `full\\_message` becomes `event[\"message\"]`.\n* if there is no `full\\_message`, `short\\_message` becomes `event[\"message\"]`.",
        "base": false,
        "name": "remap",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Whether or not to remove the leading `\\_` in GELF fields or leave them\nin place. (Logstash < 1.2 did not remove them by default.). Note that\nGELF version 1.1 format now requires all non-standard fields to be added\nas an \"additional\" field, beginning with an underscore.\n\ne.g. `\\_foo` becomes `foo`\n",
        "base": false,
        "name": "strip_leading_underscore",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-redmine/master/lib/logstash/outputs/redmine.rb",
    "name": "redmine",
    "type": "output",
    "params": [
      {
        "comments": "redmine issue priority_id\nrequired",
        "base": false,
        "name": "priority_id",
        "validate": "number",
        "required": true
      },
      {
        "comments": "redmine issue projet_id\nrequired",
        "base": false,
        "name": "project_id",
        "validate": "number",
        "required": true
      },
      {
        "comments": "redmine issue status_id\nrequired",
        "base": false,
        "name": "status_id",
        "validate": "number",
        "required": true
      },
      {
        "comments": "redmine token user used for authentication\nrequired",
        "base": false,
        "name": "token",
        "validate": "string",
        "required": true
      },
      {
        "comments": "redmine issue tracker_id\nrequired",
        "base": false,
        "name": "tracker_id",
        "validate": "number",
        "required": true
      },
      {
        "comments": "host of redmine app\nrequired\nvalue format : 'http://urlofredmine.tld' - Not add '/issues' at end",
        "base": false,
        "name": "url",
        "validate": "string",
        "required": true
      },
      {
        "comments": "redmine issue assigned_to\nnot required for post_issue",
        "base": false,
        "name": "assigned_to_id",
        "validate": "number",
        "default": "nil"
      },
      {
        "comments": "redmine issue categorie_id\nnot required for post_issue",
        "base": false,
        "name": "categorie_id",
        "validate": "number",
        "default": "nil"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "redmine issue description\nrequired",
        "base": false,
        "name": "description",
        "validate": "string",
        "default": "%{message}"
      },
      {
        "comments": "redmine issue fixed_version_id\nnot required for post_issue",
        "base": false,
        "name": "fixed_version_id",
        "validate": "number",
        "default": "nil"
      },
      {
        "comments": "redmine issue parent_issue_id\nnot required for post_issue",
        "base": false,
        "name": "parent_issue_id",
        "validate": "number",
        "default": "nil"
      },
      {
        "comments": "http request ssl trigger\nnot required",
        "base": false,
        "name": "ssl",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "redmine issue subject\nrequired",
        "base": false,
        "name": "subject",
        "validate": "string",
        "default": "%{host}"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-mongodb/master/lib/logstash/outputs/mongodb.rb",
    "name": "mongodb",
    "type": "output",
    "params": [
      {
        "comments": "The collection to use. This value can use `%{foo}` values to dynamically\nselect a collection based on data in the event.",
        "base": false,
        "name": "collection",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The database to use",
        "base": false,
        "name": "database",
        "validate": "string",
        "required": true
      },
      {
        "comments": "a MongoDB URI to connect to\nSee http://docs.mongodb.org/manual/reference/connection-string/",
        "base": false,
        "name": "uri",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "If true, an \"_id\" field will be added to the document before insertion.\nThe \"_id\" field will use the timestamp of the event and overwrite an existing\n\"_id\" field in the event.",
        "base": false,
        "name": "generateId",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If true, store the @timestamp field in mongodb as an ISODate type instead\nof an ISO8601 string.  For more information about this, see\nhttp://www.mongodb.org/display/DOCS/Dates",
        "base": false,
        "name": "isodate",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Number of seconds to wait after failure before retrying",
        "base": false,
        "name": "retry_delay",
        "validate": "number",
        "default": "3",
        "required": false
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-sqlite/master/lib/logstash/inputs/sqlite.rb",
    "name": "sqlite",
    "type": "input",
    "params": [
      {
        "comments": "The path to the sqlite database file.",
        "base": false,
        "name": "path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "How many rows to fetch at a time from each `SELECT` call.",
        "base": false,
        "name": "batch",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Any tables to exclude by name.\nBy default all tables are followed.",
        "base": false,
        "name": "exclude_tables",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-xml/master/lib/logstash/filters/xml.rb",
    "name": "xml",
    "type": "filter",
    "params": [
      {
        "comments": "Config for xml to hash is:\n[source,ruby]\n    source => source_field\n\nFor example, if you have the whole XML document in your `message` field:\n[source,ruby]\n    filter {\n      xml {\n        source => \"message\"\n      }\n    }\n\nThe above would parse the XML from the `message` field.",
        "base": false,
        "name": "source",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "By default only namespaces declarations on the root element are considered.\nThis allows to configure all namespace declarations to parse the XML document.\n\nExample:\n\n[source,ruby]\nfilter {\n  xml {\n    namespaces => {\n      \"xsl\" => \"http://www.w3.org/1999/XSL/Transform\"\n      \"xhtml\" => http://www.w3.org/1999/xhtml\"\n    }\n  }\n}\n",
        "base": false,
        "name": "namespaces",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Remove all namespaces from all nodes in the document.\nOf course, if the document had nodes with the same names but different namespaces, they will now be ambiguous.",
        "base": false,
        "name": "remove_namespaces",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "By default the filter will store the whole parsed XML in the destination\nfield as described above. Setting this to false will prevent that.",
        "base": false,
        "name": "store_xml",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Define target for placing the data\n\nFor example if you want the data to be put in the `doc` field:\n[source,ruby]\n    filter {\n      xml {\n        target => \"doc\"\n      }\n    }\n\nXML in the value of the source field will be expanded into a\ndatastructure in the `target` field.\nNote: if the `target` field already exists, it will be overridden.\nRequired if `store_xml` is true (which is the default).",
        "base": false,
        "name": "target",
        "validate": "string"
      },
      {
        "comments": "xpath will additionally select string values (non-strings will be\nconverted to strings with Ruby's `to_s` function) from parsed XML\n(using each source field defined using the method above) and place\nthose values in the destination fields. Configuration:\n[source,ruby]\nxpath => [ \"xpath-syntax\", \"destination-field\" ]\n\nValues returned by XPath parsing from `xpath-syntax` will be put in the\ndestination field. Multiple values returned will be pushed onto the\ndestination field as an array. As such, multiple matches across\nmultiple source fields will produce duplicate entries in the field.\n\nMore on XPath: http://www.w3schools.com/xpath/\n\nThe XPath functions are particularly powerful:\nhttp://www.w3schools.com/xpath/xpath_functions.asp\n",
        "base": false,
        "name": "xpath",
        "validate": "hash",
        "default": "{}"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-cidr/master/lib/logstash/filters/cidr.rb",
    "name": "cidr",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The IP address(es) to check with. Example:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"testnet\" ]\n        address => [ \"%{src_ip}\", \"%{dst_ip}\" ]\n        network => [ \"192.0.2.0/24\" ]\n      }\n    }",
        "base": false,
        "name": "address",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The IP network(s) to check against. Example:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"linklocal\" ]\n        address => [ \"%{clientip}\" ]\n        network => [ \"169.254.0.0/16\", \"fe80::/64\" ]\n      }\n    }",
        "base": false,
        "name": "network",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-irc/master/lib/logstash/outputs/irc.rb",
    "name": "irc",
    "type": "output",
    "params": [
      {
        "comments": "Channels to broadcast to.\n\nThese should be full channel names including the '#' symbol, such as\n\"#logstash\".",
        "base": false,
        "name": "channels",
        "validate": "array",
        "required": true
      },
      {
        "comments": "Address of the host to connect to",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Message format to send, event tokens are usable here",
        "base": false,
        "name": "format",
        "validate": "string",
        "default": "%{message}"
      },
      {
        "comments": "Limit the rate of messages sent to IRC in messages per second.",
        "base": false,
        "name": "messages_per_second",
        "validate": "number",
        "default": "0.5"
      },
      {
        "comments": "IRC Nickname",
        "base": false,
        "name": "nick",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "IRC server password",
        "base": false,
        "name": "password",
        "validate": "password"
      },
      {
        "comments": "Port on host to connect to.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "6667"
      },
      {
        "comments": "Static string after event",
        "base": false,
        "name": "post_string",
        "validate": "string",
        "required": false
      },
      {
        "comments": "Static string before event",
        "base": false,
        "name": "pre_string",
        "validate": "string",
        "required": false
      },
      {
        "comments": "IRC Real name",
        "base": false,
        "name": "real",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "Set this to true to enable SSL.",
        "base": false,
        "name": "secure",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "IRC Username",
        "base": false,
        "name": "user",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-tcp/master/lib/logstash/inputs/tcp.rb",
    "name": "tcp",
    "type": "input",
    "params": [
      {
        "comments": "When mode is `server`, the port to listen on.\nWhen mode is `client`, the port to connect to.",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "",
        "base": false,
        "name": "data_timeout",
        "validate": "number",
        "default": -1,
        "deprecated": "This setting is not used by this plugin. It will be removed soon."
      },
      {
        "comments": "When mode is `server`, the address to listen on.\nWhen mode is `client`, the address to connect to.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "Mode to operate in. `server` listens for client connections,\n`client` connects to a server.",
        "base": false,
        "name": "mode",
        "validate": [
          "server",
          "client"
        ],
        "default": "server"
      },
      {
        "comments": "The SSL CA certificate, chainfile or CA path. The system CA path is automatically included.",
        "base": false,
        "name": "ssl_cacert",
        "validate": "path",
        "deprecated": "This setting is deprecated in favor of ssl_extra_chain_certs as it sets a more clear expectation to add more X509 certificates to the store"
      },
      {
        "comments": "SSL certificate path",
        "base": false,
        "name": "ssl_cert",
        "validate": "path"
      },
      {
        "comments": "Enable SSL (must be set for other `ssl_` options to take effect).",
        "base": false,
        "name": "ssl_enable",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "An Array of extra X509 certificates to be added to the certificate chain.\nUseful when the CA chain is not necessary in the system store.",
        "base": false,
        "name": "ssl_extra_chain_certs",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "SSL key path",
        "base": false,
        "name": "ssl_key",
        "validate": "path"
      },
      {
        "comments": "SSL key passphrase",
        "base": false,
        "name": "ssl_key_passphrase",
        "validate": "password",
        "default": "nil"
      },
      {
        "comments": "Verify the identity of the other end of the SSL connection against the CA.\nFor input, sets the field `sslsubject` to that of the client certificate.",
        "base": false,
        "name": "ssl_verify",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-juggernaut/master/lib/logstash/outputs/juggernaut.rb",
    "name": "juggernaut",
    "type": "output",
    "params": [
      {
        "comments": "List of channels to which to publish. Dynamic names are\nvalid here, for example `logstash-%{type}`.",
        "base": false,
        "name": "channels",
        "validate": "array",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The redis database number.",
        "base": false,
        "name": "db",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "The hostname of the redis server to which juggernaut is listening.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "127.0.0.1"
      },
      {
        "comments": "How should the message be formatted before pushing to the websocket.",
        "base": false,
        "name": "message_format",
        "validate": "string"
      },
      {
        "comments": "Password to authenticate with.  There is no authentication by default.",
        "base": false,
        "name": "password",
        "validate": "password"
      },
      {
        "comments": "The port to connect on.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "6379"
      },
      {
        "comments": "Redis initial connection timeout in seconds.",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-nagios_nsca/master/lib/logstash/outputs/nagios_nsca.rb",
    "name": "nagios_nsca",
    "type": "output",
    "params": [
      {
        "comments": "The status to send to nagios. Should be 0 = OK, 1 = WARNING, 2 = CRITICAL, 3 = UNKNOWN",
        "base": false,
        "name": "nagios_status",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The nagios host or IP to send logs to. It should have a NSCA daemon running.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "The format to use when writing events to nagios. This value\nsupports any string and can include `%{name}` and other dynamic\nstrings.",
        "base": false,
        "name": "message_format",
        "validate": "string",
        "default": "%{@timestamp} %{host} %{message}"
      },
      {
        "comments": "The nagios 'host' you want to submit a passive check result to. This\nparameter accepts interpolation, e.g. you can use `@source_host` or other\nlogstash internal variables.",
        "base": false,
        "name": "nagios_host",
        "validate": "string",
        "default": "%{host}"
      },
      {
        "comments": "The nagios 'service' you want to submit a passive check result to. This\nparameter accepts interpolation, e.g. you can use `@source_host` or other\nlogstash internal variables.",
        "base": false,
        "name": "nagios_service",
        "validate": "string",
        "default": "LOGSTASH"
      },
      {
        "comments": "The port where the NSCA daemon on the nagios host listens.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "5667"
      },
      {
        "comments": "The path to the 'send_nsca' binary on the local host.",
        "base": false,
        "name": "send_nsca_bin",
        "validate": "string",
        "default": "/usr/sbin/send_nsca"
      },
      {
        "comments": "The path to the send_nsca config file on the local host.\nLeave blank if you don't want to provide a config file.",
        "base": false,
        "name": "send_nsca_config",
        "validate": "path"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-websocket/master/lib/logstash/inputs/websocket.rb",
    "name": "websocket",
    "type": "input",
    "params": [
      {
        "comments": "The URL to connect to.",
        "base": false,
        "name": "url",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Select the plugin's mode of operation. Right now only client mode\nis supported, i.e. this plugin connects to a websocket server and\nreceives events from the server as websocket messages.",
        "base": false,
        "name": "mode",
        "validate": [
          "client"
        ],
        "default": "client"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-datadog/master/lib/logstash/outputs/datadog.rb",
    "name": "datadog",
    "type": "output",
    "params": [
      {
        "comments": "Your DatadogHQ API key",
        "base": false,
        "name": "api_key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Alert type",
        "base": false,
        "name": "alert_type",
        "validate": [
          "info",
          "error",
          "warning",
          "success"
        ]
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Date Happened",
        "base": false,
        "name": "date_happened",
        "validate": "string"
      },
      {
        "comments": "Tags\nSet any custom tags for this event\nDefault are the Logstash tags if any",
        "base": false,
        "name": "dd_tags",
        "validate": "array"
      },
      {
        "comments": "Priority",
        "base": false,
        "name": "priority",
        "validate": [
          "normal",
          "low"
        ]
      },
      {
        "comments": "Source type name",
        "base": false,
        "name": "source_type_name",
        "validate": [
          "nagios",
          "hudson",
          "jenkins",
          "user",
          "my apps",
          "feed",
          "chef",
          "puppet",
          "git",
          "bitbucket",
          "fabric",
          "capistrano"
        ],
        "default": "my apps"
      },
      {
        "comments": "Text",
        "base": false,
        "name": "text",
        "validate": "string",
        "default": "%{message}"
      },
      {
        "comments": "Title",
        "base": false,
        "name": "title",
        "validate": "string",
        "default": "Logstash event for %{host}"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-oui/master/lib/logstash/filters/oui.rb",
    "name": "oui",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The source field to parse",
        "base": false,
        "name": "source",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "The target field to place all the data",
        "base": false,
        "name": "target",
        "validate": "string",
        "default": "oui"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-jms/master/lib/logstash/outputs/jms.rb",
    "name": "jms",
    "type": "output",
    "params": [
      {
        "comments": "Url to use when connecting to the JMS provider",
        "base": false,
        "name": "broker_url",
        "validate": "string"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Name of delivery mode to use\nOptions are \"persistent\" and \"non_persistent\" if not defined nothing will be passed.",
        "base": false,
        "name": "delivery_mode",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Name of the destination queue or topic to use.\nMandatory",
        "base": false,
        "name": "destination",
        "validate": "string"
      },
      {
        "comments": "Name of JMS Provider Factory class",
        "base": false,
        "name": "factory",
        "validate": "string"
      },
      {
        "comments": "Mandatory if jndi lookup is being used,\ncontains details on how to connect to JNDI server",
        "base": false,
        "name": "jndi_context",
        "validate": "hash"
      },
      {
        "comments": "Name of JNDI entry at which the Factory can be found",
        "base": false,
        "name": "jndi_name",
        "validate": "string"
      },
      {
        "comments": "Password to use when connecting to the JMS provider",
        "base": false,
        "name": "password",
        "validate": "string"
      },
      {
        "comments": "If pub-sub (topic) style should be used or not.\nMandatory",
        "base": false,
        "name": "pub_sub",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "An optional array of Jar file names to load for the specified\nJMS provider. By using this option it is not necessary\nto put all the JMS Provider specific jar files into the\njava CLASSPATH prior to starting Logstash.",
        "base": false,
        "name": "require_jars",
        "validate": "array"
      },
      {
        "comments": "Username to connect to JMS provider with",
        "base": false,
        "name": "username",
        "validate": "string"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "Yaml config file",
        "base": false,
        "name": "yaml_file",
        "validate": "string"
      },
      {
        "comments": "Yaml config file section name\nFor some known examples, see: [Example jms.yml](https://github.com/reidmorrison/jruby-jms/blob/master/examples/jms.yml)",
        "base": false,
        "name": "yaml_section",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-geoip/master/lib/logstash/filters/geoip.rb",
    "name": "geoip",
    "type": "filter",
    "params": [
      {
        "comments": "The field containing the IP address or hostname to map via geoip. If\nthis field is an array, only the first value will be used.",
        "base": false,
        "name": "source",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The path to the GeoIP database file which Logstash should use. Country, City, ASN, ISP\nand organization databases are supported.\n\nIf not specified, this will default to the GeoLiteCity database that ships\nwith Logstash.\nUp-to-date databases can be downloaded from here: <https://dev.maxmind.com/geoip/legacy/geolite/>\nPlease be sure to download a legacy format database.",
        "base": false,
        "name": "database",
        "validate": "path"
      },
      {
        "comments": "An array of geoip fields to be included in the event.\n\nPossible fields depend on the database type. By default, all geoip fields\nare included in the event.\n\nFor the built-in GeoLiteCity database, the following are available:\n`city_name`, `continent_code`, `country_code2`, `country_code3`, `country_name`,\n`dma_code`, `ip`, `latitude`, `longitude`, `postal_code`, `region_name` and `timezone`.",
        "base": false,
        "name": "fields",
        "validate": "array"
      },
      {
        "comments": "GeoIP lookup is surprisingly expensive. This filter uses an LRU cache to take advantage of the fact that\nIPs agents are often found adjacent to one another in log files and rarely have a random distribution.\nThe higher you set this the more likely an item is to be in the cache and the faster this filter will run.\nHowever, if you set this too high you can use more memory than desired.\n\nExperiment with different values for this option to find the best performance for your dataset.\n\nThis MUST be set to a value > 0. There is really no reason to not want this behavior, the overhead is minimal\nand the speed gains are large.\n\nIt is important to note that this config value is global to the geoip_type. That is to say all instances of the geoip filter\nof the same geoip_type share the same cache. The last declared cache size will 'win'. The reason for this is that there would be no benefit\nto having multiple caches for different instances at different points in the pipeline, that would just increase the\nnumber of cache misses and waste memory.",
        "base": false,
        "name": "lru_cache_size",
        "validate": "number",
        "default": "1000"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Specify the field into which Logstash should store the geoip data.\nThis can be useful, for example, if you have `src\\_ip` and `dst\\_ip` fields and\nwould like the GeoIP information of both IPs.\n\nIf you save the data to a target field other than `geoip` and want to use the\n`geo\\_point` related functions in Elasticsearch, you need to alter the template\nprovided with the Elasticsearch output and configure the output to use the\nnew template.\n\nEven if you don't use the `geo\\_point` mapping, the `[target][location]` field\nis still valid GeoJSON.",
        "base": false,
        "name": "target",
        "validate": "string",
        "default": "geoip"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-elasticsearch/master/lib/logstash/inputs/elasticsearch.rb",
    "name": "elasticsearch",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "SSL Certificate Authority file",
        "base": false,
        "name": "ca_file",
        "validate": "path"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "If set, include Elasticsearch document information such as index, type, and\nthe id in the event.\n\nIt might be important to note, with regards to metadata, that if you're\ningesting documents with the intent to re-index them (or just update them)\nthat the `action` option in the elasticsearch output want's to know how to\nhandle those things. It can be dynamically assigned with a field\nadded to the metadata.\n\nExample\n[source, ruby]\n    input {\n      elasticsearch {\n        hosts => \"es.production.mysite.org\"\n        index => \"mydata-2018.09.*\"\n        query => \"*\"\n        size => 500\n        scroll => \"5m\"\n        docinfo => true\n      }\n    }\n    output {\n      elasticsearch {\n        index => \"copy-of-production.%{[@metadata][_index]}\"\n        index_type => \"%{[@metadata][_type]}\"\n        document_id => \"%{[@metadata][_id]}\"\n      }\n    }\n",
        "base": false,
        "name": "docinfo",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "List of document metadata to move to the `docinfo_target` field\nTo learn more about Elasticsearch metadata fields read\nhttp://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_document_metadata.html",
        "base": false,
        "name": "docinfo_fields",
        "validate": "array",
        "default": "[\"_index\",\"_type\",\"_id\"]"
      },
      {
        "comments": "Where to move the Elasticsearch document information by default we use the @metadata field.",
        "base": false,
        "name": "docinfo_target",
        "validate=> string": null,
        "default": "LogStashEventMETADATA"
      },
      {
        "comments": "List of elasticsearch hosts to use for querying.\neach host can be either IP, HOST, IP:port or HOST:port\nport defaults to 9200",
        "base": false,
        "name": "hosts",
        "validate": "array"
      },
      {
        "comments": "The index or alias to search.",
        "base": false,
        "name": "index",
        "validate": "string",
        "default": "logstash-*"
      },
      {
        "comments": "Basic Auth - password",
        "base": false,
        "name": "password",
        "validate": "password"
      },
      {
        "comments": "The query to be executed.",
        "base": false,
        "name": "query",
        "validate": "string",
        "default": "\"{\"query\" { \"match_all\" {} } }\""
      },
      {
        "comments": "Enable the Elasticsearch \"scan\" search type.  This will disable\nsorting but increase speed and performance.",
        "base": false,
        "name": "scan",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "This parameter controls the keepalive time in seconds of the scrolling\nrequest and initiates the scrolling process. The timeout applies per\nround trip (i.e. between the previous scan scroll request, to the next).",
        "base": false,
        "name": "scroll",
        "validate": "string",
        "default": "1m"
      },
      {
        "comments": "This allows you to set the maximum number of hits returned per scroll.",
        "base": false,
        "name": "size",
        "validate": "number",
        "default": "1000"
      },
      {
        "comments": "SSL",
        "base": false,
        "name": "ssl",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Basic Auth - username",
        "base": false,
        "name": "user",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-jira/master/lib/logstash/outputs/jira.rb",
    "name": "jira",
    "type": "output",
    "params": [
      {
        "comments": "JIRA Issuetype number",
        "base": false,
        "name": "issuetypeid",
        "validate": "string",
        "required": true
      },
      {
        "comments": "",
        "base": false,
        "name": "password",
        "validate": "string",
        "required": true
      },
      {
        "comments": "JIRA Priority",
        "base": false,
        "name": "priority",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Javalicious has no proxy support\n#\nJIRA Project number",
        "base": false,
        "name": "projectid",
        "validate": "string",
        "required": true
      },
      {
        "comments": "JIRA Summary\n\nTruncated and appended with '...' if longer than 255 characters.",
        "base": false,
        "name": "summary",
        "validate": "string",
        "required": true
      },
      {
        "comments": "",
        "base": false,
        "name": "username",
        "validate": "string",
        "required": true
      },
      {
        "comments": "JIRA Reporter",
        "base": false,
        "name": "assignee",
        "validate": "string"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The hostname to send logs to. This should target your JIRA server\nand has to have the REST interface enabled",
        "base": false,
        "name": "host",
        "validate": "string"
      },
      {
        "comments": "JIRA Reporter",
        "base": false,
        "name": "reporter",
        "validate": "string"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-uuid/master/lib/logstash/filters/uuid.rb",
    "name": "uuid",
    "type": "filter",
    "params": [
      {
        "comments": "Add a UUID to a field.\n\nExample:\n[source,ruby]\n    filter {\n      uuid {\n        target => \"@uuid\"\n      }\n    }",
        "base": false,
        "name": "target",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If the value in the field currently (if any) should be overridden\nby the generated UUID. Defaults to `false` (i.e. if the field is\npresent, with ANY value, it won't be overridden)\n\nExample:\n[source,ruby]\n   filter {\n      uuid {\n        target    => \"@uuid\"\n        overwrite => true\n      }\n   }",
        "base": false,
        "name": "overwrite",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-influxdb/master/lib/logstash/outputs/influxdb.rb",
    "name": "influxdb",
    "type": "output",
    "params": [
      {
        "comments": "Hash of key/value pairs representing data points to send to the named database\nExample: `{'column1' => 'value1', 'column2' => 'value2'}`\n\nEvents for the same series will be batched together where possible\nBoth keys and values support sprintf formatting",
        "base": false,
        "name": "data_points",
        "validate": "hash",
        "default": "{}",
        "required": true
      },
      {
        "comments": "The hostname or IP address to reach your InfluxDB instance",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The password for the user who access to the named database",
        "base": false,
        "name": "password",
        "validate": "password",
        "default": "nil",
        "required": true
      },
      {
        "comments": "The user who has access to the named database",
        "base": false,
        "name": "user",
        "validate": "string",
        "default": "nil",
        "required": true
      },
      {
        "comments": "Allow the override of the `time` column in the event?\n\nBy default any column with a name of `time` will be ignored and the time will\nbe determined by the value of `@timestamp`.\n\nSetting this to `true` allows you to explicitly set the `time` column yourself\n\nNote: **`time` must be an epoch value in either seconds, milliseconds or microseconds**",
        "base": false,
        "name": "allow_time_override",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Allow value coercion\n\nthis will attempt to convert data point values to the appropriate type before posting\notherwise sprintf-filtered numeric values could get sent as strings\nformat is `{'column_name' => 'datatype'}`\n\ncurrently supported datatypes are `integer` and `float`\n",
        "base": false,
        "name": "coerce_values",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The database to write",
        "base": false,
        "name": "db",
        "validate": "string",
        "default": "stats"
      },
      {
        "comments": "This setting controls how many events will be buffered before sending a batch\nof events. Note that these are only batched for the same series",
        "base": false,
        "name": "flush_size",
        "validate": "number",
        "default": "100"
      },
      {
        "comments": "The amount of time since last flush before a flush is forced.\n\nThis setting helps ensure slow event rates don't get stuck in Logstash.\nFor example, if your `flush_size` is 100, and you have received 10 events,\nand it has been more than `idle_flush_time` seconds since the last flush,\nlogstash will flush those 10 events automatically.\n\nThis helps keep both fast and slow log streams moving along in\nnear-real-time.",
        "base": false,
        "name": "idle_flush_time",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "The port for InfluxDB",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "8086"
      },
      {
        "comments": "Series name - supports sprintf formatting",
        "base": false,
        "name": "series",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "Set the level of precision of `time`\n\nonly useful when overriding the time value",
        "base": false,
        "name": "time_precision",
        "validate": [
          "m",
          "s",
          "u"
        ],
        "default": "s"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-tcp/master/lib/logstash/outputs/tcp.rb",
    "name": "tcp",
    "type": "output",
    "params": [
      {
        "comments": "When mode is `server`, the address to listen on.\nWhen mode is `client`, the address to connect to.",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "When mode is `server`, the port to listen on.\nWhen mode is `client`, the port to connect to.",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The format to use when writing events to the file. This value\nsupports any string and can include `%{name}` and other dynamic\nstrings.\n\nIf this setting is omitted, the full json representation of the\nevent will be written as a single line.",
        "base": false,
        "name": "message_format",
        "validate": "string",
        "deprecated": true
      },
      {
        "comments": "Mode to operate in. `server` listens for client connections,\n`client` connects to a server.",
        "base": false,
        "name": "mode",
        "validate": [
          "server",
          "client"
        ],
        "default": "client"
      },
      {
        "comments": "When connect failed,retry interval in sec.",
        "base": false,
        "name": "reconnect_interval",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-webhdfs/master/lib/logstash/outputs/webhdfs.rb",
    "name": "webhdfs",
    "type": "output",
    "params": [
      {
        "comments": "The server name for webhdfs/httpfs connections.",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The path to the file to write to. Event fields can be used here,\nas well as date fields in the joda time format, e.g.:\n`/user/logstash/dt=%{+YYYY-MM-dd}/%{@source_host}-%{+HH}.log`",
        "base": false,
        "name": "path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The Username for webhdfs.",
        "base": false,
        "name": "user",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Compress output. One of ['none', 'snappy', 'gzip']",
        "base": false,
        "name": "compression",
        "validate": [
          "none",
          "snappy",
          "gzip"
        ],
        "default": "none"
      },
      {
        "comments": "Sending data to webhdfs if event count is above, even if `store_interval_in_secs` is not reached.",
        "base": false,
        "name": "flush_size",
        "validate": "number",
        "default": "500"
      },
      {
        "comments": "Sending data to webhdfs in x seconds intervals.",
        "base": false,
        "name": "idle_flush_time",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "The format to use when writing events to the file. This value\nsupports any string and can include `%{name}` and other dynamic\nstrings.\n\nIf this setting is omitted, the full json representation of the\nevent will be written as a single line.",
        "base": false,
        "name": "message_format",
        "validate": "string"
      },
      {
        "comments": "WebHdfs open timeout, default 30s.",
        "base": false,
        "name": "open_timeout",
        "validate": "number",
        "default": "30"
      },
      {
        "comments": "The server port for webhdfs/httpfs connections.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "50070"
      },
      {
        "comments": "The WebHdfs read timeout, default 30s.",
        "base": false,
        "name": "read_timeout",
        "validate": "number",
        "default": "30"
      },
      {
        "comments": "How long should we wait between retries.",
        "base": false,
        "name": "retry_interval",
        "validate": "number",
        "default": "0.5"
      },
      {
        "comments": "Retry some known webhdfs errors. These may be caused by race conditions when appending to same file, etc.",
        "base": false,
        "name": "retry_known_errors",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "How many times should we retry. If retry_times is exceeded, an error will be logged and the event will be discarded.",
        "base": false,
        "name": "retry_times",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "Set snappy chunksize. Only neccessary for stream format. Defaults to 32k. Max is 65536\n@see http://code.google.com/p/snappy/source/browse/trunk/framing_format.txt",
        "base": false,
        "name": "snappy_bufsize",
        "validate": "number",
        "default": "32768"
      },
      {
        "comments": "Set snappy format. One of \"stream\", \"file\". Set to stream to be hive compatible.",
        "base": false,
        "name": "snappy_format",
        "validate": [
          "stream",
          "file"
        ],
        "default": "stream"
      },
      {
        "comments": "Use httpfs mode if set to true, else webhdfs.",
        "base": false,
        "name": "use_httpfs",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-gemfire/master/lib/logstash/outputs/gemfire.rb",
    "name": "gemfire",
    "type": "output",
    "params": [
      {
        "comments": "Your client cache name",
        "base": false,
        "name": "cache_name",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "The path to a GemFire client cache XML file.\n\nExample:\n[source,xml]\n     <client-cache>\n       <pool name=\"client-pool\">\n           <locator host=\"localhost\" port=\"31331\"/>\n       </pool>\n       <region name=\"Logstash\">\n           <region-attributes refid=\"CACHING_PROXY\" pool-name=\"client-pool\" >\n           </region-attributes>\n       </region>\n     </client-cache>\n",
        "base": false,
        "name": "cache_xml_file",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "A sprintf format to use when building keys",
        "base": false,
        "name": "key_format",
        "validate": "string",
        "default": "%{host}-%{@timestamp}"
      },
      {
        "comments": "The region name",
        "base": false,
        "name": "region_name",
        "validate": "string",
        "default": "Logstash"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-datadog_metrics/master/lib/logstash/outputs/datadog_metrics.rb",
    "name": "datadog_metrics",
    "type": "output",
    "params": [
      {
        "comments": "Your DatadogHQ API key. https://app.datadoghq.com/account/settings#api",
        "base": false,
        "name": "api_key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Set any custom tags for this event,\ndefault are the Logstash tags if any.",
        "base": false,
        "name": "dd_tags",
        "validate": "array"
      },
      {
        "comments": "The name of the device that produced the metric.",
        "base": false,
        "name": "device",
        "validate": "string",
        "default": "%{metric_device}"
      },
      {
        "comments": "The name of the host that produced the metric.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "%{host}"
      },
      {
        "comments": "The name of the time series.",
        "base": false,
        "name": "metric_name",
        "validate": "string",
        "default": "%{metric_name}"
      },
      {
        "comments": "The type of the metric.",
        "base": false,
        "name": "metric_type",
        "validate": [
          "gauge",
          "counter",
          "%{metric_type}"
        ],
        "default": "%{metric_type}"
      },
      {
        "comments": "The value.",
        "base": false,
        "name": "metric_value",
        "default": "%{metric_value}"
      },
      {
        "comments": "How many events to queue before flushing to Datadog\nprior to schedule set in `@timeframe`",
        "base": false,
        "name": "queue_size",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "How often (in seconds) to flush queued events to Datadog",
        "base": false,
        "name": "timeframe",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-sqs/master/lib/logstash/outputs/sqs.rb",
    "name": "sqs",
    "type": "output",
    "params": [
      {
        "comments": "Name of SQS queue to push messages into. Note that this is just the name of the queue, not the URL or ARN.",
        "base": false,
        "name": "queue",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Set to true if you want send messages to SQS in batches with `batch_send`\nfrom the amazon sdk",
        "base": false,
        "name": "batch",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "If `batch` is set to true, the number of events we queue up for a `batch_send`.",
        "base": false,
        "name": "batch_events",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "If `batch` is set to true, the maximum amount of time between `batch_send` commands when there are pending events to flush.",
        "base": false,
        "name": "batch_timeout",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-http/master/lib/logstash/inputs/http.rb",
    "name": "http",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Apply specific codecs for specific content types.\nThe default codec will be applied only after this list is checked\nand no codec for the request's content-type is found",
        "base": false,
        "name": "additional_codecs",
        "validate": "hash",
        "default": "{ \"application/json\" => \"json\" }"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The host or ip to bind",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "The JKS keystore to validate the client's certificates",
        "base": false,
        "name": "keystore",
        "validate": "path"
      },
      {
        "comments": "Set the truststore password",
        "base": false,
        "name": "keystore_password",
        "validate": "password"
      },
      {
        "comments": "Password for basic authorization",
        "base": false,
        "name": "password",
        "validate": "password",
        "required": false
      },
      {
        "comments": "The TCP port to bind to",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "8080"
      },
      {
        "comments": "SSL Configurations\n\nEnable SSL",
        "base": false,
        "name": "ssl",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Maximum number of threads to use",
        "base": false,
        "name": "threads",
        "validate": "number",
        "default": "4"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Username for basic authorization",
        "base": false,
        "name": "user",
        "validate": "string",
        "required": false
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-example/master/lib/logstash/inputs/example.rb",
    "name": "example",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Set how frequently messages should be sent.\n\nThe default, `1`, means send a message every second.",
        "base": false,
        "name": "interval",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "The message string to use in the event.",
        "base": false,
        "name": "message",
        "validate": "string",
        "default": "Hello World!"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-unique/master/lib/logstash/filters/unique.rb",
    "name": "unique",
    "type": "filter",
    "params": [
      {
        "comments": "The fields on which to run the unique filter.",
        "base": false,
        "name": "fields",
        "validate": "array",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-jdbc/master/lib/logstash/inputs/jdbc.rb",
    "name": "jdbc",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Whether the previous run state should be preserved",
        "base": false,
        "name": "clean_run",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Path to file with last run time",
        "base": false,
        "name": "last_run_metadata_path",
        "validate": "string",
        "default": "\"#{ENV[\"HOME\"]}/.logstash_jdbc_last_run\""
      },
      {
        "comments": "Whether to force the lowercasing of identifier fields",
        "base": false,
        "name": "lowercase_column_names",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Hash of query parameter, for example `{ \"target_id\" => \"321\" }`",
        "base": false,
        "name": "parameters",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Whether to save state or not in last_run_metadata_path",
        "base": false,
        "name": "record_last_run",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Schedule of when to periodically run statement, in Cron format\nfor example: \"* * * * *\" (execute query every minute, on the minute)\n\nThere is no schedule by default. If no schedule is given, then the statement is run\nexactly once.",
        "base": false,
        "name": "schedule",
        "validate": "string"
      },
      {
        "comments": "Statement to execute\n\nTo use parameters, use named parameter syntax.\nFor example:\n\n[source, ruby]\n----------------------------------\n\"SELECT * FROM MYTABLE WHERE id = :target_id\"\n----------------------------------\n\nhere, \":target_id\" is a named parameter. You can configure named parameters\nwith the `parameters` setting.",
        "base": false,
        "name": "statement",
        "validate": "string"
      },
      {
        "comments": "Path of file containing statement to execute",
        "base": false,
        "name": "statement_filepath",
        "validate": "path"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "If tracking column value rather than timestamp, the column whose value is to be tracked",
        "base": false,
        "name": "tracking_column",
        "validate": "string"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Use an incremental column value rather than a timestamp",
        "base": false,
        "name": "use_column_value",
        "validate": "boolean",
        "default": "false"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-metricize/master/lib/logstash/filters/metricize.rb",
    "name": "metricize",
    "type": "filter",
    "params": [
      {
        "comments": "A new matrics event will be created for each metric field in this list.\nAll fields in this list will be removed from generated events.",
        "base": false,
        "name": "metrics",
        "validate": "array",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Flag indicating whether the original event should be dropped or not.",
        "base": false,
        "name": "drop_original_event",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Name of the field the metric name will be written to.",
        "base": false,
        "name": "metric_field_name",
        "validate": "string",
        "default": "metric"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Name of the field the metric value will be written to.",
        "base": false,
        "name": "value_field_name",
        "validate": "string",
        "default": "value"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-heartbeat/master/lib/logstash/inputs/heartbeat.rb",
    "name": "heartbeat",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "How many times to iterate.\nThis is typically used only for testing purposes.",
        "base": false,
        "name": "count",
        "validate": "number",
        "default": "-1"
      },
      {
        "comments": "Set how frequently messages should be sent.\n\nThe default, `60`, means send a message every 60 seconds.",
        "base": false,
        "name": "interval",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "The message string to use in the event.\n\nIf you set this to `epoch` then this plugin will use the current\ntimestamp in unix timestamp (which is by definition, UTC).  It will\noutput this value into a field called `clock`\n\nIf you set this to `sequence` then this plugin will send a sequence of\nnumbers beginning at 0 and incrementing each interval.  It will\noutput this value into a field called `clock`\n\nOtherwise, this value will be used verbatim as the event message. It\nwill output this value into a field called `message`",
        "base": false,
        "name": "message",
        "validate": "string",
        "default": "ok"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-beats/master/lib/logstash/inputs/beats.rb",
    "name": "beats",
    "type": "input",
    "params": [
      {
        "comments": "The port to listen on.",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The number of seconds before we raise a timeout,\nthis option is useful to control how much time to wait if something is blocking the pipeline.",
        "base": false,
        "name": "congestion_threshold",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "The IP address to listen on.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "Events are by default send in plain text, you can\nenable encryption by using `ssl` to true and configuring\nthe `ssl_certificate` and `ssl_key` options.",
        "base": false,
        "name": "ssl",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "SSL certificate to use.",
        "base": false,
        "name": "ssl_certificate",
        "validate": "path"
      },
      {
        "comments": "SSL key to use.",
        "base": false,
        "name": "ssl_key",
        "validate": "path"
      },
      {
        "comments": "SSL key passphrase to use.",
        "base": false,
        "name": "ssl_key_passphrase",
        "validate": "password"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "This is the default field that the specified codec will be applied",
        "base": false,
        "name": "target_field_for_codec",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-example/master/lib/logstash/outputs/example.rb",
    "name": "example",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-salesforce/master/lib/logstash/inputs/salesforce.rb",
    "name": "salesforce",
    "type": "input",
    "params": [
      {
        "comments": "Consumer Key for authentication. You must set up a new SFDC\nconnected app with oath to use this output. More information\ncan be found here:\nhttps://help.salesforce.com/apex/HTViewHelpDoc?id=connected_app_create.htm",
        "base": false,
        "name": "client_id",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Consumer Secret from your oauth enabled connected app",
        "base": false,
        "name": "client_secret",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The password used to login to sfdc",
        "base": false,
        "name": "password",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The security token for this account. For more information about\ngenerting a security token, see:\nhttps://help.salesforce.com/apex/HTViewHelpDoc?id=user_security_token.htm",
        "base": false,
        "name": "security_token",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The name of the salesforce object you are creating or updating",
        "base": false,
        "name": "sfdc_object_name",
        "validate": "string",
        "required": true
      },
      {
        "comments": "A valid salesforce user name, usually your email address.\nUsed for authentication and will be the user all objects\nare created or modified by",
        "base": false,
        "name": "username",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "By default, this uses the default Restforce API version.\nTo override this, set this to something like \"32.0\" for example",
        "base": false,
        "name": "api_version",
        "validate": "string",
        "required": false
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "These are the field names to return in the Salesforce query\nIf this is empty, all fields are returned.",
        "base": false,
        "name": "sfdc_fields",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "These options will be added to the WHERE clause in the\nSOQL statement. Additional fields can be filtered on by\nadding field1 = value1 AND field2 = value2 AND...",
        "base": false,
        "name": "sfdc_filters",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Setting this to true will convert SFDC's NamedFields__c to named_fields__c",
        "base": false,
        "name": "to_underscores",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Set this to true to connect to a sandbox sfdc instance\nlogging in through test.salesforce.com",
        "base": false,
        "name": "use_test_sandbox",
        "validate": "boolean",
        "default": "false"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-neo4j/master/lib/logstash/outputs/neo4j.rb",
    "name": "neo4j",
    "type": "output",
    "params": [
      {
        "comments": "The path within your file system where the neo4j database is located",
        "base": false,
        "name": "path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-elasticsearch_java/master/lib/logstash/outputs/elasticsearch_java.rb",
    "name": "elasticsearch_java",
    "type": "output",
    "params": [
      {
        "comments": "The name/address of the host to bind to for Elasticsearch clustering. Equivalent to the Elasticsearch option 'network.host'\noption.\nThis MUST be set for either protocol to work (node or transport)! The internal Elasticsearch node\nwill bind to this ip. This ip MUST be reachable by all nodes in the Elasticsearch cluster",
        "base": false,
        "name": "network_host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The Elasticsearch action to perform. Valid actions are:\n\n- index: indexes a document (an event from Logstash).\n- delete: deletes a document by id (An id is required for this action)\n- create: indexes a document, fails if a document by that id already exists in the index.\n- update: updates a document by id. Update has a special case where you can upsert -- update a\n  document if not already present. See the `upsert` option\n- create_unless_exists: create the document unless it already exists, in which case do nothing.\n\nFor more details on actions, check out the http://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html[Elasticsearch bulk API documentation]",
        "base": false,
        "name": "action",
        "validate": "%w(index delete create update create_unless_exists)",
        "default": "index"
      },
      {
        "comments": "The name of your cluster if you set it on the Elasticsearch side. Useful\nfor discovery when using `node` or `transport` protocols.\nBy default, it looks for a cluster named 'elasticsearch'.\nEquivalent to the Elasticsearch option 'cluster.name'",
        "base": false,
        "name": "cluster",
        "validate": "string"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "This setting no longer does anything. It exists to keep config validation\nfrom failing. It will be removed in future versions.",
        "base": false,
        "name": "max_inflight_requests",
        "validate": "number",
        "default": 50,
        "deprecated": true
      },
      {
        "comments": "The node name Elasticsearch will use when joining a cluster.\n\nBy default, this is generated internally by the ES client.",
        "base": false,
        "name": "node_name",
        "validate": "string"
      },
      {
        "comments": "Choose the protocol used to talk to Elasticsearch.\n\nThe 'node' protocol (default) will connect to the cluster as a normal Elasticsearch\nnode (but will not store data). If you use the `node` protocol, you must permit\nbidirectional communication on the port 9300 (or whichever port you have\nconfigured).\n\nIf you do not specify the `host` parameter, it will use  multicast for http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html[Elasticsearch discovery].  While this may work in a test/dev environment where multicast is enabled in\nElasticsearch, we strongly recommend http://www.elastic.co/guide/en/elasticsearch/guide/current/important-configuration-changes.html#unicast[using unicast]\nin Elasticsearch.  To connect to an Elasticsearch cluster with unicast,\nyou must include the `host` parameter (see relevant section above).\n\nThe 'transport' protocol will connect to the host you specify and will\nnot show up as a 'node' in the Elasticsearch cluster. This is useful\nin situations where you cannot permit connections outbound from the\nElasticsearch cluster to this Logstash server.\n\nAll protocols will use bulk requests when talking to Elasticsearch.",
        "base": false,
        "name": "protocol",
        "validate": [
          "node",
          "transport"
        ],
        "default": "transport"
      },
      {
        "comments": "Enable cluster sniffing (transport only).\nAsks host for the list of all cluster nodes and adds them to the hosts list\nEquivalent to the Elasticsearch option 'client.transport.sniff'",
        "base": false,
        "name": "sniffing",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "This sets the local port to bind to. Equivalent to the Elasticsrearch option 'transport.tcp.port'",
        "base": false,
        "name": "transport_tcp_port",
        "validate": "number"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-riemann/master/lib/logstash/outputs/riemann.rb",
    "name": "riemann",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "\nEnable debugging output?",
        "base": false,
        "name": "debug",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The address of the Riemann server.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "If set to true automatically map all logstash defined fields to riemann event fields.\nAll nested logstash fields will be mapped to riemann fields containing all parent keys\nseparated by dots and the deepest value.\n\nAs an example, the logstash event:\n[source,ruby]\n   {\n     \"@timestamp\":\"2013-12-10T14:36:26.151+0000\",\n     \"@version\": 1,\n     \"message\":\"log message\",\n     \"host\": \"host.domain.com\",\n     \"nested_field\": {\n                       \"key\": \"value\"\n                     }\n   }\nIs mapped to this riemann event:\n[source,ruby]\n  {\n    :time 1386686186,\n    :host host.domain.com,\n    :message log message,\n    :nested_field.key value\n  }\n\nIt can be used in conjunction with or independent of the riemann_event option.\nWhen used with the riemann_event any duplicate keys receive their value from\nriemann_event instead of the logstash event itself.",
        "base": false,
        "name": "map_fields",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The port to connect to on your Riemann server.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "5555"
      },
      {
        "comments": "The protocol to use\nUDP is non-blocking\nTCP is blocking\n\nLogstash's default output behaviour\nis to never lose events\nAs such, we use tcp as default here",
        "base": false,
        "name": "protocol",
        "validate": [
          "tcp",
          "udp"
        ],
        "default": "tcp"
      },
      {
        "comments": "A Hash to set Riemann event fields\n(http://riemann.io/concepts.html).\n\nThe following event fields are supported:\n`description`, `state`, `metric`, `ttl`, `service`\n\nTags found on the Logstash event will automatically be added to the\nRiemann event.\n\nAny other field set here will be passed to Riemann as an event attribute.\n\nExample:\n[source,ruby]\n    riemann {\n        riemann_event => {\n            \"metric\"  => \"%{metric}\"\n            \"service\" => \"%{service}\"\n        }\n    }\n\n`metric` and `ttl` values will be coerced to a floating point value.\nValues which cannot be coerced will zero (0.0).\n\n`description`, by default, will be set to the event message\nbut can be overridden here.",
        "base": false,
        "name": "riemann_event",
        "validate": "hash"
      },
      {
        "comments": "The name of the sender.\nThis sets the `host` value\nin the Riemann event",
        "base": false,
        "name": "sender",
        "validate": "string",
        "default": "%{host}"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-aggregate/master/lib/logstash/filters/aggregate.rb",
    "name": "aggregate",
    "type": "filter",
    "params": [
      {
        "comments": "The code to execute to update map, using current event.\n\nOr on the contrary, the code to execute to update event, using current map.\n\nYou will have a 'map' variable and an 'event' variable available (that is the event itself).\n\nExample value : \"map['sql_duration'] += event['duration']\"",
        "base": false,
        "name": "code",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The expression defining task ID to correlate logs.\n\nThis value must uniquely identify the task in the system.\n\nExample value : \"%{application}%{my_task_id}\"",
        "base": false,
        "name": "task_id",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Tell the filter that task is ended, and therefore, to delete map after code execution.",
        "base": false,
        "name": "end_of_task",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Tell the filter what to do with aggregate map.\n\n`create`: create the map, and execute the code only if map wasn't created before\n\n`update`: doesn't create the map, and execute the code only if map was created before\n\n`create_or_update`: create the map if it wasn't created before, execute the code in all cases",
        "base": false,
        "name": "map_action",
        "validate": "string",
        "default": "create_or_update"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The amount of seconds after a task \"end event\" can be considered lost.\n\nThe task \"map\" is evicted.\n\nDefault value (`0`) means no timeout so no auto eviction.",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "required": false,
        "default": "0"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-neo4j/master/lib/logstash/inputs/neo4j.rb",
    "name": "neo4j",
    "type": "input",
    "params": [
      {
        "comments": "The path within your file system where the neo4j database is located",
        "base": false,
        "name": "path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Cypher query used to retrieve data from the neo4j database, this statement\nshould looks like something like this:\n\nMATCH (p:`Person`)-->(m:`Movie`) WHERE m.released = 2005 RETURN *\n",
        "base": false,
        "name": "query",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Schedule of when to periodically run statement, in Cron format\nfor example: \"* * * * *\" (execute query every minute, on the minute).\nIf this variable is not specified then this input will run only once",
        "base": false,
        "name": "schedule",
        "validate": "string"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-nagios/master/lib/logstash/outputs/nagios.rb",
    "name": "nagios",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The full path to your Nagios command file.",
        "base": false,
        "name": "commandfile",
        "default": "/var/lib/nagios3/rw/nagios.cmd"
      },
      {
        "comments": "The Nagios check level. Should be one of 0=OK, 1=WARNING, 2=CRITICAL,\n3=UNKNOWN. Defaults to 2 - CRITICAL.",
        "base": false,
        "name": "nagios_level",
        "validate": [
          "0",
          "1",
          "2",
          "3"
        ],
        "default": "2"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-null/master/lib/logstash/outputs/null.rb",
    "name": "null",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-circonus/master/lib/logstash/outputs/circonus.rb",
    "name": "circonus",
    "type": "output",
    "params": [
      {
        "comments": "Annotations\nRegisters an annotation with Circonus\nThe only required field is `title` and `description`.\n`start` and `stop` will be set to `event[\"@timestamp\"]`\nYou can add any other optional annotation values as well.\nAll values will be passed through `event.sprintf`\n\nExample:\n[source,ruby]\n  [\"title\":\"Logstash event\", \"description\":\"Logstash event for %{host}\"]\nor\n[source,ruby]\n  [\"title\":\"Logstash event\", \"description\":\"Logstash event for %{host}\", \"parent_id\", \"1\"]",
        "base": false,
        "name": "annotation",
        "validate": "hash",
        "required": true,
        "default": "{}"
      },
      {
        "comments": "Your Circonus API Token",
        "base": false,
        "name": "api_token",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Your Circonus App name\nThis will be passed through `event.sprintf`\nso variables are allowed here:\n\nExample:\n `app_name => \"%{myappname}\"`",
        "base": false,
        "name": "app_name",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-udp/master/lib/logstash/inputs/udp.rb",
    "name": "udp",
    "type": "input",
    "params": [
      {
        "comments": "The port which logstash will listen on. Remember that ports less\nthan 1024 (privileged ports) may require root or elevated privileges to use.",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The maximum packet size to read from the network",
        "base": false,
        "name": "buffer_size",
        "validate": "number",
        "default": "8192"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The address which logstash will listen on.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "This is the number of unprocessed UDP packets you can hold in memory\nbefore packets will start dropping.",
        "base": false,
        "name": "queue_size",
        "validate": "number",
        "default": "2000"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Number of threads processing packets",
        "base": false,
        "name": "workers",
        "validate": "number",
        "default": "2"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-ganglia/master/lib/logstash/outputs/ganglia.rb",
    "name": "ganglia",
    "type": "output",
    "params": [
      {
        "comments": "The metric to use. This supports dynamic strings like `%{host}`",
        "base": false,
        "name": "metric",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The value to use. This supports dynamic strings like `%{bytes}`\nIt will be coerced to a floating point value. Values which cannot be\ncoerced will zero (0)",
        "base": false,
        "name": "value",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Metric group",
        "base": false,
        "name": "group",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "The address of the ganglia server.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "Lifetime in seconds of this metric",
        "base": false,
        "name": "lifetime",
        "validate": "number",
        "default": "300"
      },
      {
        "comments": "Maximum time in seconds between gmetric calls for this metric.",
        "base": false,
        "name": "max_interval",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "The type of value for this metric.",
        "base": false,
        "name": "metric_type",
        "validate": "%w{string int8 uint8 int16 uint16 int32 uint32 float double},"
      },
      {
        "comments": "The port to connect on your ganglia server.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "8649"
      },
      {
        "comments": "Metric slope, represents metric behavior",
        "base": false,
        "name": "slope",
        "validate": "%w{zero positive negative both unspecified}",
        "default": "both"
      },
      {
        "comments": "Gmetric units for metric, such as \"kb/sec\" or \"ms\" or whatever unit\nthis metric uses.",
        "base": false,
        "name": "units",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-opentsdb/master/lib/logstash/outputs/opentsdb.rb",
    "name": "opentsdb",
    "type": "output",
    "params": [
      {
        "comments": "The metric(s) to use. This supports dynamic strings like %{source_host}\nfor metric names and also for values. This is an array field with key\nof the metric name, value of the metric value, and multiple tag,values . Example:\n[source,ruby]\n    [\n      \"%{host}/uptime\",\n      %{uptime_1m} \" ,\n      \"hostname\" ,\n      \"%{host}\n      \"anotherhostname\" ,\n      \"%{host}\n    ]\n\nThe value will be coerced to a floating point value. Values which cannot be\ncoerced will zero (0)",
        "base": false,
        "name": "metrics",
        "validate": "array",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Enable debugging.",
        "base": false,
        "name": "debug",
        "validate": "boolean",
        "default": false,
        "deprecated": "This setting was never used by this plugin. It will be removed soon."
      },
      {
        "comments": "The address of the opentsdb server.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "The port to connect on your graphite server.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "4242"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-rackspace/master/lib/logstash/outputs/rackspace.rb",
    "name": "rackspace",
    "type": "output",
    "params": [
      {
        "comments": "Rackspace Cloud API Key",
        "base": false,
        "name": "api_key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Rackspace Cloud Username",
        "base": false,
        "name": "username",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Rackspace Queue Name",
        "base": false,
        "name": "queue",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "Rackspace region\nord, dfw, lon, syd, etc",
        "base": false,
        "name": "region",
        "validate": "string",
        "default": "dfw"
      },
      {
        "comments": "time for item to live in queue",
        "base": false,
        "name": "ttl",
        "validate": "number",
        "default": "360"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-elasticsearch/master/lib/logstash/outputs/elasticsearch.rb",
    "name": "elasticsearch",
    "type": "output",
    "params": [
      {
        "comments": "The Elasticsearch action to perform. Valid actions are:\n\n- index: indexes a document (an event from Logstash).\n- delete: deletes a document by id (An id is required for this action)\n- create: indexes a document, fails if a document by that id already exists in the index.\n- update: updates a document by id. Update has a special case where you can upsert -- update a\n  document if not already present. See the `upsert` option\n- A sprintf style string to change the action based on the content of the event. The value `%{[foo]}`\n  would use the foo field for the action\n\nFor more details on actions, check out the http://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html[Elasticsearch bulk API documentation]",
        "base": false,
        "name": "action",
        "validate": "string",
        "default": "index"
      },
      {
        "comments": "The .cer or .pem file to validate the server's certificate",
        "base": false,
        "name": "cacert",
        "validate": "path"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The keystore used to present a certificate to the server.\nIt can be either .jks or .p12",
        "base": false,
        "name": "keystore",
        "validate": "path"
      },
      {
        "comments": "Set the truststore password",
        "base": false,
        "name": "keystore_password",
        "validate": "password"
      },
      {
        "comments": "Password to authenticate to a secure Elasticsearch cluster",
        "base": false,
        "name": "password",
        "validate": "password"
      },
      {
        "comments": "HTTP Path at which the Elasticsearch server lives. Use this if you must run Elasticsearch behind a proxy that remaps\nthe root path for the Elasticsearch HTTP API lives.",
        "base": false,
        "name": "path",
        "validate": "string",
        "default": "/"
      },
      {
        "comments": "Set the address of a forward HTTP proxy.\nCan be either a string, such as `http://localhost:123` or a hash in the form\nof `{host: 'proxy.org' port: 80 scheme: 'http'}`.\nNote, this is NOT a SOCKS proxy, but a plain HTTP proxy",
        "base": false,
        "name": "proxy"
      },
      {
        "comments": "This setting asks Elasticsearch for the list of all cluster nodes and adds them to the hosts list.\nNote: This will return ALL nodes with HTTP enabled (including master nodes!). If you use\nthis with master nodes, you probably want to disable HTTP on them by setting\n`http.enabled` to false in their elasticsearch.yml. You can either use the `sniffing` option or\nmanually enter multiple Elasticsearch hosts using the `hosts` parameter.",
        "base": false,
        "name": "sniffing",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "How long to wait, in seconds, between sniffing attempts",
        "base": false,
        "name": "sniffing_delay",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "Enable SSL/TLS secured communication to Elasticsearch cluster",
        "base": false,
        "name": "ssl",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Option to validate the server's certificate. Disabling this severely compromises security.\nFor more information on disabling certificate verification please read\nhttps://www.cs.utexas.edu/~shmat/shmat_ccs12.pdf",
        "base": false,
        "name": "ssl_certificate_verification",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Set the timeout for network operations and requests sent Elasticsearch. If\na timeout occurs, the request will be retried.",
        "base": false,
        "name": "timeout",
        "validate": "number"
      },
      {
        "comments": "The JKS truststore to validate the server's certificate.\nUse either `:truststore` or `:cacert`",
        "base": false,
        "name": "truststore",
        "validate": "path"
      },
      {
        "comments": "Set the truststore password",
        "base": false,
        "name": "truststore_password",
        "validate": "password"
      },
      {
        "comments": "Username to authenticate to a secure Elasticsearch cluster",
        "base": false,
        "name": "user",
        "validate": "string"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-udp/master/lib/logstash/outputs/udp.rb",
    "name": "udp",
    "type": "output",
    "params": [
      {
        "comments": "The address to send messages to",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The port to send messages on",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-metriccatcher/master/lib/logstash/outputs/metriccatcher.rb",
    "name": "metriccatcher",
    "type": "output",
    "params": [
      {
        "comments": "The metrics to send. This supports dynamic strings like `%{host}`\nfor metric names and also for values. This is a hash field with key\nof the metric name, value of the metric value.\n\nThe value will be coerced to a floating point value. Values which cannot be\ncoerced will zero (0)",
        "base": false,
        "name": "biased",
        "validate": "hash"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The metrics to send. This supports dynamic strings like `%{host}`\nfor metric names and also for values. This is a hash field with key\nof the metric name, value of the metric value. Example:\n[source,ruby]\n  counter => { \"%{host}.apache.hits.%{response} => \"1\" }\n\nThe value will be coerced to a floating point value. Values which cannot be\ncoerced will zero (0)",
        "base": false,
        "name": "counter",
        "validate": "hash"
      },
      {
        "comments": "The metrics to send. This supports dynamic strings like `%{host}`\nfor metric names and also for values. This is a hash field with key\nof the metric name, value of the metric value.\n\nThe value will be coerced to a floating point value. Values which cannot be\ncoerced will zero (0)",
        "base": false,
        "name": "gauge",
        "validate": "hash"
      },
      {
        "comments": "The address of the MetricCatcher",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "The metrics to send. This supports dynamic strings like `%{host}`\nfor metric names and also for values. This is a hash field with key\nof the metric name, value of the metric value.\n\nThe value will be coerced to a floating point value. Values which cannot be\ncoerced will zero (0)",
        "base": false,
        "name": "meter",
        "validate": "hash"
      },
      {
        "comments": "The port to connect on your MetricCatcher",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "1420"
      },
      {
        "comments": "The metrics to send. This supports dynamic strings like %{host}\nfor metric names and also for values. This is a hash field with key\nof the metric name, value of the metric value. Example:\n[source,ruby]\n  timer => { \"%{host}.apache.response_time => \"%{response_time}\" }\n\nThe value will be coerced to a floating point value. Values which cannot be\ncoerced will zero (0)",
        "base": false,
        "name": "timer",
        "validate": "hash"
      },
      {
        "comments": "The metrics to send. This supports dynamic strings like `%{host}`\nfor metric names and also for values. This is a hash field with key\nof the metric name, value of the metric value.\n\nThe value will be coerced to a floating point value. Values which cannot be\ncoerced will zero (0)",
        "base": false,
        "name": "uniform",
        "validate": "hash"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-websocket/master/lib/logstash/outputs/websocket.rb",
    "name": "websocket",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The address to serve websocket data from",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "The port to serve websocket data from",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "3232"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-solr_http/master/lib/logstash/outputs/solr_http.rb",
    "name": "solr_http",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Solr document ID for events. You'd typically have a variable here, like\n'%{foo}' so you can assign your own IDs",
        "base": false,
        "name": "document_id",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Number of events to queue up before writing to Solr",
        "base": false,
        "name": "flush_size",
        "validate": "number",
        "default": "100"
      },
      {
        "comments": "Amount of time since the last flush before a flush is done even if\nthe number of buffered events is smaller than flush_size",
        "base": false,
        "name": "idle_flush_time",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "URL used to connect to Solr",
        "base": false,
        "name": "solr_url",
        "validate": "string",
        "default": "http//localhost8983/solr"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-s3/master/lib/logstash/inputs/s3.rb",
    "name": "s3",
    "type": "input",
    "params": [
      {
        "comments": "The name of the S3 bucket.",
        "base": false,
        "name": "bucket",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Append a prefix to the key (full path including file name in s3) after processing.\nIf backing up to another (or the same) bucket, this effectively lets you\nchoose a new 'folder' to place the files in",
        "base": false,
        "name": "backup_add_prefix",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Name of a S3 bucket to backup processed files to.",
        "base": false,
        "name": "backup_to_bucket",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Path of a local directory to backup processed files to.",
        "base": false,
        "name": "backup_to_dir",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "DEPRECATED: The credentials of the AWS account used to access the bucket.\nCredentials can be specified:\n- As an [\"id\",\"secret\"] array\n- As a path to a file containing AWS_ACCESS_KEY_ID=... and AWS_SECRET_ACCESS_KEY=...\n- In the environment, if not set (using variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY)",
        "base": false,
        "name": "credentials",
        "validate": "array",
        "default": [],
        "deprecated": "This only exists to be backwards compatible. This plugin now uses the AwsConfig from PluginMixins"
      },
      {
        "comments": "Whether to delete processed files from the original bucket.",
        "base": false,
        "name": "delete",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Ruby style regexp of keys to exclude from the bucket",
        "base": false,
        "name": "exclude_pattern",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Interval to wait between to check the file list again after a run is finished.\nValue is in seconds.",
        "base": false,
        "name": "interval",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "If specified, the prefix of filenames in the bucket must match (not a regexp)",
        "base": false,
        "name": "prefix",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "The AWS region for your bucket.",
        "base": false,
        "name": "region_endpoint",
        "validate": "[\"us-east-1\", \"us-west-1\", \"us-west-2\","
      },
      {
        "comments": "Where to write the since database (keeps track of the date\nthe last handled file was added to S3). The default will write\nsincedb files to some path matching \"$HOME/.sincedb*\"\nShould be a path with filename not just a directory.",
        "base": false,
        "name": "sincedb_path",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Set the directory where logstash will store the tmp files before processing them.\ndefault to the current OS temporary directory in linux /tmp/logstash",
        "base": false,
        "name": "temporary_directory",
        "validate": "string",
        "default": "File.join(Dir.tmpdir, \"logstash\")"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-librato/master/lib/logstash/outputs/librato.rb",
    "name": "librato",
    "type": "output",
    "params": [
      {
        "comments": "Your Librato account\nusually an email address",
        "base": false,
        "name": "account_id",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Your Librato API Token",
        "base": false,
        "name": "api_token",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Annotations\nRegisters an annotation with Librato\nThe only required field is `title` and `name`.\n`start_time` and `end_time` will be set to `event[\"@timestamp\"].to_i`\nYou can add any other optional annotation values as well.\nAll values will be passed through `event.sprintf`\n\nExample:\n[source,ruby]\n  {\n      \"title\" => \"Logstash event on %{host}\"\n      \"name\" => \"logstash_stream\"\n  }\nor\n[source,ruby]\n   {\n      \"title\" => \"Logstash event\"\n      \"description\" => \"%{message}\"\n      \"name\" => \"logstash_stream\"\n   }",
        "base": false,
        "name": "annotation",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Batch size\nNumber of events to batch up before sending to Librato.\n",
        "base": false,
        "name": "batch_size",
        "validate": "string",
        "default": "10"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Counters\nSend data to Librato as a counter\n\nExample:\n[source,ruby]\n    {\n        \"value\" => \"1\"\n        \"source\" => \"%{host}\"\n        \"name\" => \"messages_received\"\n    }\n\nAdditionally, you can override the `measure_time` for the event. Must be a unix timestamp:\n[source,ruby]\n    {\n        \"value\" => \"1\"\n        \"source\" => \"%{host}\"\n        \"name\" => \"messages_received\"\n        \"measure_time\" => \"%{my_unixtime_field}\"\n    }\nDefault is to use the event's timestamp",
        "base": false,
        "name": "counter",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Gauges\nSend data to Librato as a gauge\n\nExample:\n[source,ruby]\n    {\n        \"value\" => \"%{bytes_received}\"\n        \"source\" => \"%{host}\"\n        \"name\" => \"apache_bytes\"\n    }\nAdditionally, you can override the `measure_time` for the event. Must be a unix timestamp:\n[source,ruby]\n    {\n        \"value\" => \"%{bytes_received}\"\n        \"source\" => \"%{host}\"\n        \"name\" => \"apache_bytes\"\n        \"measure_time\" => \"%{my_unixtime_field}\n    }\nDefault is to use the event's timestamp",
        "base": false,
        "name": "gauge",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-hipchat/master/lib/logstash/outputs/hipchat.rb",
    "name": "hipchat",
    "type": "output",
    "params": [
      {
        "comments": "The ID or name of the room, support fieldref",
        "base": false,
        "name": "room_id",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The HipChat authentication token.",
        "base": false,
        "name": "token",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Background color for message.\nHipChat currently supports one of \"yellow\", \"red\", \"green\", \"purple\",\n\"gray\", or \"random\". (default: yellow), support fieldref",
        "base": false,
        "name": "color",
        "validate": "string",
        "default": "yellow"
      },
      {
        "comments": "Message format to send, event tokens are usable here.",
        "base": false,
        "name": "format",
        "validate": "string",
        "default": "%{message}"
      },
      {
        "comments": "The name the message will appear be sent from, you can use fieldref",
        "base": false,
        "name": "from",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "HipChat host to use",
        "base": false,
        "name": "host",
        "validate": "string"
      },
      {
        "comments": "Specify `Message Format`",
        "base": false,
        "name": "message_format",
        "validate": [
          "html",
          "text"
        ],
        "default": "html"
      },
      {
        "comments": "Whether or not this message should trigger a notification for people in the room.",
        "base": false,
        "name": "trigger_notify",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-loggly/master/lib/logstash/outputs/loggly.rb",
    "name": "loggly",
    "type": "output",
    "params": [
      {
        "comments": "The loggly http input key to send to.\nThis is usually visible in the Loggly 'Inputs' page as something like this:\n....\n    https://logs-01.loggly.net/inputs/abcdef12-3456-7890-abcd-ef0123456789\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                          \\---------->   key   <-------------/\n....\nYou can use `%{foo}` field lookups here if you need to pull the api key from\nthe event. This is mainly aimed at multitenant hosting providers who want\nto offer shipping a customer's logs to that customer's loggly account.",
        "base": false,
        "name": "key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Can Retry.\nSetting this value true helps user to send multiple retry attempts if the first request fails",
        "base": false,
        "name": "can_retry",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The hostname to send logs to. This should target the loggly http input\nserver which is usually \"logs-01.loggly.com\" (Gen2 account).\nSee Loggly HTTP endpoint documentation at\nhttps://www.loggly.com/docs/http-endpoint/",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "logs-01.loggly.com"
      },
      {
        "comments": "Should the log action be sent over https instead of plain http",
        "base": false,
        "name": "proto",
        "validate": "string",
        "default": "http"
      },
      {
        "comments": "Proxy Host",
        "base": false,
        "name": "proxy_host",
        "validate": "string"
      },
      {
        "comments": "Proxy Password",
        "base": false,
        "name": "proxy_password",
        "validate": "password",
        "default": ""
      },
      {
        "comments": "Proxy Port",
        "base": false,
        "name": "proxy_port",
        "validate": "number"
      },
      {
        "comments": "Proxy Username",
        "base": false,
        "name": "proxy_user",
        "validate": "string"
      },
      {
        "comments": "Retry count.\nIt may be possible that the request may timeout due to slow Internet connection\nif such condition appears, retry_count helps in retrying request for multiple times\nIt will try to submit request until retry_count and then halt",
        "base": false,
        "name": "retry_count",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "Loggly Tag\nTag helps you to find your logs in the Loggly dashboard easily\nYou can make a search in Loggly using tag as \"tag:logstash-contrib\"\nor the tag set by you in the config file.\n\nYou can use %{somefield} to allow for custom tag values.\nHelpful for leveraging Loggly source groups.\nhttps://www.loggly.com/docs/source-groups/",
        "base": false,
        "name": "tag",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-cloudwatch/master/lib/logstash/outputs/cloudwatch.rb",
    "name": "cloudwatch",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The default dimensions [ name, value, ... ] to use for events which do not have a `CW_dimensions` field",
        "base": false,
        "name": "dimensions",
        "validate": "hash"
      },
      {
        "comments": "The name of the field used to set the dimensions on an event metric\nThe field named here, if present in an event, must have an array of\none or more key & value pairs, for example...\n    `add_field => [ \"CW_dimensions\", \"Environment\", \"CW_dimensions\", \"prod\" ]`\nor, equivalently...\n    `add_field => [ \"CW_dimensions\", \"Environment\" ]`\n    `add_field => [ \"CW_dimensions\", \"prod\" ]`",
        "base": false,
        "name": "field_dimensions",
        "validate": "string",
        "default": "CW_dimensions"
      },
      {
        "comments": "The name of the field used to set the metric name on an event\nThe author of this plugin recommends adding this field to events in inputs &\nfilters rather than using the per-output default setting so that one output\nplugin on your logstash indexer can serve all events (which of course had\nfields set on your logstash shippers.)",
        "base": false,
        "name": "field_metricname",
        "validate": "string",
        "default": "CW_metricname"
      },
      {
        "comments": "The name of the field used to set a different namespace per event\nNote: Only one namespace can be sent to CloudWatch per API call\nso setting different namespaces will increase the number of API calls\nand those cost money.",
        "base": false,
        "name": "field_namespace",
        "validate": "string",
        "default": "CW_namespace"
      },
      {
        "comments": "The name of the field used to set the unit on an event metric",
        "base": false,
        "name": "field_unit",
        "validate": "string",
        "default": "CW_unit"
      },
      {
        "comments": "The name of the field used to set the value (float) on an event metric",
        "base": false,
        "name": "field_value",
        "validate": "string",
        "default": "CW_value"
      },
      {
        "comments": "The default metric name to use for events which do not have a `CW_metricname` field.\nBeware: If this is provided then all events which pass through this output will be aggregated and\nsent to CloudWatch, so use this carefully.  Furthermore, when providing this option, you\nwill probably want to also restrict events from passing through this output using event\ntype, tag, and field matching",
        "base": false,
        "name": "metricname",
        "validate": "string"
      },
      {
        "comments": "The default namespace to use for events which do not have a `CW_namespace` field",
        "base": false,
        "name": "namespace",
        "validate": "string",
        "default": "Logstash"
      },
      {
        "comments": "How many events to queue before forcing a call to the CloudWatch API ahead of `timeframe` schedule\nSet this to the number of events-per-timeframe you will be sending to CloudWatch to avoid extra API calls",
        "base": false,
        "name": "queue_size",
        "validate": "number",
        "default": "10000"
      },
      {
        "comments": "How often to send data to CloudWatch\nThis does not affect the event timestamps, events will always have their\nactual timestamp (to-the-minute) sent to CloudWatch.\n\nWe only call the API if there is data to send.\n\nSee the Rufus Scheduler docs for an https://github.com/jmettraux/rufus-scheduler#the-time-strings-understood-by-rufus-scheduler[explanation of allowed values]",
        "base": false,
        "name": "timeframe",
        "validate": "string",
        "default": "1m"
      },
      {
        "comments": "The default unit to use for events which do not have a `CW_unit` field\nIf you set this option you should probably set the \"value\" option along with it",
        "base": false,
        "name": "unit",
        "validate": "VALID_UNITS",
        "default": "COUNT_UNIT"
      },
      {
        "comments": "The default value to use for events which do not have a `CW_value` field\nIf provided, this must be a string which can be converted to a float, for example...\n    \"1\", \"2.34\", \".5\", and \"0.67\"\nIf you set this option you should probably set the `unit` option along with it",
        "base": false,
        "name": "value",
        "validate": "string",
        "default": "1"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-sns/master/lib/logstash/outputs/sns.rb",
    "name": "sns",
    "type": "output",
    "params": [
      {
        "comments": "Optional ARN to send messages to. If you do not set this you must\ninclude the `sns` field in your events to set the ARN on a per-message basis!",
        "base": false,
        "name": "arn",
        "validate": "string"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "When an ARN for an SNS topic is specified here, the message\n\"Logstash successfully booted\" will be sent to it when this plugin\nis registered.\n\nExample: arn:aws:sns:us-east-1:770975001275:logstash-testing\n",
        "base": false,
        "name": "publish_boot_message_arn",
        "validate": "string"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-punct/master/lib/logstash/filters/punct.rb",
    "name": "punct",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The field reference to use for punctuation stripping",
        "base": false,
        "name": "source",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "The field to store the result.",
        "base": false,
        "name": "target",
        "validate": "string",
        "default": "punct"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-pagerduty/master/lib/logstash/outputs/pagerduty.rb",
    "name": "pagerduty",
    "type": "output",
    "params": [
      {
        "comments": "The PagerDuty Service API Key",
        "base": false,
        "name": "service_key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Custom description",
        "base": false,
        "name": "description",
        "validate": "string",
        "default": "Logstash event for %{host}"
      },
      {
        "comments": "The event details. These might be data from the Logstash event fields you wish to include.\nTags are automatically included if detected so there is no need to explicitly add them here.",
        "base": false,
        "name": "details",
        "validate": "hash",
        "default": "{\"timestamp\" => \"%{@timestamp}\", \"message\" => \"%{message}\"}"
      },
      {
        "comments": "Event type",
        "base": false,
        "name": "event_type",
        "validate": [
          "trigger",
          "acknowledge",
          "resolve"
        ],
        "default": "trigger"
      },
      {
        "comments": "The service key to use. You'll need to set this up in PagerDuty beforehand.",
        "base": false,
        "name": "incident_key",
        "validate": "string",
        "default": "logstash/%{host}/%{type}"
      },
      {
        "comments": "PagerDuty API URL. You shouldn't need to change this, but is included to allow for flexibility\nshould PagerDuty iterate the API and Logstash hasn't been updated yet.",
        "base": false,
        "name": "pdurl",
        "validate": "string",
        "default": "https//events.pagerduty.com/generic/2010-04-15/create_event.json"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-elapsed/master/lib/logstash/filters/elapsed.rb",
    "name": "elapsed",
    "type": "filter",
    "params": [
      {
        "comments": "The name of the tag identifying the \"end event\"",
        "base": false,
        "name": "end_tag",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The name of the tag identifying the \"start event\"",
        "base": false,
        "name": "start_tag",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The name of the field containing the task ID.\nThis value must uniquely identify the task in the system, otherwise\nit's impossible to match the couple of events.",
        "base": false,
        "name": "unique_id_field",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "This property manage what to do when an \"end event\" matches a \"start event\".\nIf it's set to `false` (default value), the elapsed information are added\nto the \"end event\"; if it's set to `true` a new \"match event\" is created.",
        "base": false,
        "name": "new_event_on_match",
        "validate": "boolean",
        "required": false,
        "default": "false"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The amount of seconds after an \"end event\" can be considered lost.\nThe corresponding \"start event\" is discarded and an \"expired event\"\nis generated. The default value is 30 minutes (1800 seconds).",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "required": false,
        "default": "1800"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-gelf/master/lib/logstash/outputs/gelf.rb",
    "name": "gelf",
    "type": "output",
    "params": [
      {
        "comments": "Graylog2 server IP address or hostname.",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The GELF chunksize. You usually don't need to change this.",
        "base": false,
        "name": "chunksize",
        "validate": "number",
        "default": "1420"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The GELF custom field mappings. GELF supports arbitrary attributes as custom\nfields. This exposes that. Exclude the `_` portion of the field name\ne.g. `custom_fields => ['foo_field', 'some_value']`\nsets `_foo_field` = `some_value`.",
        "base": false,
        "name": "custom_fields",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The GELF facility. Dynamic values like `%{foo}` are permitted here; this\nis useful if you need to use a value from the event as the facility name.\nShould now be sent as an underscored \"additional field\" (e.g. `\\_facility`)",
        "base": false,
        "name": "facility",
        "validate": "string",
        "deprecated": true
      },
      {
        "comments": "The GELF file; this is usually the source code file in your program where\nthe log event originated. Dynamic values like `%{foo}` are permitted here.\nShould now be sent as an underscored \"additional field\" (e.g. `\\_file`).",
        "base": false,
        "name": "file",
        "validate": "string",
        "deprecated": true
      },
      {
        "comments": "The GELF full message. Dynamic values like `%{foo}` are permitted here.",
        "base": false,
        "name": "full_message",
        "validate": "string",
        "default": "%{message}"
      },
      {
        "comments": "Ignore these fields when `ship_metadata` is set. Typically this lists the\nfields used in dynamic values for GELF fields.",
        "base": false,
        "name": "ignore_metadata",
        "validate": "array",
        "default": "[\"@timestamp\",\"@version\",\"severity\",\"host\",\"source_host\",\"source_path\",\"short_message\"]"
      },
      {
        "comments": "The GELF message level. Dynamic values like `%{level}` are permitted here;\nuseful if you want to parse the 'log level' from an event and use that\nas the GELF level/severity.\n\nValues here can be integers [0..7] inclusive or any of\n\"debug\", \"info\", \"warn\", \"error\", \"fatal\" (case insensitive).\nSingle-character versions of these are also valid, \"d\", \"i\", \"w\", \"e\", \"f\",\n\"u\"\nThe following additional severity\\_labels from Logstash's  syslog\\_pri filter\nare accepted: \"emergency\", \"alert\", \"critical\",  \"warning\", \"notice\", and\n\"informational\".",
        "base": false,
        "name": "level",
        "validate": "array",
        "default": "[\"%{severity}\",\"INFO\"]"
      },
      {
        "comments": "The GELF line number; this is usually the line number in your program where\nthe log event originated. Dynamic values like `%{foo}` are permitted here, but the\nvalue should be a number.\nShould now be sent as an underscored \"additional field\" (e.g. `\\_line`).",
        "base": false,
        "name": "line",
        "validate": "string",
        "deprecated": true
      },
      {
        "comments": "Graylog2 server port number.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "12201"
      },
      {
        "comments": "Allow overriding of the GELF `sender` field. This is useful if you\nwant to use something other than the event's source host as the\n\"sender\" of an event. A common case for this is using the application name\ninstead of the hostname.",
        "base": false,
        "name": "sender",
        "validate": "string",
        "default": "%{host}"
      },
      {
        "comments": "Should Logstash ship metadata within event object? This will cause Logstash\nto ship any fields in the event (such as those created by grok) in the GELF\nmessages. These will be sent as underscored \"additional fields\".",
        "base": false,
        "name": "ship_metadata",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Ship tags within events. This will cause Logstash to ship the tags of an\nevent as the field `\\_tags`.",
        "base": false,
        "name": "ship_tags",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "The GELF short message field name. If the field does not exist or is empty,\nthe event message is taken instead.",
        "base": false,
        "name": "short_message",
        "validate": "string",
        "default": "short_message"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-riak/master/lib/logstash/outputs/riak.rb",
    "name": "riak",
    "type": "output",
    "params": [
      {
        "comments": "The bucket name to write events to\nExpansion is supported here as values are\npassed through event.sprintf\nMultiple buckets can be specified here\nbut any bucket-specific settings defined\napply to ALL the buckets.",
        "base": false,
        "name": "bucket",
        "validate": "array",
        "default": "[\"logstash-%{+YYYY.MM.dd}\"]"
      },
      {
        "comments": "Bucket properties (NYI)\nLogstash hash of properties for the bucket\ni.e.\n[source,ruby]\n    bucket_props => {\n        \"r\" => \"one\"\n        \"w\" => \"one\"\n        \"dw\", \"one\n     }\nor\n[source,ruby]\n    bucket_props => { \"n_val\" => \"3\" }\nProperties will be passed as-is",
        "base": false,
        "name": "bucket_props",
        "validate": "hash"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Search\nEnable search on the bucket defined above",
        "base": false,
        "name": "enable_search",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "SSL\nEnable SSL",
        "base": false,
        "name": "enable_ssl",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Indices\nArray of fields to add 2i on\ne.g.\n[source,ruby]\n    `indices => [\"source_host\", \"type\"]\nOff by default as not everyone runs eleveldb",
        "base": false,
        "name": "indices",
        "validate": "array"
      },
      {
        "comments": "The event key name\nvariables are valid here.\n\nChoose this carefully. Best to let riak decide.",
        "base": false,
        "name": "key_name",
        "validate": "string"
      },
      {
        "comments": "The nodes of your Riak cluster\nThis can be a single host or\na Logstash hash of node/port pairs\ne.g\n[source,ruby]\n    {\n        \"node1\" => \"8098\"\n        \"node2\" => \"8098\"\n    }",
        "base": false,
        "name": "nodes",
        "validate": "hash",
        "default": "{\"localhost\" =>  \"8098\"}"
      },
      {
        "comments": "The protocol to use\nHTTP or ProtoBuf\nApplies to ALL backends listed above\nNo mix and match",
        "base": false,
        "name": "proto",
        "validate": [
          "http",
          "pb"
        ],
        "default": "http"
      },
      {
        "comments": "SSL Options\nOptions for SSL connections\nOnly applied if SSL is enabled\nLogstash hash that maps to the riak-client options\nhere: https://github.com/basho/riak-ruby-client/wiki/Connecting-to-Riak\nYou'll likely want something like this:\n\n[source, ruby]\n    ssl_opts => {\n       \"pem\" => \"/etc/riak.pem\"\n       \"ca_path\" => \"/usr/share/certificates\"\n    }\n\nPer the riak client docs, the above sample options\nwill turn on SSL `VERIFY_PEER`",
        "base": false,
        "name": "ssl_opts",
        "validate": "hash"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-checksum/master/lib/logstash/filters/checksum.rb",
    "name": "checksum",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "",
        "base": false,
        "name": "algorithm",
        "validate": "ALGORITHMS",
        "default": "sha256"
      },
      {
        "comments": "A list of keys to use in creating the string to checksum\nKeys will be sorted before building the string\nkeys and values will then be concatenated with pipe delimeters\nand checksummed",
        "base": false,
        "name": "keys",
        "validate": "array",
        "default": "[\"message\",\"@timestamp\",\"type\"]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-s3/master/lib/logstash/outputs/s3.rb",
    "name": "s3",
    "type": "output",
    "params": [
      {
        "comments": "S3 bucket",
        "base": false,
        "name": "bucket",
        "validate": "string"
      },
      {
        "comments": "The S3 canned ACL to use when putting the file. Defaults to \"private\".",
        "base": false,
        "name": "canned_acl",
        "validate": "[\"private\", \"public_read\", \"public_read_write\", \"authenticated_read\"],"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "AWS endpoint_region",
        "base": false,
        "name": "endpoint_region",
        "validate": "[\"us-east-1\", \"us-west-1\", \"us-west-2\","
      },
      {
        "comments": "Specify a prefix to the uploaded filename, this can simulate directories on S3",
        "base": false,
        "name": "prefix",
        "validate": "string",
        "default": ""
      },
      {
        "comments": " IMPORTANT: if you use multiple instance of s3, you should specify on one of them the \"restore=> true\" and on the others \"restore => false\".\n This is hack for not destroy the new files after restoring the initial files.\n If you do not specify \"restore => true\" when logstash crashes or is restarted, the files are not sent into the bucket,\n for example if you have single Instance.",
        "base": false,
        "name": "restore",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The version of the S3 signature hash to use. Normally uses the internal client default, can be explicitly\nspecified here",
        "base": false,
        "name": "signature_version",
        "validate": [
          "v2",
          "v4"
        ]
      },
      {
        "comments": "Set the size of file in bytes, this means that files on bucket when have dimension > file_size, they are stored in two or more file.\nIf you have tags then it will generate a specific size file for every tags\nNOTE: define size of file is the better thing, because generate a local temporary file on disk and then put it in bucket.",
        "base": false,
        "name": "size_file",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "Define tags to be appended to the file on the S3 bucket.\n\nExample:\ntags => [\"elasticsearch\", \"logstash\", \"kibana\"]\n\nWill generate this file:\n\"ls.s3.logstash.local.2015-01-01T00.00.tag_elasticsearch.logstash.kibana.part0.txt\"\n",
        "base": false,
        "name": "tags",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Set the directory where logstash will store the tmp files before sending it to S3\ndefault to the current OS temporary directory in linux /tmp/logstash",
        "base": false,
        "name": "temporary_directory",
        "validate": "string",
        "default": "File.join(Dir.tmpdir, \"logstash\")"
      },
      {
        "comments": "Set the time, in minutes, to close the current sub_time_section of bucket.\nIf you define file_size you have a number of files in consideration of the section and the current tag.\n0 stay all time on listerner, beware if you specific 0 and size_file 0, because you will not put the file on bucket,\nfor now the only thing this plugin can do is to put the file when logstash restart.",
        "base": false,
        "name": "time_file",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "Specify how many workers to use to upload the files to S3",
        "base": false,
        "name": "upload_workers_count",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-range/master/lib/logstash/filters/range.rb",
    "name": "range",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Negate the range match logic, events should be outsize of the specified range to match.",
        "base": false,
        "name": "negate",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "An array of field, min, max, action tuples.\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        ranges => [ \"message\", 0, 10, \"tag:short\",\n                    \"message\", 11, 100, \"tag:medium\",\n                    \"message\", 101, 1000, \"tag:long\",\n                    \"message\", 1001, 1e1000, \"drop\",\n                    \"duration\", 0, 100, \"field:latency:fast\",\n                    \"duration\", 101, 200, \"field:latency:normal\",\n                    \"duration\", 201, 1000, \"field:latency:slow\",\n                    \"duration\", 1001, 1e1000, \"field:latency:outlier\",\n                    \"requests\", 0, 10, \"tag:too_few_%{host}_requests\" ]\n      }\n    }\n\nSupported actions are drop tag or field with specified value.\nAdded tag names and field names and field values can have `%{dynamic}` values.\n",
        "base": false,
        "name": "ranges",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-rabbitmq/master/lib/logstash/outputs/rabbitmq.rb",
    "name": "rabbitmq",
    "type": "output",
    "params": [
      {
        "comments": "The name of the exchange",
        "base": false,
        "name": "exchange",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The exchange type (fanout, topic, direct)",
        "base": false,
        "name": "exchange_type",
        "validate": "EXCHANGE_TYPES",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Is this exchange durable? (aka; Should it survive a broker restart?)",
        "base": false,
        "name": "durable",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Key to route to by default. Defaults to 'logstash'\n\n* Routing keys are ignored on fanout exchanges.",
        "base": false,
        "name": "key",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "Should RabbitMQ persist messages to disk?",
        "base": false,
        "name": "persistent",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-cipher/master/lib/logstash/filters/cipher.rb",
    "name": "cipher",
    "type": "filter",
    "params": [
      {
        "comments": "The cipher algorythm\n\nA list of supported algorithms can be obtained by\n[source,ruby]\n    puts OpenSSL::Cipher.ciphers",
        "base": false,
        "name": "algorithm",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Encrypting or decrypting some data\n\nValid values are encrypt or decrypt",
        "base": false,
        "name": "mode",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Do we have to perform a `base64` decode or encode?\n\nIf we are decrypting, `base64` decode will be done before.\nIf we are encrypting, `base64` will be done after.\n",
        "base": false,
        "name": "base64",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Cypher padding to use. Enables or disables padding.\n\nBy default encryption operations are padded using standard block padding\nand the padding is checked and removed when decrypting. If the pad\nparameter is zero then no padding is performed, the total amount of data\nencrypted or decrypted must then be a multiple of the block size or an\nerror will occur.\n\nSee EVP_CIPHER_CTX_set_padding for further information.\n\nWe are using Openssl jRuby which uses default padding to PKCS5Padding\nIf you want to change it, set this parameter. If you want to disable\nit, Set this parameter to 0\n[source,ruby]\n    filter { cipher { padding => 0 }}",
        "base": false,
        "name": "cipher_padding",
        "validate": "string"
      },
      {
        "comments": "The initialization vector to use\n\nThe cipher modes CBC, CFB, OFB and CTR all need an \"initialization\nvector\", or short, IV. ECB mode is the only mode that does not require\nan IV, but there is almost no legitimate use case for this mode\nbecause of the fact that it does not sufficiently hide plaintext patterns.",
        "base": false,
        "name": "iv",
        "validate": "string"
      },
      {
        "comments": "The key to use",
        "base": false,
        "name": "key",
        "validate": "string"
      },
      {
        "comments": "The character used to pad the key",
        "base": false,
        "name": "key_pad",
        "default": "\"\\0\""
      },
      {
        "comments": "The key size to pad\n\nIt depends of the cipher algorythm.I your key don't need\npadding, don't set this parameter\n\nExample, for AES-256, we must have 32 char long key\n[source,ruby]\n    filter { cipher { key_size => 32 }\n",
        "base": false,
        "name": "key_size",
        "validate": "number",
        "default": "32"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The field to perform filter\n\nExample, to use the @message field (default) :\n[source,ruby]\n    filter { cipher { source => \"message\" } }",
        "base": false,
        "name": "source",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "The name of the container to put the result\n\nExample, to place the result into crypt :\n[source,ruby]\n    filter { cipher { target => \"crypt\" } }",
        "base": false,
        "name": "target",
        "validate": "string",
        "default": "message"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-redis/master/lib/logstash/outputs/redis.rb",
    "name": "redis",
    "type": "output",
    "params": [
      {
        "comments": "Set to true if you want Redis to batch up values and send 1 RPUSH command\ninstead of one command per value to push on the list.  Note that this only\nworks with `data_type=\"list\"` mode right now.\n\nIf true, we send an RPUSH every \"batch_events\" events or\n\"batch_timeout\" seconds (whichever comes first).\nOnly supported for `data_type` is \"list\".",
        "base": false,
        "name": "batch",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If batch is set to true, the number of events we queue up for an RPUSH.",
        "base": false,
        "name": "batch_events",
        "validate": "number",
        "default": "50"
      },
      {
        "comments": "If batch is set to true, the maximum amount of time between RPUSH commands\nwhen there are pending events to flush.",
        "base": false,
        "name": "batch_timeout",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "How often to check for congestion. Default is one second.\nZero means to check on every event.",
        "base": false,
        "name": "congestion_interval",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "In case Redis `data_type` is `list` and has more than `@congestion_threshold` items,\nblock until someone consumes them and reduces congestion, otherwise if there are\nno consumers Redis will run out of memory, unless it was configured with OOM protection.\nBut even with OOM protection, a single Redis list can block all other users of Redis,\nuntil Redis CPU consumption reaches the max allowed RAM size.\nA default value of 0 means that this limit is disabled.\nOnly supported for `list` Redis `data_type`.",
        "base": false,
        "name": "congestion_threshold",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "Either list or channel.  If `redis_type` is list, then we will set\nRPUSH to key. If `redis_type` is channel, then we will PUBLISH to `key`.\nTODO set required true",
        "base": false,
        "name": "data_type",
        "validate": [
          "list",
          "channel"
        ],
        "required": false
      },
      {
        "comments": "The Redis database number.",
        "base": false,
        "name": "db",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "The hostname(s) of your Redis server(s). Ports may be specified on any\nhostname, which will override the global port config.\n\nFor example:\n[source,ruby]\n    \"127.0.0.1\"\n    [\"127.0.0.1\", \"127.0.0.2\"]\n    [\"127.0.0.1:6380\", \"127.0.0.1\"]",
        "base": false,
        "name": "host",
        "validate": "array",
        "default": "[\"127.0.0.1\"]"
      },
      {
        "comments": "The name of a Redis list or channel. Dynamic names are\nvalid here, for example `logstash-%{type}`.\nTODO set required true",
        "base": false,
        "name": "key",
        "validate": "string",
        "required": false
      },
      {
        "comments": "Name is used for logging in case there are multiple instances.\nTODO: delete",
        "base": false,
        "name": "name",
        "validate": "string",
        "default": "\"default\","
      },
      {
        "comments": "Password to authenticate with.  There is no authentication by default.",
        "base": false,
        "name": "password",
        "validate": "password"
      },
      {
        "comments": "The default port to connect on. Can be overridden on any hostname.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "6379"
      },
      {
        "comments": "The name of the Redis queue (we'll use RPUSH on this). Dynamic names are\nvalid here, for example `logstash-%{type}`\nTODO: delete",
        "base": false,
        "name": "queue",
        "validate": "string",
        "deprecated": true
      },
      {
        "comments": "Interval for reconnecting to failed Redis connections",
        "base": false,
        "name": "reconnect_interval",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "Shuffle the host list during Logstash startup.",
        "base": false,
        "name": "shuffle_hosts",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Redis initial connection timeout in seconds.",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-de_dot/master/lib/logstash/filters/de_dot.rb",
    "name": "de_dot",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The `fields` array should contain a list of known fields to act on.\nIf undefined, all top-level fields will be checked.  Sub-fields must be\nmanually specified in the array.  For example: `[\"field.suffix\",\"[foo][bar.suffix]\"]`\nwill result in \"field_suffix\" and nested or sub field [\"foo\"][\"bar_suffix\"]\n\nWARNING: This is an expensive operation.\n",
        "base": false,
        "name": "fields",
        "validate": "array"
      },
      {
        "comments": "If `nested` is _true_, then create sub-fields instead of replacing dots with\na different separator.",
        "base": false,
        "name": "nested",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Replace dots with this value.",
        "base": false,
        "name": "separator",
        "validate": "string",
        "default": "_"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-http_poller/master/lib/logstash/inputs/http_poller.rb",
    "name": "http_poller",
    "type": "input",
    "params": [
      {
        "comments": "How often (in seconds) the urls will be called",
        "base": false,
        "name": "interval",
        "validate": "number",
        "required": true
      },
      {
        "comments": "A Hash of urls in this format : `\"name\" => \"url\"`.\nThe name and the url will be passed in the outputed event",
        "base": false,
        "name": "urls",
        "validate": "hash",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "If you'd like to work with the request/response metadata.\nSet this value to the name of the field you'd like to store a nested\nhash of metadata.",
        "base": false,
        "name": "metadata_target",
        "validate": "string",
        "default": "@metadata"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Define the target field for placing the received data. If this setting is omitted, the data will be stored at the root (top level) of the event.",
        "base": false,
        "name": "target",
        "validate": "string"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-boundary/master/lib/logstash/outputs/boundary.rb",
    "name": "boundary",
    "type": "output",
    "params": [
      {
        "comments": "Your Boundary API key",
        "base": false,
        "name": "api_key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Your Boundary Org ID",
        "base": false,
        "name": "org_id",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Auto\nIf set to true, logstash will try to pull boundary fields out\nof the event. Any field explicitly set by config options will\noverride these.\n`['type', 'subtype', 'creation_time', 'end_time', 'links', 'tags', 'loc']`",
        "base": false,
        "name": "auto",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Sub-Type",
        "base": false,
        "name": "bsubtype",
        "validate": "string"
      },
      {
        "comments": "Tags\nSet any custom tags for this event\nDefault are the Logstash tags if any",
        "base": false,
        "name": "btags",
        "validate": "array"
      },
      {
        "comments": "Type",
        "base": false,
        "name": "btype",
        "validate": "string"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "End time\nOverride the stop time\nNote that Boundary requires this to be seconds since epoch\nIf overriding, it is your responsibility to type this correctly\nBy default this is set to `event[\"@timestamp\"].to_i`",
        "base": false,
        "name": "end_time",
        "validate": "string"
      },
      {
        "comments": "Start time\nOverride the start time\nNote that Boundary requires this to be seconds since epoch\nIf overriding, it is your responsibility to type this correctly\nBy default this is set to `event[\"@timestamp\"].to_i`",
        "base": false,
        "name": "start_time",
        "validate": "string"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-json_encode/master/lib/logstash/filters/json_encode.rb",
    "name": "json_encode",
    "type": "filter",
    "params": [
      {
        "comments": "The field to convert to JSON.",
        "base": false,
        "name": "source",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The field to write the JSON into. If not specified, the source\nfield will be overwritten.",
        "base": false,
        "name": "target",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-i18n/master/lib/logstash/filters/i18n.rb",
    "name": "i18n",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Replaces non-ASCII characters with an ASCII approximation, or\nif none exists, a replacement character which defaults to `?`\n\nExample:\n[source,ruby]\n    filter {\n      i18n {\n         transliterate => [\"field1\", \"field2\"]\n      }\n    }",
        "base": false,
        "name": "transliterate",
        "validate": "array"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-metrics/master/lib/logstash/filters/metrics.rb",
    "name": "metrics",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The clear interval, when all counter are reset.\n\nIf set to -1, the default value, the metrics will never be cleared.\nOtherwise, should be a multiple of 5s.",
        "base": false,
        "name": "clear_interval",
        "validate": "number",
        "default": "-1"
      },
      {
        "comments": "The flush interval, when the metrics event is created. Must be a multiple of 5s.",
        "base": false,
        "name": "flush_interval",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "Don't track events that have @timestamp older than some number of seconds.\n\nThis is useful if you want to only include events that are near real-time\nin your metrics.\n\nExample, to only count events that are within 10 seconds of real-time, you\nwould do this:\n\n    filter {\n      metrics {\n        meter => [ \"hits\" ]\n        ignore_older_than => 10\n      }\n    }",
        "base": false,
        "name": "ignore_older_than",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "syntax: `meter => [ \"name of metric\", \"name of metric\" ]`",
        "base": false,
        "name": "meter",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The percentiles that should be measured",
        "base": false,
        "name": "percentiles",
        "validate": "array",
        "default": "[1,5,10,90,95,99,100]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The rates that should be measured, in minutes.\nPossible values are 1, 5, and 15.",
        "base": false,
        "name": "rates",
        "validate": "array",
        "default": "[1,5,15]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "syntax: `timer => [ \"name of metric\", \"%{time_value}\" ]`",
        "base": false,
        "name": "timer",
        "validate": "hash",
        "default": "{}"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-xmpp/master/lib/logstash/outputs/xmpp.rb",
    "name": "xmpp",
    "type": "output",
    "params": [
      {
        "comments": "The message to send. This supports dynamic strings like `%{host}`",
        "base": false,
        "name": "message",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The xmpp password for the user/identity.",
        "base": false,
        "name": "password",
        "validate": "password",
        "required": true
      },
      {
        "comments": "The user or resource ID, like foo@example.com.",
        "base": false,
        "name": "user",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The xmpp server to connect to. This is optional. If you omit this setting,\nthe host on the user/identity is used. (foo.com for user@foo.com)",
        "base": false,
        "name": "host",
        "validate": "string"
      },
      {
        "comments": "if muc/multi-user-chat required, give the name of the room that\nyou want to join: room@conference.domain/nick",
        "base": false,
        "name": "rooms",
        "validate": "array"
      },
      {
        "comments": "The users to send messages to",
        "base": false,
        "name": "users",
        "validate": "array"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-google_cloud_storage/master/lib/logstash/outputs/google_cloud_storage.rb",
    "name": "google_cloud_storage",
    "type": "output",
    "params": [
      {
        "comments": "GCS bucket name, without \"gs://\" or any other prefix.",
        "base": false,
        "name": "bucket",
        "validate": "string",
        "required": true
      },
      {
        "comments": "GCS path to private key file.",
        "base": false,
        "name": "key_path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "GCS service account.",
        "base": false,
        "name": "service_account",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Time pattern for log file, defaults to hourly files.\nMust Time.strftime patterns: www.ruby-doc.org/core-2.0/Time.html#method-i-strftime",
        "base": false,
        "name": "date_pattern",
        "validate": "string",
        "default": "%Y-%m-%dT%H00"
      },
      {
        "comments": "Flush interval in seconds for flushing writes to log files. 0 will flush\non every message.",
        "base": false,
        "name": "flush_interval_secs",
        "validate": "number",
        "default": "2"
      },
      {
        "comments": "Gzip output stream when writing events to log files.",
        "base": false,
        "name": "gzip",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "GCS private key password.",
        "base": false,
        "name": "key_password",
        "validate": "string",
        "default": "notasecret"
      },
      {
        "comments": "Log file prefix. Log file will follow the format:\n<prefix>_hostname_date<.part?>.log",
        "base": false,
        "name": "log_file_prefix",
        "validate": "string",
        "default": "logstash_gcs"
      },
      {
        "comments": "Sets max file size in kbytes. 0 disable max file check.",
        "base": false,
        "name": "max_file_size_kbytes",
        "validate": "number",
        "default": "10000"
      },
      {
        "comments": "The event format you want to store in files. Defaults to plain text.",
        "base": false,
        "name": "output_format",
        "validate": [
          "json",
          "plain"
        ],
        "default": "plain"
      },
      {
        "comments": "Directory where temporary files are stored.\nDefaults to /tmp/logstash-gcs-<random-suffix>",
        "base": false,
        "name": "temp_directory",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "Uploader interval when uploading new files to GCS. Adjust time based\non your time pattern (for example, for hourly files, this interval can be\naround one hour).",
        "base": false,
        "name": "uploader_interval_secs",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-translate/master/lib/logstash/filters/translate.rb",
    "name": "translate",
    "type": "filter",
    "params": [
      {
        "comments": "The name of the logstash event field containing the value to be compared for a\nmatch by the translate filter (e.g. `message`, `host`, `response_code`).\n\nIf this field is an array, only the first value will be used.",
        "base": false,
        "name": "field",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The destination field you wish to populate with the translated code. The default\nis a field named `translation`. Set this to the same value as source if you want\nto do a substitution, in this case filter will allways succeed. This will clobber\nthe old value of the source field!",
        "base": false,
        "name": "destination",
        "validate": "string",
        "default": "translation"
      },
      {
        "comments": "The dictionary to use for translation, when specified in the logstash filter\nconfiguration item (i.e. do not use the `@dictionary_path` file)\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        dictionary => [ \"100\", \"Continue\",\n                        \"101\", \"Switching Protocols\",\n                        \"merci\", \"thank you\",\n                        \"old version\", \"new version\" ]\n      }\n    }\nNOTE: it is an error to specify both `dictionary` and `dictionary_path`",
        "base": false,
        "name": "dictionary",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The full path of the external dictionary file. The format of the table\nshould be a standard YAML, JSON or CSV. Make sure you specify any integer-based keys\nin quotes. For example, the YAML file should look something like this:\n[source,ruby]\n    \"100\": Continue\n    \"101\": Switching Protocols\n    merci: gracias\n    old version: new version\n\nNOTE: it is an error to specify both `dictionary` and `dictionary_path`\nNOTE: Currently supported formats are YAML, JSON and CSV, format selection is\nbased on the file extension, json for JSON, (yaml|yml) for YAML and csv for CSV.\nNOTE: The JSON format only supports simple key/value, unnested objects. The CSV\nformat expects exactly two columns with the first serving as the original text,\nthe second column as the replacement",
        "base": false,
        "name": "dictionary_path",
        "validate": "path"
      },
      {
        "comments": "If `exact => false`, and logstash receives the same event, the destination field\nwill be also set to `bar`. However, if logstash receives an event with the `data` field\nset to `foofing`, the destination field will be set to `barfing`.\n\nSet both `exact => true` AND `regex => `true` if you would like to match using dictionary\nkeys as regular expressions. A large dictionary could be expensive to match in this case.",
        "base": false,
        "name": "exact",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "In case no translation occurs in the event (no matches), this will add a default\ntranslation string, which will always populate `field`, if the match failed.\n\nFor example, if we have configured `fallback => \"no match\"`, using this dictionary:\n[source,ruby]\n    foo: bar\n\nThen, if logstash received an event with the field `foo` set to `bar`, the destination\nfield would be set to `bar`. However, if logstash received an event with `foo` set to `nope`,\nthen the destination field would still be populated, but with the value of `no match`.\nThis configuration can be dynamic and include parts of the event using the `%{field}` syntax.",
        "base": false,
        "name": "fallback",
        "validate": "string"
      },
      {
        "comments": "If the destination (or target) field already exists, this configuration item specifies\nwhether the filter should skip translation (default) or overwrite the target field\nvalue with the new translation value.",
        "base": false,
        "name": "override",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "When using a dictionary file, this setting will indicate how frequently\n(in seconds) logstash will check the dictionary file for updates.",
        "base": false,
        "name": "refresh_interval",
        "validate": "number",
        "default": "300"
      },
      {
        "comments": "If you'd like to treat dictionary keys as regular expressions, set `exact => true`.\nNote: this is activated only when `exact => true`.",
        "base": false,
        "name": "regex",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-zeromq/master/lib/logstash/outputs/zeromq.rb",
    "name": "zeromq",
    "type": "output",
    "params": [
      {
        "comments": "The default logstash topologies work as follows:\n\n* pushpull - inputs are pull, outputs are push\n* pubsub - inputs are subscribers, outputs are publishers\n* pair - inputs are clients, inputs are servers\n\nIf the predefined topology flows don't work for you,\nyou can change the 'mode' setting",
        "base": false,
        "name": "topology",
        "validate": [
          "pushpull",
          "pubsub",
          "pair"
        ],
        "required": true
      },
      {
        "comments": "0mq socket address to connect or bind.\nPlease note that `inproc://` will not work with logstashi.\nFor each we use a context per thread.\nBy default, inputs bind/listen and outputs connect.",
        "base": false,
        "name": "address",
        "validate": "array",
        "default": "[\"tcp//127.0.0.12120\"]"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Server mode binds/listens. Client mode connects.",
        "base": false,
        "name": "mode",
        "validate": [
          "server",
          "client"
        ],
        "default": "client"
      },
      {
        "comments": "This exposes zmq_setsockopt for advanced tuning.\nSee http://api.zeromq.org/2-1:zmq-setsockopt for details.\n\nThis is where you would set values like:\n\n* ZMQ::HWM - high water mark\n* ZMQ::IDENTITY - named queues\n* ZMQ::SWAP_SIZE - space for disk overflow\n\nExample:\n[source,ruby]\n    sockopt => {\n       \"ZMQ::HWM\" => 50\n       \"ZMQ::IDENTITY\"  => \"my_named_queue\"\n    }",
        "base": false,
        "name": "sockopt",
        "validate": "hash"
      },
      {
        "comments": "This is used for the 'pubsub' topology only.\nOn inputs, this allows you to filter messages by topic.\nOn outputs, this allows you to tag a message for routing.\nNOTE: ZeroMQ does subscriber-side filtering\nNOTE: Topic is evaluated with `event.sprintf` so macros are valid here.",
        "base": false,
        "name": "topic",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-pipe/master/lib/logstash/outputs/pipe.rb",
    "name": "pipe",
    "type": "output",
    "params": [
      {
        "comments": "Command line to launch and pipe to",
        "base": false,
        "name": "command",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The format to use when writing events to the pipe. This value\nsupports any string and can include `%{name}` and other dynamic\nstrings.\n\nIf this setting is omitted, the full json representation of the\nevent will be written as a single line.",
        "base": false,
        "name": "message_format",
        "validate": "string"
      },
      {
        "comments": "Close pipe that hasn't been used for TTL seconds. -1 or 0 means never close.",
        "base": false,
        "name": "ttl",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-kafka/master/lib/logstash/outputs/kafka.rb",
    "name": "kafka",
    "type": "output",
    "params": [
      {
        "comments": "The topic to produce messages to",
        "base": false,
        "name": "topic_id",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The number of acknowledgments the producer requires the leader to have received\nbefore considering a request complete.\n\nacks=0,   the producer will not wait for any acknowledgment from the server at all.\nacks=1,   This will mean the leader will write the record to its local log but\n          will respond without awaiting full acknowledgement from all followers.\nacks=all, This means the leader will wait for the full set of in-sync replicas to acknowledge the record.",
        "base": false,
        "name": "acks",
        "validate": [
          "0",
          "1",
          "all"
        ],
        "default": "1"
      },
      {
        "comments": "The producer will attempt to batch records together into fewer requests whenever multiple\nrecords are being sent to the same partition. This helps performance on both the client\nand the server. This configuration controls the default batch size in bytes.",
        "base": false,
        "name": "batch_size",
        "validate": "number",
        "default": "16384"
      },
      {
        "comments": "When our memory buffer is exhausted we must either stop accepting new\nrecords (block) or throw errors. By default this setting is true and we block,\nhowever in some scenarios blocking is not desirable and it is better to immediately give an error.",
        "base": false,
        "name": "block_on_buffer_full",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "This is for bootstrapping and the producer will only use it for getting metadata (topics,\npartitions and replicas). The socket connections for sending the actual data will be\nestablished based on the broker information returned in the metadata. The format is\n`host1:port1,host2:port2`, and the list can be a subset of brokers or a VIP pointing to a\nsubset of brokers.",
        "base": false,
        "name": "bootstrap_servers",
        "validate": "string",
        "default": "localhost9092"
      },
      {
        "comments": "The total bytes of memory the producer can use to buffer records waiting to be sent to the server.",
        "base": false,
        "name": "buffer_memory",
        "validate": "number",
        "default": "33554432"
      },
      {
        "comments": "The id string to pass to the server when making requests.\nThe purpose of this is to be able to track the source of requests beyond just\nip/port by allowing a logical application name to be included with the request",
        "base": false,
        "name": "client_id",
        "validate": "string"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The compression type for all data generated by the producer.\nThe default is none (i.e. no compression). Valid values are none, gzip, or snappy.",
        "base": false,
        "name": "compression_type",
        "validate": [
          "none",
          "gzip",
          "snappy"
        ],
        "default": "none"
      },
      {
        "comments": "Serializer class for the key of the message",
        "base": false,
        "name": "key_serializer",
        "validate": "string",
        "default": "org.apache.kafka.common.serialization.StringSerializer"
      },
      {
        "comments": "The producer groups together any records that arrive in between request\ntransmissions into a single batched request. Normally this occurs only under\nload when records arrive faster than they can be sent out. However in some circumstances\nthe client may want to reduce the number of requests even under moderate load.\nThis setting accomplishes this by adding a small amount of artificial delay—that is,\nrather than immediately sending out a record the producer will wait for up to the given delay\nto allow other records to be sent so that the sends can be batched together.",
        "base": false,
        "name": "linger_ms",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "The maximum size of a request",
        "base": false,
        "name": "max_request_size",
        "validate": "number",
        "default": "1048576"
      },
      {
        "comments": "The key for the message",
        "base": false,
        "name": "message_key",
        "validate": "string"
      },
      {
        "comments": "the timeout setting for initial metadata request to fetch topic metadata.",
        "base": false,
        "name": "metadata_fetch_timeout_ms",
        "validate": "number",
        "default": "60000"
      },
      {
        "comments": "the max time in milliseconds before a metadata refresh is forced.",
        "base": false,
        "name": "metadata_max_age_ms",
        "validate": "number",
        "default": "300000"
      },
      {
        "comments": "The size of the TCP receive buffer to use when reading data",
        "base": false,
        "name": "receive_buffer_bytes",
        "validate": "number",
        "default": "32768"
      },
      {
        "comments": "The amount of time to wait before attempting to reconnect to a given host when a connection fails.",
        "base": false,
        "name": "reconnect_backoff_ms",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "Setting a value greater than zero will cause the client to\nresend any record whose send fails with a potentially transient error.",
        "base": false,
        "name": "retries",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "The amount of time to wait before attempting to retry a failed produce request to a given topic partition.",
        "base": false,
        "name": "retry_backoff_ms",
        "validate": "number",
        "default": "100"
      },
      {
        "comments": "The size of the TCP send buffer to use when sending data.",
        "base": false,
        "name": "send_buffer_bytes",
        "validate": "number",
        "default": "131072"
      },
      {
        "comments": "The configuration controls the maximum amount of time the server will wait for acknowledgments\nfrom followers to meet the acknowledgment requirements the producer has specified with the\nacks configuration. If the requested number of acknowledgments are not met when the timeout\nelapses an error will be returned. This timeout is measured on the server side and does not\ninclude the network latency of the request.",
        "base": false,
        "name": "timeout_ms",
        "validate": "number",
        "default": "30000"
      },
      {
        "comments": "Serializer class for the value of the message",
        "base": false,
        "name": "value_serializer",
        "validate": "string",
        "default": "org.apache.kafka.common.serialization.StringSerializer"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-statsd/master/lib/logstash/outputs/statsd.rb",
    "name": "statsd",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "A count metric. `metric_name => count` as hash. `%{fieldname}` substitutions are\nallowed in the metric names.",
        "base": false,
        "name": "count",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Enable debugging.",
        "base": false,
        "name": "debug",
        "validate": "boolean",
        "default": false,
        "deprecated": "This setting was never used by this plugin. It will be removed soon."
      },
      {
        "comments": "A decrement metric. Metric names as array. `%{fieldname}` substitutions are\nallowed in the metric names.",
        "base": false,
        "name": "decrement",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "A gauge metric. `metric_name => gauge` as hash. `%{fieldname}` substitutions are\nallowed in the metric names.",
        "base": false,
        "name": "gauge",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The hostname or IP address of the statsd server.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "An increment metric. Metric names as array. `%{fieldname}` substitutions are\nallowed in the metric names.",
        "base": false,
        "name": "increment",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The statsd namespace to use for this metric. `%{fieldname}` substitutions are\nallowed.",
        "base": false,
        "name": "namespace",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "The port to connect to on your statsd server.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "8125"
      },
      {
        "comments": "The sample rate for the metric.",
        "base": false,
        "name": "sample_rate",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "The name of the sender. Dots will be replaced with underscores. `%{fieldname}`\nsubstitutions are allowed.",
        "base": false,
        "name": "sender",
        "validate": "string",
        "default": "%{host}"
      },
      {
        "comments": "A set metric. `metric_name => \"string\"` to append as hash. `%{fieldname}`\nsubstitutions are allowed in the metric names.",
        "base": false,
        "name": "set",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "A timing metric. `metric_name => duration` as hash. `%{fieldname}` substitutions\nare allowed in the metric names.",
        "base": false,
        "name": "timing",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-elasticsearch_http/master/lib/logstash/outputs/elasticsearch_http.rb",
    "name": "elasticsearch_http",
    "type": "output",
    "params": [
      {
        "comments": "The hostname or IP address to reach your Elasticsearch server.",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The document ID for the index. Useful for overwriting existing entries in\nElasticsearch with the same ID.",
        "base": false,
        "name": "document_id",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "This plugin uses the bulk index api for improved indexing performance.\nTo make efficient bulk api calls, we will buffer a certain number of\nevents before flushing that out to Elasticsearch. This setting\ncontrols how many events will be buffered before sending a batch\nof events.",
        "base": false,
        "name": "flush_size",
        "validate": "number",
        "default": "100"
      },
      {
        "comments": "The amount of time since last flush before a flush is forced.\n\nThis setting helps ensure slow event rates don't get stuck in Logstash.\nFor example, if your `flush_size` is 100, and you have received 10 events,\nand it has been more than `idle_flush_time` seconds since the last flush,\nlogstash will flush those 10 events automatically.\n\nThis helps keep both fast and slow log streams moving along in\nnear-real-time.",
        "base": false,
        "name": "idle_flush_time",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "The index to write events to. This can be dynamic using the %{foo} syntax.\nThe default value will partition your indices by day so you can more easily\ndelete old data or only search specific date ranges.",
        "base": false,
        "name": "index",
        "validate": "string",
        "default": "logstash-%{+YYYY.MM.dd}"
      },
      {
        "comments": "The index type to write events to. Generally you should try to write only\nsimilar events to the same 'type'. String expansion '%{foo}' works here.",
        "base": false,
        "name": "index_type",
        "validate": "string"
      },
      {
        "comments": "Starting in Logstash 1.3 (unless you set option \"manage_template\" to false)\na default mapping template for Elasticsearch will be applied, if you do not\nalready have one set to match the index pattern defined (default of\n\"logstash-%{+YYYY.MM.dd}\"), minus any variables.  For example, in this case\nthe template will be applied to all indices starting with logstash-*\n\nIf you have dynamic templating (e.g. creating indices based on field names)\nthen you should set \"manage_template\" to false and use the REST API to upload\nyour templates manually.",
        "base": false,
        "name": "manage_template",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "The HTTP Basic Auth password used to access your elasticsearch server.",
        "base": false,
        "name": "password",
        "validate": "password",
        "default": "nil"
      },
      {
        "comments": "The port for Elasticsearch HTTP interface to use.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "9200"
      },
      {
        "comments": "Set the type of Elasticsearch replication to use. If async\nthe index request to Elasticsearch to return after the primary\nshards have been written. If sync (default), index requests\nwill wait until the primary and the replica shards have been\nwritten.",
        "base": false,
        "name": "replication",
        "validate": [
          "async",
          "sync"
        ],
        "default": "sync"
      },
      {
        "comments": "You can set the path to your own template here, if you so desire.\nIf not the included template will be used.",
        "base": false,
        "name": "template",
        "validate": "path"
      },
      {
        "comments": "This configuration option defines how the template is named inside Elasticsearch.\nNote that if you have used the template management features and subsequently\nchange this you will need to prune the old template manually, e.g.\ncurl -XDELETE <http://localhost:9200/_template/OldTemplateName?pretty>\nwhere OldTemplateName is whatever the former setting was.",
        "base": false,
        "name": "template_name",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "Overwrite the current template with whatever is configured\nin the template and template_name directives.",
        "base": false,
        "name": "template_overwrite",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The HTTP Basic Auth username used to access your elasticsearch server.",
        "base": false,
        "name": "user",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-stdout/master/lib/logstash/outputs/stdout.rb",
    "name": "stdout",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-stomp/master/lib/logstash/outputs/stomp.rb",
    "name": "stomp",
    "type": "output",
    "params": [
      {
        "comments": "The destination to read events from. Supports string expansion, meaning\n`%{foo}` values will expand to the field value.\n\nExample: \"/topic/logstash\"",
        "base": false,
        "name": "destination",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The address of the STOMP server.",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Enable debugging output?",
        "base": false,
        "name": "debug",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The password to authenticate with.",
        "base": false,
        "name": "password",
        "validate": "password",
        "default": ""
      },
      {
        "comments": "The port to connect to on your STOMP server.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "61613"
      },
      {
        "comments": "The username to authenticate with.",
        "base": false,
        "name": "user",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "The vhost to use",
        "base": false,
        "name": "vhost",
        "validate": "string",
        "default": "nil"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-http/master/lib/logstash/outputs/http.rb",
    "name": "http",
    "type": "output",
    "params": [
      {
        "comments": "The HTTP Verb. One of \"put\", \"post\", \"patch\", \"delete\", \"get\", \"head\"",
        "base": false,
        "name": "http_method",
        "validate": "VALID_METHODS",
        "required": true
      },
      {
        "comments": "URL to use",
        "base": false,
        "name": "url",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Content type\n\nIf not specified, this defaults to the following:\n\n* if format is \"json\", \"application/json\"\n* if format is \"form\", \"application/x-www-form-urlencoded\"",
        "base": false,
        "name": "content_type",
        "validate": "string"
      },
      {
        "comments": "Set the format of the http body.\n\nIf form, then the body will be the mapping (or whole event) converted\ninto a query parameter string, e.g. `foo=bar&baz=fizz...`\n\nIf message, then the body will be the result of formatting the event according to message\n\nOtherwise, the event is sent as json.",
        "base": false,
        "name": "format",
        "validate": [
          "json",
          "form",
          "message"
        ],
        "default": "json"
      },
      {
        "comments": "Custom headers to use\nformat is `headers => [\"X-My-Header\", \"%{host}\"]`",
        "base": false,
        "name": "headers",
        "validate": "hash"
      },
      {
        "comments": "This lets you choose the structure and parts of the event that are sent.\n\n\nFor example:\n[source,ruby]\n   mapping => {\"foo\", \"%{host}\", \"bar\", \"%{type}\"}",
        "base": false,
        "name": "mapping",
        "validate": "hash"
      },
      {
        "comments": "",
        "base": false,
        "name": "message",
        "validate": "string"
      },
      {
        "comments": "DEPRECATED. Set 'ssl_certificate_validation' instead",
        "base": false,
        "name": "verify_ssl",
        "validate": "boolean",
        "default": true,
        "deprecated": "\"Please use \"ssl_certificate_validation\" instead. This option will be removed in a future release!\""
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-graphite/master/lib/logstash/outputs/graphite.rb",
    "name": "graphite",
    "type": "output",
    "params": [
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Enable debug output.",
        "base": false,
        "name": "debug",
        "validate": "boolean",
        "default": false,
        "deprecated": "This setting was never used by this plugin. It will be removed soon."
      },
      {
        "comments": "Exclude regex matched metric names, by default exclude unresolved %{field} strings.",
        "base": false,
        "name": "exclude_metrics",
        "validate": "array",
        "default": "[ \"%\\{[^}]+\\}\" ]"
      },
      {
        "comments": "An array indicating that these event fields should be treated as metrics\nand will be sent verbatim to Graphite. You may use either `fields_are_metrics`\nor `metrics`, but not both.",
        "base": false,
        "name": "fields_are_metrics",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The hostname or IP address of the Graphite server.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "Include only regex matched metric names.",
        "base": false,
        "name": "include_metrics",
        "validate": "array",
        "default": "[\".*\"]"
      },
      {
        "comments": "The metric(s) to use. This supports dynamic strings like %{host}\nfor metric names and also for values. This is a hash field with key\nbeing the metric name, value being the metric value. Example:\n[source,ruby]\n    metrics => { \"%{host}/uptime\" => \"%{uptime_1m}\" }\n\nThe value will be coerced to a floating point value. Values which cannot be\ncoerced will be set to zero (0). You may use either `metrics` or `fields_are_metrics`,\nbut not both.",
        "base": false,
        "name": "metrics",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Defines the format of the metric string. The placeholder '*' will be\nreplaced with the name of the actual metric.\n[source,ruby]\n    metrics_format => \"foo.bar.*.sum\"\n\nNOTE: If no metrics_format is defined, the name of the metric will be used as fallback.",
        "base": false,
        "name": "metrics_format",
        "validate": "string",
        "default": "DEFAULT_METRICS_FORMAT"
      },
      {
        "comments": "When hashes are passed in as values they are broken out into a dotted notation\nFor instance if you configure this plugin with\n# [source,ruby]\n    metrics => \"mymetrics\"\n\nand \"mymetrics\" is a nested hash of '{a => 1, b => { c => 2 }}'\nthis plugin will generate two metrics: a => 1, and b.c => 2 .\nIf you've specified a 'metrics_format' it will respect that,\nbut you still may want control over the separator within these nested key names.\nThis config setting changes the separator from the '.' default.",
        "base": false,
        "name": "nested_object_separator",
        "validate": "string",
        "default": "."
      },
      {
        "comments": "The port to connect to on the Graphite server.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "2003"
      },
      {
        "comments": "Interval between reconnect attempts to Carbon.",
        "base": false,
        "name": "reconnect_interval",
        "validate": "number",
        "default": "2"
      },
      {
        "comments": "Should metrics be resent on failure?",
        "base": false,
        "name": "resend_on_failure",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Use this field for the timestamp instead of '@timestamp' which is the\ndefault. Useful when backfilling or just getting more accurate data into\ngraphite since you probably have a cache layer infront of Logstash.",
        "base": false,
        "name": "timestamp_field",
        "validate": "string",
        "default": "@timestamp"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-graphtastic/master/lib/logstash/outputs/graphtastic.rb",
    "name": "graphtastic",
    "type": "output",
    "params": [
      {
        "comments": "the number of metrics to send to GraphTastic at one time. 60 seems to be the perfect\namount for UDP, with default packet size.",
        "base": false,
        "name": "batch_number",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "if using rest as your end point you need to also provide the application url\nit defaults to localhost/graphtastic.  You can customize the application url\nby changing the name of the .war file.  There are other ways to change the\napplication context, but they vary depending on the Application Server in use.\nPlease consult your application server documentation for more on application\ncontexts.",
        "base": false,
        "name": "context",
        "validate": "string",
        "default": "graphtastic"
      },
      {
        "comments": "setting allows you to specify where we save errored transactions\nthis makes the most sense at this point - will need to decide\non how we reintegrate these error metrics\nNOT IMPLEMENTED!",
        "base": false,
        "name": "error_file",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "host for the graphtastic server - defaults to 127.0.0.1",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "127.0.0.1"
      },
      {
        "comments": "options are udp(fastest - default) - rmi(faster) - rest(fast) - tcp(don't use TCP yet - some problems - errors out on linux)",
        "base": false,
        "name": "integration",
        "validate": [
          "udp",
          "tcp",
          "rmi",
          "rest"
        ],
        "default": "udp"
      },
      {
        "comments": "metrics hash - you will provide a name for your metric and the metric\ndata as key value pairs.  so for example:\n\n[source,ruby]\nmetrics => { \"Response\" => \"%{response}\" }\n\nexample for the logstash config\n\n[source,ruby]\nmetrics => [ \"Response\", \"%{response}\" ]\n\nNOTE: you can also use the dynamic fields for the key value as well as the actual value",
        "base": false,
        "name": "metrics",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "port for the graphtastic instance - defaults to 1199 for RMI, 1299 for TCP, 1399 for UDP, and 8080 for REST",
        "base": false,
        "name": "port",
        "validate": "number"
      },
      {
        "comments": "number of attempted retry after send error - currently only way to integrate\nerrored transactions - should try and save to a file or later consumption\neither by graphtastic utility or by this program after connectivity is\nensured to be established.",
        "base": false,
        "name": "retries",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-multiline/master/lib/logstash/filters/multiline.rb",
    "name": "multiline",
    "type": "filter",
    "params": [
      {
        "comments": "The regular expression to match.",
        "base": false,
        "name": "pattern",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If the pattern matched, does event belong to the next or previous event?",
        "base": false,
        "name": "what",
        "validate": [
          "previous",
          "next"
        ],
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Allow duplcate values on the source field.",
        "base": false,
        "name": "allow_duplicates",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "The maximum age an event can be (in seconds) before it is automatically\nflushed.",
        "base": false,
        "name": "max_age",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "Negate the regexp pattern ('if not matched')",
        "base": false,
        "name": "negate",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Logstash ships by default with a bunch of patterns, so you don't\nnecessarily need to define this yourself unless you are adding additional\npatterns.\n\nPattern files are plain text with format:\n[source,ruby]\n    NAME PATTERN\n\nFor example:\n[source,ruby]\n    NUMBER \\d+",
        "base": false,
        "name": "patterns_dir",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": false,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "The field name to execute the pattern match on.",
        "base": false,
        "name": "source",
        "validate": "string",
        "default": "message"
      },
      {
        "comments": "The stream identity is how the multiline filter determines which stream an\nevent belongs to. This is generally used for differentiating, say, events\ncoming from multiple files in the same file input, or multiple connections\ncoming from a tcp input.\n\nThe default value here is usually what you want, but there are some cases\nwhere you want to change it. One such example is if you are using a tcp\ninput with only one client connecting at any time. If that client\nreconnects (due to error or client restart), then logstash will identify\nthe new connection as a new stream and break any multiline goodness that\nmay have occurred between the old and new connection. To solve this use\ncase, you can use `%{@source_host}.%{@type}` instead.",
        "base": false,
        "name": "stream_identity",
        "validate": "string",
        "default": "%{host}.%{path}.%{type}"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-file/master/lib/logstash/inputs/file.rb",
    "name": "file",
    "type": "input",
    "params": [
      {
        "comments": "The path(s) to the file(s) to use as an input.\nYou can use filename patterns here, such as `/var/log/*.log`.\nIf you use a pattern like `/var/log/**/*.log`, a recursive search\nof `/var/log` will be done for all `*.log` files.\nPaths must be absolute and cannot be relative.\n\nYou may also configure multiple paths. See an example\non the <<array,Logstash configuration page>>.",
        "base": false,
        "name": "path",
        "validate": "array",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this option is specified, the file input closes any files that remain\nunmodified for longer than the specified timespan in seconds.\nThe default is 1 hour",
        "base": false,
        "name": "close_older",
        "validate": "number",
        "default": "1 * 60 * 60"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "set the new line delimiter, defaults to \"\\n\"",
        "base": false,
        "name": "delimiter",
        "validate": "string",
        "default": "\\n"
      },
      {
        "comments": "How often (in seconds) we expand the filename patterns in the\n`path` option to discover new files to watch.",
        "base": false,
        "name": "discover_interval",
        "validate": "number",
        "default": "15"
      },
      {
        "comments": "Exclusions (matched against the filename, not full path). Filename\npatterns are valid here, too. For example, if you have\n[source,ruby]\n    path => \"/var/log/*\"\n\nYou might want to exclude gzipped files:\n[source,ruby]\n    exclude => \"*.gz\"",
        "base": false,
        "name": "exclude",
        "validate": "array"
      },
      {
        "comments": "If this option is specified, when the file input discovers a file that\nwas last modified before the specified timespan in seconds, the file is\nignored. After it's discovery, if an ignored file is modified it is no\nlonger ignored and any new data is read. The default is 24 hours.",
        "base": false,
        "name": "ignore_older",
        "validate": "number",
        "default": "24 * 60 * 60"
      },
      {
        "comments": "Path of the sincedb database file (keeps track of the current\nposition of monitored log files) that will be written to disk.\nThe default will write sincedb files to some path matching `$HOME/.sincedb*`\nNOTE: it must be a file path and not a directory path",
        "base": false,
        "name": "sincedb_path",
        "validate": "string"
      },
      {
        "comments": "How often (in seconds) to write a since database with the current position of\nmonitored log files.",
        "base": false,
        "name": "sincedb_write_interval",
        "validate": "number",
        "default": "15"
      },
      {
        "comments": "Choose where Logstash starts initially reading files: at the beginning or\nat the end. The default behavior treats files like live streams and thus\nstarts at the end. If you have old data you want to import, set this\nto 'beginning'.\n\nThis option only modifies \"first contact\" situations where a file\nis new and not seen before, i.e. files that don't have a current\nposition recorded in a sincedb file read by Logstash. If a file\nhas already been seen before, this option has no effect and the\nposition recorded in the sincedb file will be used.",
        "base": false,
        "name": "start_position",
        "validate": [
          "beginning",
          "end"
        ],
        "default": "end"
      },
      {
        "comments": "How often (in seconds) we stat files to see if they have been modified.\nIncreasing this interval will decrease the number of system calls we make,\nbut increase the time to detect new log lines.",
        "base": false,
        "name": "stat_interval",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-google_bigquery/master/lib/logstash/outputs/google_bigquery.rb",
    "name": "google_bigquery",
    "type": "output",
    "params": [
      {
        "comments": "BigQuery dataset to which these events will be added to.",
        "base": false,
        "name": "dataset",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Path to private key file for Google Service Account.",
        "base": false,
        "name": "key_path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Google Cloud Project ID (number, not Project Name!).",
        "base": false,
        "name": "project_id",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Service account to access Google APIs.",
        "base": false,
        "name": "service_account",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Schema for log data. It must follow this format:\n<field1-name>:<field1-type>,<field2-name>:<field2-type>,...\nExample: path:STRING,status:INTEGER,score:FLOAT",
        "base": false,
        "name": "csv_schema",
        "validate": "string",
        "required": false,
        "default": "nil"
      },
      {
        "comments": "Time pattern for BigQuery table, defaults to hourly tables.\nMust Time.strftime patterns: www.ruby-doc.org/core-2.0/Time.html#method-i-strftime",
        "base": false,
        "name": "date_pattern",
        "validate": "string",
        "default": "%Y-%m-%dT%H00"
      },
      {
        "comments": "Deleter interval when checking if upload jobs are done for file deletion.\nThis only affects how long files are on the hard disk after the job is done.",
        "base": false,
        "name": "deleter_interval_secs",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "Flush interval in seconds for flushing writes to log files. 0 will flush\non every message.",
        "base": false,
        "name": "flush_interval_secs",
        "validate": "number",
        "default": "2"
      },
      {
        "comments": "Indicates if BigQuery should allow extra values that are not represented in the table schema.\nIf true, the extra values are ignored. If false, records with extra columns are treated as bad records, and if there are too many bad records, an invalid error is returned in the job result. The default value is false.",
        "base": false,
        "name": "ignore_unknown_values",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Schema for log data, as a hash. Example:\njson_schema => {\n    fields => [{\n        name => \"timestamp\"\n        type => \"TIMESTAMP\"\n    }, {\n        name => \"host\"\n        type => \"STRING\"\n    }, {\n        name => \"message\"\n        type => \"STRING\"\n    }]\n}",
        "base": false,
        "name": "json_schema",
        "validate": "hash",
        "required": false,
        "default": "nil"
      },
      {
        "comments": "Private key password for service account private key.",
        "base": false,
        "name": "key_password",
        "validate": "string",
        "default": "notasecret"
      },
      {
        "comments": "BigQuery table ID prefix to be used when creating new tables for log data.\nTable name will be <table_prefix><table_separator><date>",
        "base": false,
        "name": "table_prefix",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "BigQuery table separator to be added between the table_prefix and the\ndate suffix.",
        "base": false,
        "name": "table_separator",
        "validate": "string",
        "default": "_"
      },
      {
        "comments": "Directory where temporary files are stored.\nDefaults to /tmp/logstash-bq-<random-suffix>",
        "base": false,
        "name": "temp_directory",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "Temporary local file prefix. Log file will follow the format:\n<prefix>_hostname_date.part?.log",
        "base": false,
        "name": "temp_file_prefix",
        "validate": "string",
        "default": "logstash_bq"
      },
      {
        "comments": "Uploader interval when uploading new files to BigQuery. Adjust time based\non your time pattern (for example, for hourly files, this interval can be\naround one hour).",
        "base": false,
        "name": "uploader_interval_secs",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-xmpp/master/lib/logstash/inputs/xmpp.rb",
    "name": "xmpp",
    "type": "input",
    "params": [
      {
        "comments": "The xmpp password for the user/identity.",
        "base": false,
        "name": "password",
        "validate": "password",
        "required": true
      },
      {
        "comments": "The user or resource ID, like `foo@example.com`.",
        "base": false,
        "name": "user",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Set to true to enable greater debugging in XMPP. Useful for debugging\nnetwork/authentication erros.",
        "base": false,
        "name": "debug",
        "validate": "boolean",
        "default": false,
        "deprecated": "Use the logstash --debug flag for this instead."
      },
      {
        "comments": "The xmpp server to connect to. This is optional. If you omit this setting,\nthe host on the user/identity is used. (`foo.com` for `user@foo.com`)",
        "base": false,
        "name": "host",
        "validate": "string"
      },
      {
        "comments": "if muc/multi-user-chat required, give the name of the room that\nyou want to join: `room@conference.domain/nick`",
        "base": false,
        "name": "rooms",
        "validate": "array"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-zenoss/master/lib/logstash/inputs/zenoss.rb",
    "name": "zenoss",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The name of the exchange to bind the queue. This is analogous to the 'rabbitmq\noutput' [config 'name'](../outputs/rabbitmq)",
        "base": false,
        "name": "exchange",
        "validate": "string",
        "default": "zenoss.zenevents"
      },
      {
        "comments": "Your rabbitmq server address",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "The routing key to use. This is only valid for direct or fanout exchanges\n\n* Routing keys are ignored on topic exchanges.\n* Wildcards are not valid on direct exchanges.",
        "base": false,
        "name": "key",
        "validate": "string",
        "default": "zenoss.zenevent.#"
      },
      {
        "comments": "Your rabbitmq password",
        "base": false,
        "name": "password",
        "validate": "password",
        "default": "zenoss"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Your rabbitmq username",
        "base": false,
        "name": "user",
        "validate": "string",
        "default": "zenoss"
      },
      {
        "comments": "The vhost to use. If you don't know what this is, leave the default.",
        "base": false,
        "name": "vhost",
        "validate": "string",
        "default": "/zenoss"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-syslog/master/lib/logstash/outputs/syslog.rb",
    "name": "syslog",
    "type": "output",
    "params": [
      {
        "comments": "syslog server address to connect to",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "syslog server port to connect to",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "application name for syslog message. The new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.",
        "base": false,
        "name": "appname",
        "validate": "string",
        "default": "LOGSTASH"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "facility label for syslog message\ndefault fallback to user-level as in rfc3164\nThe new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.",
        "base": false,
        "name": "facility",
        "validate": "string",
        "default": "user-level"
      },
      {
        "comments": "message text to log. The new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.",
        "base": false,
        "name": "message",
        "validate": "string",
        "default": "%{message}"
      },
      {
        "comments": "message id for syslog message. The new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.",
        "base": false,
        "name": "msgid",
        "validate": "string",
        "default": "-"
      },
      {
        "comments": "syslog priority\nThe new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.",
        "base": false,
        "name": "priority",
        "validate": "string",
        "default": "%{syslog_pri}"
      },
      {
        "comments": "process id for syslog message. The new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.",
        "base": false,
        "name": "procid",
        "validate": "string",
        "default": "-"
      },
      {
        "comments": "syslog server protocol. you can choose between udp and tcp",
        "base": false,
        "name": "protocol",
        "validate": [
          "tcp",
          "udp"
        ],
        "default": "udp"
      },
      {
        "comments": "when connection fails, retry interval in sec.",
        "base": false,
        "name": "reconnect_interval",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "syslog message format: you can choose between rfc3164 or rfc5424",
        "base": false,
        "name": "rfc",
        "validate": [
          "rfc3164",
          "rfc5424"
        ],
        "default": "rfc3164"
      },
      {
        "comments": "severity label for syslog message\ndefault fallback to notice as in rfc3164\nThe new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.",
        "base": false,
        "name": "severity",
        "validate": "string",
        "default": "notice"
      },
      {
        "comments": "source host for syslog message. The new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.",
        "base": false,
        "name": "sourcehost",
        "validate": "string",
        "default": "%{host}"
      },
      {
        "comments": "timestamp for syslog message",
        "base": false,
        "name": "timestamp",
        "validate": "string",
        "default": "%{@timestamp}",
        "deprecated": "This setting is no longer necessary. The RFC setting will determine what time format is used."
      },
      {
        "comments": "use label parsing for severity and facility levels\nuse priority field if set to false",
        "base": false,
        "name": "use_labels",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-sqs/master/lib/logstash/inputs/sqs.rb",
    "name": "sqs",
    "type": "input",
    "params": [
      {
        "comments": "Name of the SQS Queue name to pull messages from. Note that this is just the name of the queue, not the URL or ARN.",
        "base": false,
        "name": "queue",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Name of the event field in which to store the SQS message ID",
        "base": false,
        "name": "id_field",
        "validate": "string"
      },
      {
        "comments": "Name of the event field in which to store the SQS message MD5 checksum",
        "base": false,
        "name": "md5_field",
        "validate": "string"
      },
      {
        "comments": "Polling frequency, default is 20 seconds",
        "base": false,
        "name": "polling_frequency",
        "validate": "number",
        "default": "DEFAULT_POLLING_FREQUENCY"
      },
      {
        "comments": "Name of the event field in which to store the SQS message Sent Timestamp",
        "base": false,
        "name": "sent_timestamp_field",
        "validate": "string"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-wmi/master/lib/logstash/inputs/wmi.rb",
    "name": "wmi",
    "type": "input",
    "params": [
      {
        "comments": "WMI query",
        "base": false,
        "name": "query",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Polling interval",
        "base": false,
        "name": "interval",
        "validate": "number",
        "default": "10"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-environment/master/lib/logstash/filters/environment.rb",
    "name": "environment",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Specify a hash of field names and the environment variable name with the\nvalue you want imported into Logstash. For example:\n\n   add_metadata_from_env { \"field_name\" => \"ENV_VAR_NAME\" }\n\nor\n\n   add_metadata_from_env {\n     \"field1\" => \"ENV1\"\n     \"field2\" => \"ENV2\"\n     # \"field_n\" => \"ENV_n\"\n   }",
        "base": false,
        "name": "add_metadata_from_env",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-snmptrap/master/lib/logstash/inputs/snmptrap.rb",
    "name": "snmptrap",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "SNMP Community String to listen for.",
        "base": false,
        "name": "community",
        "validate": "array",
        "default": "public"
      },
      {
        "comments": "The address to listen on",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "The port to listen on. Remember that ports less than 1024 (privileged\nports) may require root to use. hence the default of 1062.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "1062"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "directory of YAML MIB maps  (same format ruby-snmp uses)",
        "base": false,
        "name": "yamlmibdir",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-log4j/master/lib/logstash/inputs/log4j.rb",
    "name": "log4j",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Read timeout in seconds. If a particular TCP connection is\nidle for more than this timeout period, we will assume\nit is dead and close it.\nIf you never want to timeout, use -1.",
        "base": false,
        "name": "data_timeout",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "When mode is `server`, the address to listen on.\nWhen mode is `client`, the address to connect to.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "Mode to operate in. `server` listens for client connections,\n`client` connects to a server.",
        "base": false,
        "name": "mode",
        "validate": [
          "server",
          "client"
        ],
        "default": "server"
      },
      {
        "comments": "When mode is `server`, the port to listen on.\nWhen mode is `client`, the port to connect to.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "4560"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-jmx/master/lib/logstash/inputs/jmx.rb",
    "name": "jmx",
    "type": "input",
    "params": [
      {
        "comments": "Path where json conf files are stored",
        "base": false,
        "name": "path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Indicate number of thread launched to retrieve metrics",
        "base": false,
        "name": "nb_thread",
        "validate": "number",
        "default": "4"
      },
      {
        "comments": "Indicate interval between two jmx metrics retrieval\n(in s)",
        "base": false,
        "name": "polling_frequency",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-irc/master/lib/logstash/inputs/irc.rb",
    "name": "irc",
    "type": "input",
    "params": [
      {
        "comments": "Channels to join and read messages from.\n\nThese should be full channel names including the '#' symbol, such as\n\"#logstash\".\n\nFor passworded channels, add a space and the channel password, such as\n\"#logstash password\".\n",
        "base": false,
        "name": "channels",
        "validate": "array",
        "required": true
      },
      {
        "comments": "Host of the IRC Server to connect to.",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Catch all IRC channel/user events not just channel messages",
        "base": false,
        "name": "catch_all",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Gather and send user counts for channels - this requires catch_all and will force it",
        "base": false,
        "name": "get_stats",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "IRC Nickname",
        "base": false,
        "name": "nick",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "IRC Server password",
        "base": false,
        "name": "password",
        "validate": "password"
      },
      {
        "comments": "Port for the IRC Server",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "6667"
      },
      {
        "comments": "IRC Real name",
        "base": false,
        "name": "real",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "Set this to true to enable SSL.",
        "base": false,
        "name": "secure",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "How often in minutes to get the user count stats",
        "base": false,
        "name": "stats_interval",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "IRC Username",
        "base": false,
        "name": "user",
        "validate": "string",
        "default": "logstash"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-exec/master/lib/logstash/inputs/exec.rb",
    "name": "exec",
    "type": "input",
    "params": [
      {
        "comments": "Command to run. For example, `uptime`",
        "base": false,
        "name": "command",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Interval to run the command. Value is in seconds.",
        "base": false,
        "name": "interval",
        "validate": "number",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Set this to true to enable debugging on an input.",
        "base": false,
        "name": "debug",
        "validate": "boolean",
        "default": false,
        "deprecated": "This setting was never used by this plugin. It will be removed soon."
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-imap/master/lib/logstash/inputs/imap.rb",
    "name": "imap",
    "type": "input",
    "params": [
      {
        "comments": "",
        "base": false,
        "name": "host",
        "validate": "string",
        "required": true
      },
      {
        "comments": "",
        "base": false,
        "name": "password",
        "validate": "password",
        "required": true
      },
      {
        "comments": "",
        "base": false,
        "name": "user",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "",
        "base": false,
        "name": "check_interval",
        "validate": "number",
        "default": "300"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "For multipart messages, use the first part that has this\ncontent-type as the event message.",
        "base": false,
        "name": "content_type",
        "validate": "string",
        "default": "text/plain"
      },
      {
        "comments": "",
        "base": false,
        "name": "delete",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "",
        "base": false,
        "name": "fetch_count",
        "validate": "number",
        "default": "50"
      },
      {
        "comments": "",
        "base": false,
        "name": "folder",
        "validate": "string",
        "default": "INBOX"
      },
      {
        "comments": "",
        "base": false,
        "name": "lowercase_headers",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "",
        "base": false,
        "name": "port",
        "validate": "number"
      },
      {
        "comments": "",
        "base": false,
        "name": "secure",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "",
        "base": false,
        "name": "verify_cert",
        "validate": "boolean",
        "default": "true"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-ganglia/master/lib/logstash/inputs/ganglia.rb",
    "name": "ganglia",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The address to listen on",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "The port to listen on. Remember that ports less than 1024 (privileged\nports) may require root to use.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "8649"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-generator/master/lib/logstash/inputs/generator.rb",
    "name": "generator",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Set how many messages should be generated.\n\nThe default, `0`, means generate an unlimited number of events.",
        "base": false,
        "name": "count",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "The lines to emit, in order. This option cannot be used with the 'message'\nsetting.\n\nExample:\n[source,ruby]\n    input {\n      generator {\n        lines => [\n          \"line 1\",\n          \"line 2\",\n          \"line 3\"\n        ]\n        # Emit all lines 3 times.\n        count => 3\n      }\n    }\n\nThe above will emit `line 1` then `line 2` then `line`, then `line 1`, etc...",
        "base": false,
        "name": "lines",
        "validate": "array"
      },
      {
        "comments": "The message string to use in the event.\n\nIf you set this to `stdin` then this plugin will read a single line from\nstdin and use that as the message string for every event.\n\nOtherwise, this value will be used verbatim as the event message.",
        "base": false,
        "name": "message",
        "validate": "string",
        "default": "Hello world!"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-pipe/master/lib/logstash/inputs/pipe.rb",
    "name": "pipe",
    "type": "input",
    "params": [
      {
        "comments": "Command to run and read events from, one line at a time.\n\nExample:\n[source,ruby]\n   command => \"echo hello world\"",
        "base": false,
        "name": "command",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-rabbitmq/master/lib/logstash/inputs/rabbitmq.rb",
    "name": "rabbitmq",
    "type": "input",
    "params": [
      {
        "comments": "Enable message acknowledgement",
        "base": false,
        "name": "ack",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "Extra queue arguments as an array.\nTo make a RabbitMQ queue mirrored, use: `{\"x-ha-policy\" => \"all\"}`",
        "base": false,
        "name": "arguments",
        "validate": "array",
        "default": "{}"
      },
      {
        "comments": "Should the queue be deleted on the broker when the last consumer\ndisconnects? Set this option to `false` if you want the queue to remain\non the broker, queueing up messages until a consumer comes along to\nconsume them.",
        "base": false,
        "name": "auto_delete",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Is this queue durable? (aka; Should it survive a broker restart?)",
        "base": false,
        "name": "durable",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The name of the exchange to bind the queue to.",
        "base": false,
        "name": "exchange",
        "validate": "string"
      },
      {
        "comments": "Is the queue exclusive? Exclusive queues can only be used by the connection\nthat declared them and will be deleted when it is closed (e.g. due to a Logstash\nrestart).",
        "base": false,
        "name": "exclusive",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The routing key to use when binding a queue to the exchange.\nThis is only relevant for direct or topic exchanges.\n\n* Routing keys are ignored on fanout exchanges.\n* Wildcards are not valid on direct exchanges.",
        "base": false,
        "name": "key",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "Passive queue creation? Useful for checking queue existance without modifying server state",
        "base": false,
        "name": "passive",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Prefetch count. Number of messages to prefetch",
        "base": false,
        "name": "prefetch_count",
        "validate": "number",
        "default": "256"
      },
      {
        "comments": "The name of the queue Logstash will consume events from.",
        "base": false,
        "name": "queue",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-rackspace/master/lib/logstash/inputs/rackspace.rb",
    "name": "rackspace",
    "type": "input",
    "params": [
      {
        "comments": "Rackspace Cloud API Key",
        "base": false,
        "name": "api_key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Rackspace Cloud Username",
        "base": false,
        "name": "username",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "number of messages to claim\nMin: 1, Max: 10",
        "base": false,
        "name": "claim",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Rackspace Queue Name",
        "base": false,
        "name": "queue",
        "validate": "string",
        "default": "logstash"
      },
      {
        "comments": "Rackspace region\n`ord, dfw, lon, syd,` etc",
        "base": false,
        "name": "region",
        "validate": "string",
        "default": "dfw"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "length of time to hold claim\nMin: 60",
        "base": false,
        "name": "ttl",
        "validate": "number",
        "default": "60"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-zeromq/master/lib/logstash/inputs/zeromq.rb",
    "name": "zeromq",
    "type": "input",
    "params": [
      {
        "comments": "0mq topology\nThe default logstash topologies work as follows:\n\n* pushpull - inputs are pull, outputs are push\n* pubsub - inputs are subscribers, outputs are publishers\n* pair - inputs are clients, inputs are servers\n\nIf the predefined topology flows don't work for you,\nyou can change the `mode` setting\nTODO (lusis) add req/rep MAYBE\nTODO (lusis) add router/dealer",
        "base": false,
        "name": "topology",
        "validate": [
          "pushpull",
          "pubsub",
          "pair"
        ],
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "0mq socket address to connect or bind\nPlease note that `inproc://` will not work with logstash\nas each we use a context per thread.\nBy default, inputs bind/listen\nand outputs connect",
        "base": false,
        "name": "address",
        "validate": "array",
        "default": "[\"tcp//*2120\"]"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "mode\nserver mode binds/listens\nclient mode connects",
        "base": false,
        "name": "mode",
        "validate": [
          "server",
          "client"
        ],
        "default": "server"
      },
      {
        "comments": "sender\noverrides the sender to\nset the source of the event\ndefault is `zmq+topology://type/`",
        "base": false,
        "name": "sender",
        "validate": "string"
      },
      {
        "comments": "0mq socket options\nThis exposes `zmq_setsockopt`\nfor advanced tuning\nsee http://api.zeromq.org/2-1:zmq-setsockopt for details\n\nThis is where you would set values like:\n\n * `ZMQ::HWM` - high water mark\n * `ZMQ::IDENTITY` - named queues\n * `ZMQ::SWAP_SIZE` - space for disk overflow\n\nexample: `sockopt => [\"ZMQ::HWM\", 50, \"ZMQ::IDENTITY\", \"my_named_queue\"]`",
        "base": false,
        "name": "sockopt",
        "validate": "hash"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "0mq topic\nThis is used for the `pubsub` topology only\nOn inputs, this allows you to filter messages by topic\nOn outputs, this allows you to tag a message for routing\nNOTE: ZeroMQ does subscriber side filtering.\nNOTE: All topics have an implicit wildcard at the end\nYou can specify multiple topics here",
        "base": false,
        "name": "topic",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-relp/master/lib/logstash/inputs/relp.rb",
    "name": "relp",
    "type": "input",
    "params": [
      {
        "comments": "The port to listen on.",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The address to listen on.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "The SSL CA certificate, chainfile or CA path. The system CA path is automatically included.",
        "base": false,
        "name": "ssl_cacert",
        "validate": "path"
      },
      {
        "comments": "SSL certificate path",
        "base": false,
        "name": "ssl_cert",
        "validate": "path"
      },
      {
        "comments": "Enable SSL (must be set for other `ssl_` options to take effect).",
        "base": false,
        "name": "ssl_enable",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "SSL key path",
        "base": false,
        "name": "ssl_key",
        "validate": "path"
      },
      {
        "comments": "SSL key passphrase",
        "base": false,
        "name": "ssl_key_passphrase",
        "validate": "password",
        "default": "nil"
      },
      {
        "comments": "Verify the identity of the other end of the SSL connection against the CA.\nFor input, sets the field `sslsubject` to that of the client certificate.",
        "base": false,
        "name": "ssl_verify",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-redis/master/lib/logstash/inputs/redis.rb",
    "name": "redis",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The number of events to return from Redis using EVAL.",
        "base": false,
        "name": "batch_count",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Specify either list or channel.  If `redis\\_type` is `list`, then we will BLPOP the\nkey.  If `redis\\_type` is `channel`, then we will SUBSCRIBE to the key.\nIf `redis\\_type` is `pattern_channel`, then we will PSUBSCRIBE to the key.\nTODO: change required to true",
        "base": false,
        "name": "data_type",
        "validate": [
          "list",
          "channel",
          "pattern_channel"
        ],
        "required": false
      },
      {
        "comments": "The Redis database number.",
        "base": false,
        "name": "db",
        "validate": "number",
        "default": "0"
      },
      {
        "comments": "The hostname of your Redis server.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "127.0.0.1"
      },
      {
        "comments": "The name of a Redis list or channel.\nTODO: change required to true",
        "base": false,
        "name": "key",
        "validate": "string",
        "required": false
      },
      {
        "comments": "The `name` configuration is used for logging in case there are multiple instances.\nThis feature has no real function and will be removed in future versions.",
        "base": false,
        "name": "name",
        "validate": "string",
        "default": "default",
        "deprecated": true
      },
      {
        "comments": "Password to authenticate with. There is no authentication by default.",
        "base": false,
        "name": "password",
        "validate": "password"
      },
      {
        "comments": "The port to connect on.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "6379"
      },
      {
        "comments": "The name of the Redis queue (we'll use BLPOP against this).\nTODO: remove soon.",
        "base": false,
        "name": "queue",
        "validate": "string",
        "deprecated": true
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Initial connection timeout in seconds.",
        "base": false,
        "name": "timeout",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-stomp/master/lib/logstash/inputs/stomp.rb",
    "name": "stomp",
    "type": "input",
    "params": [
      {
        "comments": "The destination to read events from.\n\nExample: `/topic/logstash`",
        "base": false,
        "name": "destination",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The address of the STOMP server.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "localhost",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Enable debugging output?",
        "base": false,
        "name": "debug",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The password to authenticate with.",
        "base": false,
        "name": "password",
        "validate": "password",
        "default": ""
      },
      {
        "comments": "The port to connet to on your STOMP server.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "61613"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "The username to authenticate with.",
        "base": false,
        "name": "user",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "The vhost to use",
        "base": false,
        "name": "vhost",
        "validate": "string",
        "default": "nil"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-stdin/master/lib/logstash/inputs/stdin.rb",
    "name": "stdin",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-github/master/lib/logstash/inputs/github.rb",
    "name": "github",
    "type": "input",
    "params": [
      {
        "comments": "The port to listen on",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "If Secret is defined, we drop the events that don't match.\nOtherwise, we'll just add a invalid tag",
        "base": false,
        "name": "drop_invalid",
        "validate": "boolean"
      },
      {
        "comments": "The ip to listen on",
        "base": false,
        "name": "ip",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "Your GitHub Secret Token for the webhook",
        "base": false,
        "name": "secret_token",
        "validate": "string",
        "required": false
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-syslog/master/lib/logstash/inputs/syslog.rb",
    "name": "syslog",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Labels for facility levels. These are defined in RFC3164.",
        "base": false,
        "name": "facility_labels",
        "validate": "array",
        "default": "[\"kernel\",\"user-level\",\"mail\",\"system\",\"security/authorization\",\"syslogd\",\"line printer\",\"network news\",\"UUCP\",\"clock\",\"security/authorization\",\"FTP\",\"NTP\",\"log audit\",\"log alert\",\"clock\",\"local0\",\"local1\",\"local2\",\"local3\",\"local4\",\"local5\",\"local6\",\"local7\"]"
      },
      {
        "comments": "The address to listen on.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "Specify a locale to be used for date parsing using either IETF-BCP47 or POSIX language tag.\nSimple examples are `en`,`en-US` for BCP47 or `en_US` for POSIX.\nIf not specified, the platform default will be used.\n\nThe locale is mostly necessary to be set for parsing month names (pattern with MMM) and\nweekday names (pattern with EEE).\n",
        "base": false,
        "name": "locale",
        "validate": "string"
      },
      {
        "comments": "The port to listen on. Remember that ports less than 1024 (privileged\nports) may require root to use.",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "514"
      },
      {
        "comments": "Labels for severity levels. These are defined in RFC3164.",
        "base": false,
        "name": "severity_labels",
        "validate": "array",
        "default": "[\"Emergency\",\"Alert\",\"Critical\",\"Error\",\"Warning\",\"Notice\",\"Informational\",\"Debug\"]"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Specify a time zone canonical ID to be used for date parsing.\nThe valid IDs are listed on the [Joda.org available time zones page](http://joda-time.sourceforge.net/timezones.html).\nThis is useful in case the time zone cannot be extracted from the value,\nand is not the platform default.\nIf this is not specified the platform default will be used.\nCanonical ID is good as it takes care of daylight saving time for you\nFor example, `America/Los_Angeles` or `Europe/France` are valid IDs.",
        "base": false,
        "name": "timezone",
        "validate": "string"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "Use label parsing for severity and facility levels.",
        "base": false,
        "name": "use_labels",
        "validate": "boolean",
        "default": "true"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-anonymize/master/lib/logstash/filters/anonymize.rb",
    "name": "anonymize",
    "type": "filter",
    "params": [
      {
        "comments": "digest/hash type",
        "base": false,
        "name": "algorithm",
        "validate": [
          "SHA1",
          "SHA256",
          "SHA384",
          "SHA512",
          "MD5",
          "MURMUR3",
          "IPV4_NETWORK"
        ],
        "required": true,
        "default": "SHA1"
      },
      {
        "comments": "The fields to be anonymized",
        "base": false,
        "name": "fields",
        "validate": "array",
        "required": true
      },
      {
        "comments": "Hashing key\nWhen using MURMUR3 the key is ignored but must still be set.\nWhen using IPV4_NETWORK key is the subnet prefix lentgh",
        "base": false,
        "name": "key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-twitter/master/lib/logstash/inputs/twitter.rb",
    "name": "twitter",
    "type": "input",
    "params": [
      {
        "comments": "Your Twitter App's consumer key\n\nDon't know what this is? You need to create an \"application\"\non Twitter, see this url: <https://dev.twitter.com/apps/new>",
        "base": false,
        "name": "consumer_key",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Your Twitter App's consumer secret\n\nIf you don't have one of these, you can create one by\nregistering a new application with Twitter:\n<https://dev.twitter.com/apps/new>",
        "base": false,
        "name": "consumer_secret",
        "validate": "password",
        "required": true
      },
      {
        "comments": "Your oauth token.\n\nTo get this, login to Twitter with whatever account you want,\nthen visit <https://dev.twitter.com/apps>\n\nClick on your app (used with the consumer_key and consumer_secret settings)\nThen at the bottom of the page, click 'Create my access token' which\nwill create an oauth token and secret bound to your account and that\napplication.",
        "base": false,
        "name": "oauth_token",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Your oauth token secret.\n\nTo get this, login to Twitter with whatever account you want,\nthen visit <https://dev.twitter.com/apps>\n\nClick on your app (used with the consumer_key and consumer_secret settings)\nThen at the bottom of the page, click 'Create my access token' which\nwill create an oauth token and secret bound to your account and that\napplication.",
        "base": false,
        "name": "oauth_token_secret",
        "validate": "password",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "A comma separated list of user IDs, indicating the users to\nreturn statuses for in the Twitter stream.\nSee https://dev.twitter.com/streaming/overview/request-parameters#follow\nfor more details.",
        "base": false,
        "name": "follows",
        "validate": "array"
      },
      {
        "comments": "Record full tweet object as given to us by the Twitter Streaming API.",
        "base": false,
        "name": "full_tweet",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Lets you ingore the retweets coming out of the Twitter API. Default => false",
        "base": false,
        "name": "ignore_retweets",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Any keywords to track in the Twitter stream. For multiple keywords, use\nthe syntax [\"foo\", \"bar\"]. There's a logical OR between each keyword\nstring listed and a logical AND between words separated by spaces per\nkeyword string.\nSee https://dev.twitter.com/streaming/overview/request-parameters#track\nfor more details.\n\nThe wildcard \"*\" option is not supported. To ingest a sample stream of\nall tweets, the use_samples option is recommended.",
        "base": false,
        "name": "keywords",
        "validate": "array"
      },
      {
        "comments": "A list of BCP 47 language identifiers corresponding to any of the languages listed\non Twitter’s advanced search page will only return tweets that have been detected\nas being written in the specified languages.",
        "base": false,
        "name": "languages",
        "validate": "array"
      },
      {
        "comments": "A comma-separated list of longitude, latitude pairs specifying a set\nof bounding boxes to filter tweets by.\nSee https://dev.twitter.com/streaming/overview/request-parameters#locations\nfor more details.",
        "base": false,
        "name": "locations",
        "validate": "string"
      },
      {
        "comments": "Location of the proxy, by default the same machine as the one running this LS instance",
        "base": false,
        "name": "proxy_address",
        "validate": "string",
        "default": "127.0.0.1"
      },
      {
        "comments": "Port where the proxy is listening, by default 3128 (squid)",
        "base": false,
        "name": "proxy_port",
        "validate": "number",
        "default": "3128"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      },
      {
        "comments": "When to use a proxy to handle the connections",
        "base": false,
        "name": "use_proxy",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Returns a small random sample of all public statuses. The tweets returned\nby the default access level are the same, so if two different clients connect\nto this endpoint, they will see the same tweets. If set to true, the keywords,\nfollows, locations, and languages options will be ignored. Default => false",
        "base": false,
        "name": "use_samples",
        "validate": "boolean",
        "default": "false"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-eventlog/master/lib/logstash/inputs/eventlog.rb",
    "name": "eventlog",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "How frequently should tail check for new event logs in ms (default: 1 second)",
        "base": false,
        "name": "interval",
        "validate": "number",
        "default": "1000"
      },
      {
        "comments": "Event Log Name\nSystem and Security may require that privileges are given to the user running logstash.\nsee more at: https://social.technet.microsoft.com/forums/windowsserver/en-US/d2f813db-6142-4b5b-8d86-253ebb740473/easy-way-to-read-security-log",
        "base": false,
        "name": "logfile",
        "validate": [
          "Application",
          "Security",
          "System"
        ],
        "default": "Application"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-puppet_facter/master/lib/logstash/inputs/puppet_facter.rb",
    "name": "puppet_facter",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "uppet environment",
        "base": false,
        "name": "environment",
        "validate": "string",
        "default": "production"
      },
      {
        "comments": "emote IP Address to connect to",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "oll Interval in seconds",
        "base": false,
        "name": "interval",
        "validate": "number",
        "default": "600"
      },
      {
        "comments": "emote port to connect to",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "8140"
      },
      {
        "comments": "SL Private Key",
        "base": false,
        "name": "private_key",
        "validate": "path"
      },
      {
        "comments": "SL Public Key",
        "base": false,
        "name": "public_key",
        "validate": "path"
      },
      {
        "comments": "SL Enabled?",
        "base": false,
        "name": "ssl",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-ruby/master/lib/logstash/filters/ruby.rb",
    "name": "ruby",
    "type": "filter",
    "params": [
      {
        "comments": "The code to execute for every event.\nYou will have an `event` variable available that is the event itself.",
        "base": false,
        "name": "code",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Any code to execute at logstash startup-time",
        "base": false,
        "name": "init",
        "validate": "string"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-graphite/master/lib/logstash/inputs/graphite.rb",
    "name": "graphite",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-lumberjack/master/lib/logstash/inputs/lumberjack.rb",
    "name": "lumberjack",
    "type": "input",
    "params": [
      {
        "comments": "The port to listen on.",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "SSL certificate to use.",
        "base": false,
        "name": "ssl_certificate",
        "validate": "path",
        "required": true
      },
      {
        "comments": "SSL key to use.",
        "base": false,
        "name": "ssl_key",
        "validate": "path",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The number of seconds before we raise a timeout,\nthis option is useful to control how much time to wait if something is blocking the pipeline.",
        "base": false,
        "name": "congestion_threshold",
        "validate": "number",
        "default": "5"
      },
      {
        "comments": "The IP address to listen on.",
        "base": false,
        "name": "host",
        "validate": "string",
        "default": "0.0.0.0"
      },
      {
        "comments": "This setting no longer has any effect and will be removed in a future release.",
        "base": false,
        "name": "max_clients",
        "validate": "number",
        "deprecated": "This setting no longer has any effect. See https//github.com/logstash-plugins/logstash-input-lumberjack/pull/12 for the history of this change"
      },
      {
        "comments": "SSL key passphrase to use.",
        "base": false,
        "name": "ssl_key_passphrase",
        "validate": "password"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-lumberjack/master/lib/logstash/outputs/lumberjack.rb",
    "name": "lumberjack",
    "type": "output",
    "params": [
      {
        "comments": "list of addresses lumberjack can send to",
        "base": false,
        "name": "hosts",
        "validate": "array",
        "required": true
      },
      {
        "comments": "the port to connect to",
        "base": false,
        "name": "port",
        "validate": "number",
        "required": true
      },
      {
        "comments": "ssl certificate to use",
        "base": false,
        "name": "ssl_certificate",
        "validate": "path",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "To make efficient calls to the lumberjack output we are buffering events locally.\nif the number of events exceed the number the declared `flush_size` we will\nsend them to the logstash server.",
        "base": false,
        "name": "flush_size",
        "validate": "number",
        "default": "1024"
      },
      {
        "comments": "The amount of time since last flush before a flush is forced.\n\nThis setting helps ensure slow event rates don't get stuck in Logstash.\nFor example, if your `flush_size` is 100, and you have received 10 events,\nand it has been more than `idle_flush_time` seconds since the last flush,\nLogstash will flush those 10 events automatically.\n\nThis helps keep both fast and slow log streams moving along in\nnear-real-time.",
        "base": false,
        "name": "idle_flush_time",
        "validate": "number",
        "default": "1"
      },
      {
        "comments": "window size",
        "base": false,
        "name": "window_size",
        "validate": "number",
        "deprecated": "Use `flush_size`",
        "require": false
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-csv/master/lib/logstash/outputs/csv.rb",
    "name": "csv",
    "type": "output",
    "params": [
      {
        "comments": "The field names from the event that should be written to the CSV file.\nFields are written to the CSV in the same order as the array.\nIf a field does not exist on the event, an empty string will be written.\nSupports field reference syntax eg: `fields => [\"field1\", \"[nested][field]\"]`.",
        "base": false,
        "name": "fields",
        "validate": "array",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Options for CSV output. This is passed directly to the Ruby stdlib to_csv function.\nFull documentation is available on the http://ruby-doc.org/stdlib-2.0.0/libdoc/csv/rdoc/index.html[Ruby CSV documentation page].\nA typical use case would be to use alternative column or row seperators eg: `csv_options => {\"col_sep\" => \"\\t\" \"row_sep\" => \"\\r\\n\"}` gives tab seperated data with windows line endings",
        "base": false,
        "name": "csv_options",
        "validate": "hash",
        "required": false,
        "default": "Hash.new"
      },
      {
        "comments": "Option to not escape/munge string values. Please note turning off this option\nmay not make the values safe in your spreadsheet application",
        "base": false,
        "name": "spreadsheet_safe",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-json/master/lib/logstash/filters/json.rb",
    "name": "json",
    "type": "filter",
    "params": [
      {
        "comments": "The configuration for the JSON filter:\n[source,ruby]\n    source => source_field\n\nFor example, if you have JSON data in the `message` field:\n[source,ruby]\n    filter {\n      json {\n        source => \"message\"\n      }\n    }\n\nThe above would parse the json from the `message` field",
        "base": false,
        "name": "source",
        "validate": "string",
        "required": true
      },
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Define the target field for placing the parsed data. If this setting is\nomitted, the JSON data will be stored at the root (top level) of the event.\n\nFor example, if you want the data to be put in the `doc` field:\n[source,ruby]\n    filter {\n      json {\n        target => \"doc\"\n      }\n    }\n\nJSON in the value of the `source` field will be expanded into a\ndata structure in the `target` field.\n\nNOTE: if the `target` field already exists, it will be overwritten!",
        "base": false,
        "name": "target",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-exec/master/lib/logstash/outputs/exec.rb",
    "name": "exec",
    "type": "output",
    "params": [
      {
        "comments": "Command line to execute via subprocess. Use `dtach` or `screen` to\nmake it non blocking. This value can include `%{name}` and other\ndynamic strings.",
        "base": false,
        "name": "command",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-email/master/lib/logstash/outputs/email.rb",
    "name": "email",
    "type": "output",
    "params": [
      {
        "comments": "The fully-qualified email address to send the email to.\n\nThis field also accepts a comma-separated string of addresses, for example:\n`\"me@host.com, you@host.com\"`\n\nYou can also use dynamic fields from the event with the `%{fieldname}` syntax.",
        "base": false,
        "name": "to",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The address used to connect to the mail server",
        "base": false,
        "name": "address",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "Attachments - specify the name(s) and location(s) of the files.",
        "base": false,
        "name": "attachments",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Authentication method used when identifying with the server",
        "base": false,
        "name": "authentication",
        "validate": "string"
      },
      {
        "comments": "Body for the email - plain text only.",
        "base": false,
        "name": "body",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "The fully-qualified email address(es) to include as cc: address(es).\n\nThis field also accepts a comma-separated string of addresses, for example:\n`\"me@host.com, you@host.com\"`",
        "base": false,
        "name": "cc",
        "validate": "string"
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "contenttype : for multipart messages, set the content-type and/or charset of the HTML part.\nNOTE: this may not be functional (KH)",
        "base": false,
        "name": "contenttype",
        "validate": "string",
        "default": "\"text/html",
        "charset=UTF-8\"": null
      },
      {
        "comments": "Run the mail relay in debug mode",
        "base": false,
        "name": "debug",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Domain used to send the email messages",
        "base": false,
        "name": "domain",
        "validate": "string",
        "default": "localhost"
      },
      {
        "comments": "The fully-qualified email address for the From: field in the email.",
        "base": false,
        "name": "from",
        "validate": "string",
        "default": "logstash.alert@nowhere.com"
      },
      {
        "comments": "HTML Body for the email, which may contain HTML markup.",
        "base": false,
        "name": "htmlbody",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "Password to authenticate with the server",
        "base": false,
        "name": "password",
        "validate": "string"
      },
      {
        "comments": "Port used to communicate with the mail server",
        "base": false,
        "name": "port",
        "validate": "number",
        "default": "25"
      },
      {
        "comments": "The fully qualified email address for the Reply-To: field.",
        "base": false,
        "name": "replyto",
        "validate": "string"
      },
      {
        "comments": "Subject: for the email.",
        "base": false,
        "name": "subject",
        "validate": "string",
        "default": ""
      },
      {
        "comments": "Enables TLS when communicating with the server",
        "base": false,
        "name": "use_tls",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Username to authenticate with the server",
        "base": false,
        "name": "username",
        "validate": "string"
      },
      {
        "comments": "How Logstash should send the email, either via SMTP or by invoking sendmail.",
        "base": false,
        "name": "via",
        "validate": "string",
        "default": "smtp"
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-drop/master/lib/logstash/filters/drop.rb",
    "name": "drop",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Drop filter.\n\nDrops everything that gets to this filter.\n\nThis is best used in combination with conditionals, for example:\n[source,ruby]\n    filter {\n      if [loglevel] == \"debug\" {\n        drop { }\n      }\n    }\n\nThe above will only pass events to the drop filter if the loglevel field is\n`debug`. This will cause all events matching to be dropped.\nDrop all the events within a pre-configured percentage.\n\nThis is useful if you just need a percentage but not the whole.\n\nExample, to only drop around 40% of the events that have the field loglevel wiht value \"debug\".\n\n    filter {\n      if [loglevel] == \"debug\" {\n        drop {\n          percentage => 40\n        }\n      }\n    }",
        "base": false,
        "name": "percentage",
        "validate": "number",
        "default": "100"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-heroku/master/lib/logstash/inputs/heroku.rb",
    "name": "heroku",
    "type": "input",
    "params": [
      {
        "comments": "The name of your heroku application. This is usually the first part of the\nthe domain name `my-app-name.herokuapp.com`",
        "base": false,
        "name": "app",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-output-file/master/lib/logstash/outputs/file.rb",
    "name": "file",
    "type": "output",
    "params": [
      {
        "comments": "The path to the file to write. Event fields can be used here,\nlike `/var/log/logstash/%{host}/%{application}`\nOne may also utilize the path option for date-based log\nrotation via the joda time format. This will use the event\ntimestamp.\nE.g.: `path => \"./test-%{+YYYY-MM-dd}.txt\"` to create\n`./test-2013-05-29.txt`\n\nIf you use an absolute path you cannot start with a dynamic string.\nE.g: `/%{myfield}/`, `/test-%{myfield}/` are not valid paths",
        "base": false,
        "name": "path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "If the a file is deleted, but an event is comming with the need to be stored\nin such a file, the plugin will created a gain this file. Default => true",
        "base": false,
        "name": "create_if_deleted",
        "validate": "boolean",
        "default": "true"
      },
      {
        "comments": "Dir access mode to use. Note that due to the bug in jruby system umask\nis ignored on linux: https://github.com/jruby/jruby/issues/3426\nSetting it to -1 uses default OS value.\nExample: `\"dir_mode\" => 0750`",
        "base": false,
        "name": "dir_mode",
        "validate": "number",
        "default": "-1"
      },
      {
        "comments": "File access mode to use. Note that due to the bug in jruby system umask\nis ignored on linux: https://github.com/jruby/jruby/issues/3426\nSetting it to -1 uses default OS value.\nExample: `\"file_mode\" => 0640`",
        "base": false,
        "name": "file_mode",
        "validate": "number",
        "default": "-1"
      },
      {
        "comments": "If the generated path is invalid, the events will be saved\ninto this file and inside the defined path.",
        "base": false,
        "name": "filename_failure",
        "validate": "string",
        "default": "_filepath_failures"
      },
      {
        "comments": "Flush interval (in seconds) for flushing writes to log files.\n0 will flush on every message.",
        "base": false,
        "name": "flush_interval",
        "validate": "number",
        "default": "2"
      },
      {
        "comments": "Gzip the output stream before writing to disk.",
        "base": false,
        "name": "gzip",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "The format to use when writing events to the file. This value\nsupports any string and can include `%{name}` and other dynamic\nstrings.\n\nIf this setting is omitted, the full json representation of the\nevent will be written as a single line.",
        "base": false,
        "name": "message_format",
        "validate": "string",
        "deprecated": "\"You can achieve the same behavior with the \"line\" codec\""
      },
      {
        "comments": "The number of workers to use for this output.\nNote that this setting may not be useful for all outputs.",
        "base": true,
        "name": "workers",
        "validate": "number",
        "default": "1"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-varnishlog/master/lib/logstash/inputs/varnishlog.rb",
    "name": "varnishlog",
    "type": "input",
    "params": [
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-filter-mutate/master/lib/logstash/filters/mutate.rb",
    "name": "mutate",
    "type": "filter",
    "params": [
      {
        "comments": "If this filter is successful, add any arbitrary fields to this event.\nField names can be dynamic and include parts of the event using the `%{field}`.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_field => { \"foo_%{somefield}\" => \"Hello world, from %{host}\" }\n      }\n    }\n[source,ruby]\n    # You can also add multiple fields at once:\n    filter {\n      %PLUGIN% {\n        add_field => {\n          \"foo_%{somefield}\" => \"Hello world, from %{host}\"\n          \"new_field\" => \"new_static_value\"\n        }\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add field `foo_hello` if it is present, with the\nvalue above and the `%{host}` piece replaced with that value from the\nevent. The second example would also add a hardcoded field.",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "If this filter is successful, add arbitrary tags to the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also add multiple tags at once:\n    filter {\n      %PLUGIN% {\n        add_tag => [ \"foo_%{somefield}\", \"taggedy_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).",
        "base": true,
        "name": "add_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Convert a field's value to a different type, like turning a string to an\ninteger. If the field value is an array, all members will be converted.\nIf the field is a hash, no action will be taken.\n\nIf the conversion type is `boolean`, the acceptable values are:\n\n* **True:** `true`, `t`, `yes`, `y`, and `1`\n* **False:** `false`, `f`, `no`, `n`, and `0`\n\nIf a value other than these is provided, it will pass straight through\nand log a warning message.\n\nValid conversion targets are: integer, float, string, and boolean.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n        convert => { \"fieldname\" => \"integer\" }\n      }\n    }",
        "base": false,
        "name": "convert",
        "validate": "hash"
      },
      {
        "comments": "Convert a string field by applying a regular expression and a replacement.\nIf the field is not a string, no action will be taken.\n\nThis configuration takes an array consisting of 3 elements per\nfield/substitution.\n\nBe aware of escaping any backslash in the config file.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n        gsub => [\n          # replace all forward slashes with underscore\n          \"fieldname\", \"/\", \"_\",\n          # replace backslashes, question marks, hashes, and minuses\n          # with a dot \".\"\n          \"fieldname2\", \"[\\\\?#-]\", \".\"\n        ]\n      }\n    }\n",
        "base": false,
        "name": "gsub",
        "validate": "array"
      },
      {
        "comments": "Join an array with a separator character. Does nothing on non-array fields.\n\nExample:\n[source,ruby]\n   filter {\n     mutate {\n       join => { \"fieldname\" => \",\" }\n     }\n   }",
        "base": false,
        "name": "join",
        "validate": "hash"
      },
      {
        "comments": "Convert a string to its lowercase equivalent.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n        lowercase => [ \"fieldname\" ]\n      }\n    }",
        "base": false,
        "name": "lowercase",
        "validate": "array"
      },
      {
        "comments": "Merge two fields of arrays or hashes.\nString fields will be automatically be converted into an array, so:\n==========================\n  `array` + `string` will work\n  `string` + `string` will result in an 2 entry array in `dest_field`\n  `array` and `hash` will not work\n==========================\nExample:\n[source,ruby]\n    filter {\n      mutate {\n         merge => { \"dest_field\" => \"added_field\" }\n      }\n    }",
        "base": false,
        "name": "merge",
        "validate": "hash"
      },
      {
        "comments": "Call the filter flush method at regular interval.\nOptional.",
        "base": true,
        "name": "periodic_flush",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Remove one or more fields.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n        remove => [ \"client\" ]  # Removes the 'client' field\n      }\n    }\n\nThis option is deprecated, instead use `remove_field` option available in all\nfilters.",
        "base": false,
        "name": "remove",
        "validate": "array",
        "deprecated": true
      },
      {
        "comments": "If this filter is successful, remove arbitrary fields from this event.\nFields names can be dynamic and include parts of the event using the %{field}\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple fields at once:\n    filter {\n      %PLUGIN% {\n        remove_field => [ \"foo_%{somefield}\", \"my_extraneous_field\" ]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the field with name `foo_hello` if it is present. The second\nexample would remove an additional, non-dynamic field.",
        "base": true,
        "name": "remove_field",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "If this filter is successful, remove arbitrary tags from the event.\nTags can be dynamic and include parts of the event using the `%{field}`\nsyntax.\n\nExample:\n[source,ruby]\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\" ]\n      }\n    }\n[source,ruby]\n    # You can also remove multiple tags at once:\n    filter {\n      %PLUGIN% {\n        remove_tag => [ \"foo_%{somefield}\", \"sad_unwanted_tag\"]\n      }\n    }\n\nIf the event has field `\"somefield\" == \"hello\"` this filter, on success,\nwould remove the tag `foo_hello` if it is present. The second example\nwould remove a sad, unwanted tag as well.",
        "base": true,
        "name": "remove_tag",
        "validate": "array",
        "default": "[]"
      },
      {
        "comments": "Rename one or more fields.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n        # Renames the 'HOSTORIP' field to 'client_ip'\n        rename => { \"HOSTORIP\" => \"client_ip\" }\n      }\n    }",
        "base": false,
        "name": "rename",
        "validate": "hash"
      },
      {
        "comments": "Replace a field with a new value. The new value can include `%{foo}` strings\nto help you build a new value from other parts of the event.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n        replace => { \"message\" => \"%{source_host}: My new message\" }\n      }\n    }",
        "base": false,
        "name": "replace",
        "validate": "hash"
      },
      {
        "comments": "Split a field to an array using a separator character. Only works on string\nfields.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n         split => { \"fieldname\" => \",\" }\n      }\n    }",
        "base": false,
        "name": "split",
        "validate": "hash"
      },
      {
        "comments": "Strip whitespace from field. NOTE: this only works on leading and trailing whitespace.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n         strip => [\"field1\", \"field2\"]\n      }\n    }",
        "base": false,
        "name": "strip",
        "validate": "array"
      },
      {
        "comments": "Update an existing field with a new value. If the field does not exist,\nthen no action will be taken.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n        update => { \"sample\" => \"My new message\" }\n      }\n    }",
        "base": false,
        "name": "update",
        "validate": "hash"
      },
      {
        "comments": "Convert a string to its uppercase equivalent.\n\nExample:\n[source,ruby]\n    filter {\n      mutate {\n        uppercase => [ \"fieldname\" ]\n      }\n    }",
        "base": false,
        "name": "uppercase",
        "validate": "array"
      }
    ]
  },
  {
    "url": "https://raw.githubusercontent.com/logstash-plugins/logstash-input-unix/master/lib/logstash/inputs/unix.rb",
    "name": "unix",
    "type": "input",
    "params": [
      {
        "comments": "When mode is `server`, the path to listen on.\nWhen mode is `client`, the path to connect to.",
        "base": false,
        "name": "path",
        "validate": "string",
        "required": true
      },
      {
        "comments": "Add a field to an event",
        "base": true,
        "name": "add_field",
        "validate": "hash",
        "default": "{}"
      },
      {
        "comments": "The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.",
        "base": true,
        "name": "codec",
        "validate": "codec",
        "default": "plain"
      },
      {
        "comments": "The 'read' timeout in seconds. If a particular connection is idle for\nmore than this timeout period, we will assume it is dead and close it.\n\nIf you never want to timeout, use -1.",
        "base": false,
        "name": "data_timeout",
        "validate": "number",
        "default": "-1"
      },
      {
        "comments": "Remove socket file in case of EADDRINUSE failure",
        "base": false,
        "name": "force_unlink",
        "validate": "boolean",
        "default": "false"
      },
      {
        "comments": "Mode to operate in. `server` listens for client connections,\n`client` connects to a server.",
        "base": false,
        "name": "mode",
        "validate": [
          "server",
          "client"
        ],
        "default": "server"
      },
      {
        "comments": "Add any number of arbitrary tags to your event.\n\nThis can help with processing later.",
        "base": true,
        "name": "tags",
        "validate": "array"
      },
      {
        "comments": "Add a `type` field to all events handled by this input.\n\nTypes are used mainly for filter activation.\n\nThe type is stored as part of the event itself, so you can\nalso use the type to search for it in Kibana.\n\nIf you try to set a type on an event that already has one (for\nexample when you send an event from a shipper to an indexer) then\na new input will not override the existing type. A type set at\nthe shipper stays with that event for its life even\nwhen sent to another Logstash server.",
        "base": true,
        "name": "type",
        "validate": "string"
      }
    ]
  }
]